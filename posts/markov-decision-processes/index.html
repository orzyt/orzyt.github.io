<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    <title>强化学习（二）：马尔可夫决策过程 - 扬涛的博客</title>
    <meta itemprop="name" content="强化学习（二）：马尔可夫决策过程 - 扬涛的博客">
    
    <meta name="keywords" content="扬涛的博客, 郑扬涛, orzyt, leetcode">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    
    <link rel="alternate" href="/atom.xml" title="扬涛的博客" type="application/atom+xml">
    
    
    <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" href="/libs/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="/libs/headroom/animate.css">
    

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/bootstrap/js/bootstrap.min.js"></script>
    <script src="/libs/headroom/headroom.min.js"></script>
    <script src="/libs/headroom/jQuery.headroom.min.js"></script>
    <script src="/libs/jquery/sort.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?63f226f2212285cc30d6ffa0636da07a";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    


</head>
</html>
<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">扬涛的博客</span>
            </a>
            <nav id="main-nav">
                <div>
                    <ul class="nav navbar-nav">
                    
                        <a class="main-nav-link" href="/.">主页</a>
                    
                        <a class="main-nav-link" href="/archives">归档</a>
                    
                        <a class="main-nav-link" href="/study">资源</a>
                    
                        <a class="main-nav-link" href="/about">关于</a>
                    
                    
                    </ul>
                </div>
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/img/avatar.png">
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <ul class="menu outer">
            
                <a class="main-nav-link" href="/.">主页</a>
            
                <a class="main-nav-link" href="/archives">归档</a>
            
                <a class="main-nav-link" href="/study">资源</a>
            
                <a class="main-nav-link" href="/about">关于</a>
            
            
        </ul>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/img/avatar.png">
            <h2 id="name">郑扬涛</h2>
            <h3 id="title">M.S. Student @ Beihang Univ.</h3>
            <span id="location"><i class="fa fa-map-marker"></i>Beijing, China</span>
            <a id="follow" target="_blank" href="https://www.zhihu.com/people/orzyt/activities">关注我</a>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="https://github.com/orzyt" target="_blank" title="github">
                            <i class="fa fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="https://www.linkedin.com/in/orzyt" target="_blank" title="linkedin">
                            <i class="fa fa-linkedin"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="https://weibo.com/orzyt" target="_blank" title="weibo">
                            <i class="fa fa-weibo"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/atom.xml" target="_blank" title="rss">
                            <i class="fa fa-rss"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main"><article id="post-markov-decision-processes" class="article article-type-post" itemscope="" itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
    
        
            <h1 class="article-title" itemprop="name">
                强化学习（二）：马尔可夫决策过程
            </h1>
        
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/posts/markov-decision-processes/">
            <time datetime="2019-02-27T07:38:18.000Z" itemprop="datePublished">2019-02-27</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/强化学习/">强化学习</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/强化学习/">强化学习</a>, <a class="tag-link" href="/tags/马尔可夫决策过程/">马尔可夫决策过程</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
            
                
                    <hr>
<p>在上一篇文章 <a href="https://orzyt.cn/posts/introduction-to-rl">强化学习（一）：强化学习简介</a> 中, 我们介绍了强化学习的一些基本概念.</p>
<p>本文将介绍用来对强化学习问题进行建模的<strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>.</p>
<a id="more"></a>
<h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><h3 id="马尔可夫决策过程简介"><a href="#马尔可夫决策过程简介" class="headerlink" title="马尔可夫决策过程简介"></a>马尔可夫决策过程简介</h3><p><strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>形式上用来描述强化学习中的环境.</p>
<p>其中,环境是<strong>完全可观测的(fully observable)</strong>,即当前状态可以完全表征过程.</p>
<p>几乎所有的强化学习问题都能用MDPs来描述：</p>
<ul>
<li>最优控制问题可以描述成连续MDPs;</li>
<li>部分观测环境可以转化成MDPs;</li>
<li>赌博机问题是只有一个状态的MDPs.</li>
</ul>
<hr>
<h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3nzaa8yj20mn0593ym.jpg" alt="马尔科夫性质" width="60%" height="60%"></p>
<p>马尔科夫性质(Markov Property)表明: <strong>未来只与现在有关,而与过去无关.</strong></p>
<hr>
<h3 id="状态转移矩阵"><a href="#状态转移矩阵" class="headerlink" title="状态转移矩阵"></a>状态转移矩阵</h3><p>对于一个马尔可夫状态$S$及其后继状态$S’$,其状态转移概率由下式定义:</p>
<script type="math/tex; mode=display">
\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]</script><p><strong>状态转移矩阵(State Transition Matrix)$\mathcal{P}$</strong>定义了从所有状态$S$转移到所有后继状态$S’$的概率.</p>
<script type="math/tex; mode=display">
\mathcal { P } = \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \dots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n 1 } } & { \cdots } & { \mathcal { P } _ { n n } } \end{array} \right]</script><p>其中,$n$为状态个数,且矩阵的每行和为1.</p>
<hr>
<h3 id="马尔可夫过程-1"><a href="#马尔可夫过程-1" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h3><p><strong>马尔可夫过程(Markov Process)</strong>是一个无记忆的随机过程(memoryless random process).</p>
<p>即,随机状态$S_1, S_2, \dots$序列具有马尔可夫性质.</p>
<blockquote>
<p>马尔可夫过程(或马尔可夫链)是一个二元组$&lt;\mathcal{S}, \mathcal{P}&gt;$</p>
<ul>
<li>$\mathcal{S}$: (有限)状态集</li>
<li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li>
</ul>
</blockquote>
<p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l1vm9xkzj20c80act96.jpg" alt="Example: Student Markov Chain" width="50%" height="50%"></p>
<p>圆圈代表状态, 箭头代表状态之间的转移, 数值代表转移概率.</p>
<p>状态转移矩阵$\mathcal{P}$如下:</p>
<script type="math/tex; mode=display">
{\mathcal P} =\begin{bmatrix}  & C1 & C2 & C3 &  Pass & Pub & FB & Sleep\\  C1 & &0.5 &  &   & & 0.5 & \\ C2  & & &  0.8 & & & &0.2\\ C3  & & &  & 0.6& 0.4& &\\ Pass  & & &  & & & &1.0\\ Pub  &0.2 & 0.4& 0.4 & & & &\\ FB  &0.1 & &  & & & 0.9 &\\ Sleep  & & &  & & & &1.0 \end{bmatrix}</script><hr>
<h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><p><strong>马尔可夫奖励过程(Markov Reward Process, MRP)</strong>是<em>带有奖励的马尔可夫链</em>.</p>
<blockquote>
<p>马尔可夫奖励过程是一个四元组&lt;$\mathcal{S}$, $\mathcal{P}$, <font color="red">$\mathcal{R}$</font>, <font color="red">$\mathcal{\gamma}$</font>&gt;</p>
<ul>
<li>$\mathcal{S}$: (有限)状态集</li>
<li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li>
<li><font color="red"> $\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right]$ </font></li>
<li><font color="red"> $\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </font>

</li>
</ul>
</blockquote>
<p><img src="http://wx1.sinaimg.cn/large/8662e3cegy1g0l2klvnixj20cf0aowf0.jpg" alt="Example: Student MRP" width="50%" height="50%"></p>
<h3 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h3><blockquote>
<p><strong>回报(Return)</strong> $G_t$ 是从时间 $t$ 开始的总折扣奖励.</p>
<script type="math/tex; mode=display">G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 }</script></blockquote>
<ul>
<li>折扣因子 $\gamma \in [ 0,1 ]$ 表示未来的奖励在当前的价值. 由于未来的奖励充满不确定性, 因此需要乘上折扣因子;</li>
<li>$\gamma$ 接近 $0$ 表明更注重当前的奖励(myopic);</li>
<li>$\gamma$ 接近 $1$ 表明更具有远见(far-sighted).</li>
</ul>
<hr>
<h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数(Value Function) $v(s)$ 表示一个状态 $s$ 的长期价值(long-term value).</p>
<blockquote>
<p>一个马尔可夫奖励过程(MRP)的<strong>状态值函数 $v(s)$</strong>是从状态 $s$ 开始的期望回报.</p>
<script type="math/tex; mode=display">v ( s ) = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right]</script></blockquote>
<hr>
<h3 id="MRPs的贝尔曼方程"><a href="#MRPs的贝尔曼方程" class="headerlink" title="MRPs的贝尔曼方程"></a>MRPs的贝尔曼方程</h3><p>值函数可以被分解为两部分:</p>
<ul>
<li>立即奖励 $R_{t+1}$</li>
<li>后继状态的折扣价值 $\gamma v(S_{t+1})$</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned} 
v ( s ) & = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right] \\ 
& = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \gamma ^ { 2 } R _ { t + 3 } + \ldots | S _ { t } = s \right] \\ 
& = \mathbb { E } \left[ R _ { t + 1 } + \gamma \left( R _ { t + 2 } + \gamma R _ { t + 3 } + \ldots \right) | S _ { t } = s \right] \\ 
& = \mathbb { E } \left[ R _ { t + 1 } + \gamma G _ { t + 1 } | S _ { t } = s \right] \\ 
& = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \mathbb { E } \left[ \gamma G _ { t + 1 } | S _ { t } = s \right]\\ 
& = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \gamma v \left( S _ { t + 1 } \right)\\ 
& = \mathbb { E } \left[ R _ { t + 1 } + \gamma v \left( S _ { t + 1 } \right) | S _ { t } = s \right] 
\end{aligned}
\tag{1}
\label{eq:mrp-bellman-equation}</script><p>上式表明, $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 的值函数之间满足递推关系. </p>
<p>该递推式也称为<strong>贝尔曼方程(Bellman Equation)</strong>.</p>
<p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0l3fh3jb3j207802zglh.jpg" alt="Bellman Equation for MRPs" width="30%" height="30%"></p>
<p>如果已知概率转移矩阵 $\mathcal{P}$, 则可将公式\eqref{eq:mrp-bellman-equation}变形为:</p>
<script type="math/tex; mode=display">
v ( s ) = \mathcal { R } _ { s } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } v \left( s ^ { \prime } \right)
\tag{2}
\label{eq:mrp-bellman-equation-2}</script><p>例子:</p>
<p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l3pbm9ixj20c30b5mxp.jpg" alt="Example: Bellman Equation for Student MRP" width="40%" height="40%"></p>
<p><strong>贝尔曼方程的矩阵形式:</strong></p>
<p>可将公式\eqref{eq:mrp-bellman-equation-2}改写为矩阵形式:</p>
<script type="math/tex; mode=display">
v = \mathcal { R } + \gamma \mathcal { P } v</script><p>其中, $v$ 为一个列向量, 向量的元素为每个状态的值函数.</p>
<script type="math/tex; mode=display">
\left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right] = \left[ \begin{array} { c } { \mathcal { R } _ { 1 } } \\ { \vdots } \\ { \mathcal { R } _ { n } } \end{array} \right] + \gamma \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \ldots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n1 } } & { \ldots } & { \mathcal { P } _ { n n } } \end{array} \right] \left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right]</script><p>观测贝尔曼方程的矩阵形式, 可知其为线性方程, 可直接求解如下.</p>
<script type="math/tex; mode=display">
\begin{aligned} 
v & = \mathcal { R } + \gamma \mathcal { P } v \\
( I - \gamma \mathcal { P } ) v & = \mathcal { R } \\
v & = ( I - \gamma \mathcal { P } ) ^ { - 1 } \mathcal { R }
\end{aligned}</script><p>计算复杂度为: $\mathcal{O}(n^3)$. 因此, 只适合直接求解小规模的MRP问题.</p>
<p>对于大规模的MRP问题, 通常采取以下的迭代方法:</p>
<ul>
<li>动态规划(Dynamic programming)</li>
<li>蒙特卡洛评估(Monte-Carlo evaluation)</li>
<li>时序差分学习(Temporal-Difference learning)</li>
</ul>
<hr>
<h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p><strong>马尔可夫决策过程(Markov Decision Process, MDP)</strong>是<em>带有决策的马尔可夫奖励过程</em>.</p>
<blockquote>
<p>马尔可夫决策过程是一个五元组&lt;$\mathcal{S}$, <font color="red">$\mathcal{A}$</font>, $\mathcal{P}$, $\mathcal{R}$, $\mathcal{\gamma}$&gt;</p>
<ul>
<li>$\mathcal{S}$: 有限的状态集</li>
<li><font color="red"> $\mathcal{A}$: 有限的动作集</font></li>
<li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } ^ {a}= \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s, A _ { t } = a \right]$</li>
<li>$\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } ^ {a} = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s, A _ { t } = a \right]$</li>
<li>$\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </li>
</ul>
</blockquote>
<p>例子:</p>
<p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0l47drh0vj20g30d93zc.jpg" alt="Example: Student MDP" width="45%" height="45%"></p>
<hr>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><blockquote>
<p><strong>策略(Policy) $\pi$</strong> 是给定状态的动作分布.</p>
<script type="math/tex; mode=display">\pi ( a | s ) = \mathbb { P } \left[ A _ { t } = a | S _ { t } = s \right]</script></blockquote>
<ul>
<li>策略完全决定智能体的行为;</li>
<li>MDP策略值依赖于当前状态(无关历史);</li>
<li>策略是固定的(与时间无关). $A _ { t } \sim \pi ( \cdot | S _ { t } ) , \forall t &gt; 0$</li>
</ul>
<p>给定一个马尔可夫决策过程 $M = &lt;\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma}&gt;$ 和 一个策略 $\pi$, 其可以转化为<em>马尔可夫过程</em>和<em>马尔可夫奖励过程</em>.</p>
<ul>
<li><p>状态序列 $S_1, S_2, \dots$ 是马尔科夫决策过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}&gt;$.</p>
</li>
<li><p>状态和奖励序列 $S_1, R_2, S_2, \dots$ 是马尔科夫奖励过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$.</p>
</li>
</ul>
<p>其中,</p>
<script type="math/tex; mode=display">
\mathcal{P}_{s,s'}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{P}_{ss'}^{a}</script><script type="math/tex; mode=display">
\mathcal{R}_{s}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{R}_{s}^{a}</script><hr>
<h3 id="值函数-1"><a href="#值函数-1" class="headerlink" title="值函数"></a>值函数</h3><p><strong>值函数(Value Function)</strong>可分为<strong>状态值函数(state-value function)</strong>和<strong>动作值函数(action-value function)</strong>.</p>
<blockquote>
<p>MDP的<strong>状态值函数 $v_{\pi}(s)$ </strong>是从状态 $s$ 开始, 然后按照策略 $\pi$ 决策所获得的期望回报.</p>
<script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t | S_t = s \right]</script><p>MDP的<strong>动作值函数 $q_{\pi}(s, a)$ </strong>是从状态 $s$ 开始, 采取动作 $a$, 然后按照策略 $\pi$ 决策所获得的期望回报.</p>
<script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t = a \right]</script></blockquote>
<hr>
<h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>状态值函数可以被分解为两部分, <strong>立即奖励 + 后继状态的折扣价值</strong>.</p>
<script type="math/tex; mode=display">
v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s \right]</script><p>动作值函数也可以类似地分解.</p>
<script type="math/tex; mode=display">
q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \right]</script><hr>
<p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0ldl141fkj20bb04xq2x.jpg" width="40%" height="40%"></p>
<p>上图中, 空心圆圈代表状态, 实心圆圈代表动作.</p>
<p>在已知策略 $\pi$ 的情况下, 状态值函数 $v_{\pi}(s)$ 可以用动作值函数 $q_{\pi}(s, a)$ 进行表示:</p>
<script type="math/tex; mode=display">
v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a) 
\tag{3}
\label{eq:mdp-state-value-function}</script><hr>
<p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0lds6jc80j20b004rmx6.jpg" width="40%" height="40%"></p>
<p>同理, 动作值函数 $q_{\pi}(s, a)$ 也可以用状态值函数 $v_{\pi}(s)$ 进行表示:</p>
<script type="math/tex; mode=display">
q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}v_{\pi}(s') 
\tag{4}
\label{eq:mdp-action-value-function}</script><hr>
<p><strong>状态值函数的贝尔曼期望方程:</strong></p>
<p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0le5yxgeij20b706hdfx.jpg" width="40%" height="40%"></p>
<p>将公式\eqref{eq:mdp-action-value-function}代入公式\eqref{eq:mdp-state-value-function}中, 可得状态值函数的贝尔曼期望方程:</p>
<script type="math/tex; mode=display">
v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{\pi}(s')  \right)</script><hr>
<p><strong>动作值函数的贝尔曼期望方程:</strong></p>
<p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0le9cf2u7j20bd05wwek.jpg" width="40%" height="40%"></p>
<p>将公式\eqref{eq:mdp-state-value-function}代入公式\eqref{eq:mdp-action-value-function}中, 可得动作值函数的贝尔曼期望方程:</p>
<script type="math/tex; mode=display">
q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} \sum \limits_{a' \in \mathcal{A}} \pi (a' | s') q_{\pi}(s', a')</script><hr>
<p>例子:</p>
<p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lecy0oxgj20h90dcwfj.jpg" alt="状态值函数的贝尔曼期望方程示例" width="55%" height="55%"></p>
<hr>
<p><strong>贝尔曼期望方程的矩阵形式:</strong></p>
<script type="math/tex; mode=display">
v_{\pi} = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} v_{\pi}</script><p>可直接求解:</p>
<script type="math/tex; mode=display">
v_{\pi} = (I - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}</script><hr>
<h3 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h3><blockquote>
<p><strong>最优状态值函数(optimal state-value function)</strong> $v_{*}(s)$ 是所有策略中最大的值函数.</p>
<script type="math/tex; mode=display">v_{*}(s) = \max \limits_{\pi}v_{\pi}(s)</script><p><strong>最优动作值函数(optimal action-value function)</strong> $q_{*}(s, a)$ 是所有策略中最大的动作值函数.</p>
<script type="math/tex; mode=display">q_{*}(s, a) = \max \limits_{\pi}q_{\pi}(s, a)</script></blockquote>
<ul>
<li>最优值函数代表了MDP的最好性能.</li>
<li>当得知最优值函数时, MDP可被认为”已解决”.</li>
</ul>
<hr>
<p>例子: </p>
<p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leoxfaylj20h70ee75c.jpg" alt="Student MDP中的最优状态值函数" width="50%" height="50%"></p>
<hr>
<p>例子:</p>
<p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leqk38l4j20hh0eg75i.jpg" alt="Student MDP中的最优动作值函数" width="50%" height="50%"></p>
<p>注: 根据公式\eqref{eq:mdp-state-value-function}, Pub动作的最优值应为 $q_{*} = +1 + (0.2 \times 6 + 0.4 \times 8 + 0.4 \times 10) = 9.4$.</p>
<hr>
<h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p>首先定义策略之间的偏序关系, 使得策略之间可以进行比较:</p>
<script type="math/tex; mode=display">
\pi \geq \pi ' \quad \text{if} \quad  v_{\pi}(s) \geq v_{\pi '}(s) , \forall s</script><p>对于任意的MDP来说:</p>
<ul>
<li>存在一个最优策略 $\pi_{*}$, 使得 $\pi_{*} \geq \pi, \forall \pi$</li>
<li>所有的最优策略都能取得最优值函数 $v_{\pi_{*}}(s) = v_{*}(s)$</li>
<li>所有的最优策略都能取得最优动作值函数 $q_{\pi_{*}}(s, a) = q_{*}(s, a)$</li>
</ul>
<hr>
<p><strong>寻找最优策略</strong></p>
<p>一个最优策略可以通过最大化所有的 $q_{*}(s, a)$ 得到:</p>
<script type="math/tex; mode=display">
\pi_{*} \left( a | s \right) = \left \{ 
\begin{array}{ll}
1 \ {\mathbb {if}} \ a = \operatorname*{argmax} \limits_{a \in \mathcal{A}} \ q_{*} \left( s,a \right) \\              
0 \ {\mathbb {otherwise}}              
\end{array} \right.</script><ul>
<li>对于任意的MDP, 总存在确定的最优策略</li>
<li>如果我们知道 $q_{*}(s, a)$, 则可以立即得到最优策略</li>
</ul>
<hr>
<p>例子:</p>
<p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lfhg0710j20hn0ehjsl.jpg" alt="Student MDP的最优策略" width="50%" height="50%"></p>
<p>图中红色弧线表示每个状态的最优决策.</p>
<hr>
<h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>$v_{*}$可以通过贝尔曼最优方程递归得到:</p>
<p><img src="https://ws1.sinaimg.cn/large/8662e3cegy1g0lfkujh38j20b804uaa2.jpg" width="40%" height="40%"></p>
<script type="math/tex; mode=display">
v_{*}(s) = \max \limits_{a} q_{*}(s, a)
\tag{5}
\label{eq:state-bellman-optimal-equation}</script><p>与公式\eqref{eq:mdp-state-value-function}的贝尔曼期望方程进行比较, 此时不再取均值, 而是取最大值.</p>
<hr>
<p>$q_{*}$与公式\eqref{eq:mdp-action-value-function}类似:</p>
<p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m10t6s7vj208003a747.jpg" width="40%" height="40%"></p>
<script type="math/tex; mode=display">
q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)
\tag{6}
\label{eq:action-bellman-optimal-equation}</script><hr>
<p><strong>状态值函数的贝尔曼最优方程</strong></p>
<p><img src="https://wx3.sinaimg.cn/large/8662e3cegy1g0m14a2fenj208m04xq2x.jpg" width="40%" height="40%"></p>
<p>将公式\eqref{eq:action-bellman-optimal-equation}代入公式\eqref{eq:state-bellman-optimal-equation}可得 $v_{*}$ 的贝尔曼最优方程:</p>
<script type="math/tex; mode=display">
v _ { * } ( s ) = \max _ { a } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script><hr>
<p><strong>动作值函数的贝尔曼最优方程</strong></p>
<p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0m18irqg7j208804bgll.jpg" width="40%" height="40%"></p>
<p>将公式\eqref{eq:state-bellman-optimal-equation}代入公式\eqref{eq:action-bellman-optimal-equation}可得 $q_{*}$ 的贝尔曼最优方程:</p>
<script type="math/tex; mode=display">
q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \max _ { a ^ { \prime } } q _ { * } \left( s ^ { \prime } , a ^ { \prime } \right)</script><hr>
<p>例子:</p>
<p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m1ato6q2j20d70atjs3.jpg" alt="Student MDP贝尔曼最优方程" width="50%" height="50%"></p>
<hr>
<h3 id="贝尔曼最优方程的求解"><a href="#贝尔曼最优方程的求解" class="headerlink" title="贝尔曼最优方程的求解"></a>贝尔曼最优方程的求解</h3><p>贝尔曼最优方程<strong>不是线性的</strong>(因为有取$max$操作), 因此没有封闭解(Closed-form solution).</p>
<p>通常采用迭代求解方法:</p>
<ul>
<li>值迭代(Value Iteration)</li>
<li>策略迭代(Policy Iteration)</li>
<li>Q-Learning</li>
<li>Sarsa</li>
</ul>
<h2 id="MDP的扩展"><a href="#MDP的扩展" class="headerlink" title="MDP的扩展"></a>MDP的扩展</h2><ul>
<li>无穷和连续的MDPs</li>
<li>部分可观测的MDPs</li>
<li>不折扣, 平均奖励MDPs</li>
</ul>
<hr>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第三章</li>
<li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf" target="_blank" rel="noopener">Lecture2: Markov Decision Processes</a></li>
</ul>

                
            
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://orzyt.cn/posts/markov-decision-processes/" data-id="cjtnzd3e600oxiksio49nceam" class="article-share-link"><i class="fa fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fa fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fa fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fa fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fa fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    
        <a href="https://orzyt.cn/posts/markov-decision-processes/#comments" class="article-comment-link">评论</a>
    

        </footer>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/posts/planning-by-dp/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">上一篇</strong>
            <div class="article-nav-title">
                
                    强化学习（三）：动态规划
                
            </div>
        </a>
    
    
        <a href="/posts/introduction-to-rl/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">下一篇</strong>
            <div class="article-nav-title">强化学习（一）：强化学习简介</div>
        </a>
    
</nav>


    
</article>


    
    
        <section id="comments">
    <div id="gitalkContainer"></div>
</section>
    

</section>
            
                
<aside id="sidebar">
   
        
<div class="toc-article" id="toc">
    <h3 class="widget-title" style="font-size:14px;"><i class="fa fa-list-alt" style="color: #888;font-size: 14px;margin-right: 10px;"></i>文章目录</h3>
    <div class="widget-toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫过程"><span class="toc-number">1.</span> <span class="toc-text">马尔可夫过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#马尔可夫决策过程简介"><span class="toc-number">1.1.</span> <span class="toc-text">马尔可夫决策过程简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#马尔可夫性质"><span class="toc-number">1.2.</span> <span class="toc-text">马尔可夫性质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#状态转移矩阵"><span class="toc-number">1.3.</span> <span class="toc-text">状态转移矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#马尔可夫过程-1"><span class="toc-number">1.4.</span> <span class="toc-text">马尔可夫过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫奖励过程"><span class="toc-number">2.</span> <span class="toc-text">马尔可夫奖励过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#回报"><span class="toc-number">2.1.</span> <span class="toc-text">回报</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#值函数"><span class="toc-number">2.2.</span> <span class="toc-text">值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MRPs的贝尔曼方程"><span class="toc-number">2.3.</span> <span class="toc-text">MRPs的贝尔曼方程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#马尔可夫决策过程"><span class="toc-number">3.</span> <span class="toc-text">马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#策略"><span class="toc-number">3.1.</span> <span class="toc-text">策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#值函数-1"><span class="toc-number">3.2.</span> <span class="toc-text">值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝尔曼期望方程"><span class="toc-number">3.3.</span> <span class="toc-text">贝尔曼期望方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优值函数"><span class="toc-number">3.4.</span> <span class="toc-text">最优值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最优策略"><span class="toc-number">3.5.</span> <span class="toc-text">最优策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝尔曼最优方程"><span class="toc-number">3.6.</span> <span class="toc-text">贝尔曼最优方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝尔曼最优方程的求解"><span class="toc-number">3.7.</span> <span class="toc-text">贝尔曼最优方程的求解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MDP的扩展"><span class="toc-number">4.</span> <span class="toc-text">MDP的扩展</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-number">5.</span> <span class="toc-text">参考资料</span></a></li></ol>
    </div>
</div>

    
        

    
        


    
</aside>
<div id="toTop" class="fa fa-angle-up"></div>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2019 orzyt<br>
            Powered by <a rel="”nofollow”" href="http://hexo.io/" target="_blank">Hexo</a> | Hosted by <a rel="”nofollow”" href="https://pages.coding.me">Coding Pages</a>
        </div>
    </div>
</footer>
        
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
	<script>
        var gitalk = new Gitalk({
            id: location.pathname,
            clientID: '3628f46ff8965c297258', 
            clientSecret: 'da2ef51d9fe687db577e834867d253a697f19a9f',
            repo: 'comments',
            owner: 'orzyt',
            admin: ['orzyt'],
            distractionFreeMode: false,
            perPage: 20,
            createIssueManually: false,
            language: 'zh-CN', 
        })
        gitalk.render('gitalkContainer')
	</script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML.js"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
    <!-- page.__post !== true --><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>

