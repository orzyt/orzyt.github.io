<!DOCTYPE html>
<html lang="zh">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    <title>Hands-on Machine learning 之 TensorFLow入门 - 扬涛的博客</title>
    <meta itemprop="name" content="Hands-on Machine learning 之 TensorFLow入门 - 扬涛的博客">
    
    <meta name="keywords" content="扬涛的博客, 郑扬涛, orzyt, leetcode">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    
    <link rel="alternate" href="/atom.xml" title="扬涛的博客" type="application/atom+xml">
    
    
    <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" href="/libs/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="/libs/headroom/animate.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/bootstrap/js/bootstrap.min.js"></script>
    <script src="/libs/headroom/headroom.min.js"></script>
    <script src="/libs/headroom/jQuery.headroom.min.js"></script>
    <script src="/libs/jquery/sort.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?63f226f2212285cc30d6ffa0636da07a";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    


</head>
</html>
<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">扬涛的博客</span>
            </a>
            <nav id="main-nav">
                <div>
                    <ul class="nav navbar-nav">
                    
                        <a class="main-nav-link" href="/.">主页</a>
                    
                        <a class="main-nav-link" href="/archives">归档</a>
                    
                        <a class="main-nav-link" href="/tags">标签</a>
                    
                        <a class="main-nav-link" href="/about">关于</a>
                    
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
                            更多 
                            <b class="caret"></b>
                        </a>
                        <ul class="dropdown-menu">
                            
                                <li><a href="/study">学习资源汇总</a></li>
                            
                        </ul>
                    </li>
                    </ul>
                </div>
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/img/avatar.png">
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <ul class="menu outer">
            
                <a class="main-nav-link" href="/.">主页</a>
            
                <a class="main-nav-link" href="/archives">归档</a>
            
                <a class="main-nav-link" href="/tags">标签</a>
            
                <a class="main-nav-link" href="/about">关于</a>
                           
            <li class="dropdown">
                <a href="#" class="dropdown-toggle" data-toggle="dropdown">
                    更多 
                    <b class="caret"></b>
                </a>
                <ul class="dropdown-menu" style="line-height:20px;min-width:120px;">
                    
                        <li><a href="/study" style="line-height:20px;padding:2px 2px;">学习资源汇总</a></li>
                    
                </ul>
            </li>
        </ul>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/img/avatar.png">
            <h2 id="name">郑扬涛</h2>
            <h3 id="title">废材码农 @ Beihang Univ.</h3>
            <span id="location"><i class="fa fa-map-marker"></i>Beijing, China</span>
            <a id="follow" target="_blank" href="https://www.zhihu.com/people/orzyt/activities">关注我</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                138
                <span>文章</span>
            </div>
            <div class="article-info-block">
                58
                <span>标签</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="https://github.com/orzyt" target="_blank" title="github" class="tooltip">
                            <i class="fa fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="https://www.facebook.com/orzyt" target="_blank" title="facebook" class="tooltip">
                            <i class="fa fa-facebook"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="https://weibo.com/orzyt" target="_blank" title="weibo" class="tooltip">
                            <i class="fa fa-weibo"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/atom.xml" target="_blank" title="rss" class="tooltip">
                            <i class="fa fa-rss"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main"><article id="post-hands-on-ml-up-and-running-tensorflow" class="article article-type-post" itemscope="" itemprop="blogPost">
    <div class="article-inner">
        
            
	
		<img src="https://tuchuang001.com/images/2017/11/09/banner.png" class="article-banner">
	



        
        
            <header class="article-header">
                
    
    
        
            <h1 class="article-title" itemprop="name">
                Hands-on Machine learning 之 TensorFLow入门
            </h1>
        
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/posts/hands-on-ml-up-and-running-tensorflow/">
            <time datetime="2017-12-01T16:00:00.000Z" itemprop="datePublished">2017-12-02</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/TensorFlow/">TensorFlow</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/TensorFlow/">TensorFlow</a>, <a class="tag-link" href="/tags/hands-on-ML/">hands-on ML</a>, <a class="tag-link" href="/tags/机器学习/">机器学习</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
            
                
                <div id="toc" class="toc-article">
                    <strong class="toc-title">文章目录</strong>
                    
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#简介"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建图并在会话中运行"><span class="toc-number">2.</span> <span class="toc-text">创建图并在会话中运行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#管理计算图"><span class="toc-number">3.</span> <span class="toc-text">管理计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#节点的生命周期"><span class="toc-number">4.</span> <span class="toc-text">节点的生命周期</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用TensorFlow进行线性回归"><span class="toc-number">5.</span> <span class="toc-text">使用TensorFlow进行线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现梯度下降"><span class="toc-number">6.</span> <span class="toc-text">实现梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#手动计算梯度"><span class="toc-number">6.1.</span> <span class="toc-text">手动计算梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用autodiff"><span class="toc-number">6.2.</span> <span class="toc-text">使用autodiff</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用优化器"><span class="toc-number">6.3.</span> <span class="toc-text">使用优化器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为训练算法提供数据"><span class="toc-number">7.</span> <span class="toc-text">为训练算法提供数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型的存储及读取"><span class="toc-number">8.</span> <span class="toc-text">模型的存储及读取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#存储模型"><span class="toc-number">8.1.</span> <span class="toc-text">存储模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#读取模型"><span class="toc-number">8.2.</span> <span class="toc-text">读取模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用TensorBoard进行可视化"><span class="toc-number">9.</span> <span class="toc-text">使用TensorBoard进行可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#命名域"><span class="toc-number">10.</span> <span class="toc-text">命名域</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模块化"><span class="toc-number">11.</span> <span class="toc-text">模块化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#共享变量"><span class="toc-number">12.</span> <span class="toc-text">共享变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#复用ReLU的阈值"><span class="toc-number">12.1.</span> <span class="toc-text">复用ReLU的阈值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#复用CNN卷积层参数"><span class="toc-number">12.2.</span> <span class="toc-text">复用CNN卷积层参数</span></a></li></ol></li></ol>
                    

                </div>
                
                
                    <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><img src="https://blog.rescale.com/wp-content/uploads/2017/02/markblogtensorflow.png" alt=""></p>
<p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener"><strong>TensorFlow</strong></a>是Google在2015年11月份开源的人工智能系统，是之前所开发的深度学习基础架构<code>DistBelief</code>的改进版本，该系统可以被用于语音识别、图像识别等多个领域。本文将介绍<code>TensorFlow</code>的基本概念和常见用法。</p>
<a id="more"></a>
<hr>
<h2 id="创建图并在会话中运行"><a href="#创建图并在会话中运行" class="headerlink" title="创建图并在会话中运行"></a>创建图并在会话中运行</h2><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_036.png" alt="一个简单的数据流图"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入tensorflow</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量x，y及计算节点f</span></span><br><span class="line">x = tf.Variable(<span class="number">3</span>, name=<span class="string">'x'</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4</span>, name=<span class="string">'y'</span>)</span><br><span class="line">f = x * x * y + y + <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>值得注意的是，上述代码并没有进行任何的运算，仅仅是创建了一个<code>计算图</code>（computation graph）而已。实际上，连变量都还没有被初始化。</p>
<p>为了对该计算图进行运算，我们必须创建一个<code>会话</code>（session）。然后在会话中初始化变量，以及计算<code>f</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session() <span class="comment"># 创建会话</span></span><br><span class="line">sess.run(x.initializer) <span class="comment"># 初始化变量x</span></span><br><span class="line">sess.run(y.initializer) <span class="comment"># 初始化变量y</span></span><br><span class="line">result = sess.run(f) <span class="comment"># 计算f</span></span><br><span class="line">print(result) <span class="comment"># 打印结果</span></span><br><span class="line">&gt;&gt; <span class="number">42</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.close() <span class="comment"># 关闭会话</span></span><br></pre></td></tr></table></figure>
<p>重复使用<code>sess.run(...)</code>可能有点繁琐，有一个更好的方式是使用python的<a href="https://docs.python.org/3/reference/compound_stmts.html#the-with-statement" target="_blank" rel="noopener"><code>with</code></a>语句。</p>
<p>在<code>with</code>语句块开始时，创建的会话会成为计算图的默认会话。在语句块结束时，创建的会话也会自动结束。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个会话并命名为sess</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x.initializer.run() <span class="comment"># 等同于 sess.run(x.initializer)</span></span><br><span class="line">    y.initializer.run() <span class="comment"># 等同于 sess.run(y.initializer)</span></span><br><span class="line">    result = f.eval() <span class="comment"># result = sess.run(f)</span></span><br></pre></td></tr></table></figure>
<p>除了手动初始化每个变量，也可以使用<code>global_variables_initializer()</code>函数初始化所有变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意，并没有立即将变量初始化，而是创建一个初始化节点</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run() <span class="comment"># 在这里才真正初始化</span></span><br><span class="line">    result = f.eval()</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="管理计算图"><a href="#管理计算图" class="headerlink" title="管理计算图"></a>管理计算图</h2><p>所有创建的<code>节点</code>（node）都会自动添加进默认图中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 判断变量x1所在的图是否是默认图</span></span><br><span class="line">x1.graph <span class="keyword">is</span> tf.get_default_graph()</span><br><span class="line">&gt;&gt; <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<p>但是，有的时候需要管理多个独立的图。我们便可以创建一个临时的图，并在<code>with</code>语句块内将其设置为默认图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph = tf.Graph() <span class="comment"># 创建图</span></span><br><span class="line"><span class="comment"># 在with内将graph设置为默认图</span></span><br><span class="line"><span class="keyword">with</span> graph.as_default(): </span><br><span class="line">    <span class="comment"># 此时创建的变量应该在图graph里</span></span><br><span class="line">    x2 = tf.Variable(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断x2所在的图，是不是真的在graph里</span></span><br><span class="line">x2.graph <span class="keyword">is</span> graph</span><br><span class="line">&gt;&gt; <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同样，判断x2是不是在全局的默认图里(显然不是的)</span></span><br><span class="line">x2.graph <span class="keyword">is</span> tf.get_default_graph()</span><br><span class="line">&gt;&gt; <span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<p>若想要将默认图重置（删除图中所有的节点），可以使用<code>tf.reset_default_graph()</code>函数。</p>
<hr>
<h2 id="节点的生命周期"><a href="#节点的生命周期" class="headerlink" title="节点的生命周期"></a>节点的生命周期</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个简单的图</span></span><br><span class="line">w = tf.constant(<span class="number">3</span>)</span><br><span class="line">x = w + <span class="number">2</span></span><br><span class="line">y = x + <span class="number">5</span></span><br><span class="line">z = x * <span class="number">3</span></span><br><span class="line"><span class="comment"># 在会话中计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(y.eval()) <span class="comment"># 10</span></span><br><span class="line">    print(z.eval()) <span class="comment"># 15</span></span><br></pre></td></tr></table></figure>
<p>在会话中，为了计算<code>y</code>，会自动检测出<code>y</code>依赖于<code>x</code>，而<code>x</code>又依赖于<code>w</code>。那么，将会依次计算<code>w</code>和<code>x</code>，最后再计算<code>y</code>。而为了计算<code>z</code>，也同样会依次计算<code>w</code>和<code>x</code>。</p>
<p>注意在此过程中，<code>w</code>和<code>x</code>的值<strong>并不会被重复使用</strong>！也就是说，上述代码总共对<code>w</code>和<code>x</code>计算了两次(即使两次的结果都是一样的)。</p>
<p>因此，对于同一张图的多次运算，除了<code>tf.Variable()</code>变量外，其他节点的值在一次运行结束后都会被丢弃，不会被重复使用。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">节点类型</th>
<th style="text-align:center">生命周期</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>tf.Variable()</code></td>
<td style="text-align:center">整个会话</td>
</tr>
<tr>
<td style="text-align:center"><code>others</code></td>
<td style="text-align:center">会话的某次运行</td>
</tr>
</tbody>
</table>
</div>
<p>为了高效地求得<code>y</code>和<code>z</code>的值，可以在会话的一次运行内同时计算它们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 同时计算y和z，此时w和x只计算一次</span></span><br><span class="line">    y_val, z_val = sess.run([y, z])</span><br><span class="line">    print(y_val) <span class="comment"># 10</span></span><br><span class="line">    print(z_val) <span class="comment"># 15</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="使用TensorFlow进行线性回归"><a href="#使用TensorFlow进行线性回归" class="headerlink" title="使用TensorFlow进行线性回归"></a>使用TensorFlow进行线性回归</h2><p>TensorFlow的操作（operations，记作ops）可以接收任意多个输入，可以产生任意多个输出。</p>
<p>比如<code>addition</code>和<code>multiplication</code>可以接受2个输入，产生1个输出。<br>而被成为<strong>源操作</strong>（source ops）的<code>Constants</code>和<code>variables</code>，则没有输入。</p>
<p>其中，输入和输出都是多维数组，称之为<strong>张量</strong>（tensor）。</p>
<hr>
<p>接下来，将使用<a href="http://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">sklearn</a>的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html" target="_blank" rel="noopener">加利福尼亚房屋数据</a>来进行线性回归。</p>
<p>对于线性回归参数<code>theta</code>的拟合，将使用正规方程（Normal Equation）计算：$\theta = (X^T X)^{-1}X^T y$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入sklearn自带的加利福尼亚房屋数据</span></span><br><span class="line">housing = fetch_california_housing()</span><br><span class="line"><span class="comment"># 样本数及特征数</span></span><br><span class="line">m, n = housing.data.shape</span><br><span class="line"><span class="comment"># 添加bias</span></span><br><span class="line">housing_data_plus_bias = np.c_[np.ones((m, <span class="number">1</span>)), housing.data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的常量节点X和y，分别用来存放样本和标签</span></span><br><span class="line">X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.constant(housing.target.reshape(<span class="number">-1</span>, <span class="number">1</span>), dtype=tf.float32, name=<span class="string">'y'</span>)</span><br><span class="line"><span class="comment"># 计算X的转置</span></span><br><span class="line">XT = tf.transpose(X)</span><br><span class="line"><span class="comment"># 使用正规方程计算theta</span></span><br><span class="line">theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次注意，上述代码并没有进行实际的运算，只是在构建计算图而已</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 在会话中计算theta的值</span></span><br><span class="line">    theta_value = theta.eval()</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="实现梯度下降"><a href="#实现梯度下降" class="headerlink" title="实现梯度下降"></a>实现梯度下降</h2><p>接下来将使用<strong>批梯度下降</strong>（Batch Gradient Descent）方法来进行线性回归参数的拟合。</p>
<p>使用梯度下降方法一般要先对特征进行<strong>标准化</strong>（normalize，即减均值，除方差）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaled_housing_data = scaler.fit_transform(housing.data)</span><br><span class="line">scaled_housing_data_plus_bias = np.c_[np.ones((m, <span class="number">1</span>)), scaled_housing_data]</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="手动计算梯度"><a href="#手动计算梯度" class="headerlink" title="手动计算梯度"></a>手动计算梯度</h3><p>$\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})$<br>其中，$\alpha$是学习率，$m$是批样本数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">1000</span> <span class="comment"># 遍历1000次数据集</span></span><br><span class="line">learning_rate = <span class="number">0.01</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的常量节点X和y，分别用来存放样本和标签</span></span><br><span class="line"><span class="comment"># 特征已经过normalize，并加上了bias</span></span><br><span class="line">X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.constant(housing.target.reshape(<span class="number">-1</span>, <span class="number">1</span>), dtype=tf.float32, name=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的变量节点theta，用来存放待求解的参数（使用均匀分布初始化节点）</span></span><br><span class="line">theta = tf.Variable(tf.random_uniform([n + <span class="number">1</span>, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'theta'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建预测节点</span></span><br><span class="line">y_pred = tf.matmul(X, theta, name=<span class="string">'predictions'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建误差节点</span></span><br><span class="line">error = y_pred - y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数节点，使用均方根误差（mean square error）</span></span><br><span class="line">mse = tf.reduce_mean(tf.square(error), name=<span class="string">'mse'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建梯度计算节点</span></span><br><span class="line">gradients = <span class="number">1</span> / m * tf.matmul(tf.transpose(X), error)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建赋值节点，即 theta = theta - learning_rate * gradients</span></span><br><span class="line">training_op = tf.assign(theta, theta - learning_rate * gradients)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量初始化节点</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="comment"># 每100个epoch打印当前的loss值</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch'</span>, epoch, <span class="string">'MSE='</span>, mse.eval())</span><br><span class="line">        <span class="comment"># 进行梯度下降更新参数theta</span></span><br><span class="line">        sess.run(training_op)</span><br><span class="line">    <span class="comment"># 训练完成后，计算最终的theta参数值</span></span><br><span class="line">    best_theta = theta.eval()</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="使用autodiff"><a href="#使用autodiff" class="headerlink" title="使用autodiff"></a>使用autodiff</h3><p>在前面的代码中，需要我们事先手动计算好loss function（MSE）的梯度才能进行训练。<br>虽然在线性回归里面求解梯度还不算复杂，但是对于深度神经网络来说，梯度的求解将会让人十分头疼。</p>
<p>在TensorFlow中，提供了<code>autodiff</code>能够帮助我们自动计算梯度。</p>
<p>只需将之前的梯度计算替换为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradients = tf.gradients(mse, [theta])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p><code>gradients()</code>函数接受一个操作节点（如，<code>mse</code>损失函数计算节点），以及一系列需要求解梯度的变量（如，<code>theta</code>），最终返回对应的梯度列表。</p>
<hr>
<h3 id="使用优化器"><a href="#使用优化器" class="headerlink" title="使用优化器"></a>使用优化器</h3><p>TensorFlow能够自动计算梯度已经很方便了，但是可以将事情变得更加简单——使用优化器。</p>
<p>比如，使用<strong>梯度下降优化器</strong>（Gardient Descent optimizer）。</p>
<p>便可以简单地将之前<code>gradients = ...</code>和<code>training_op = ...</code>直接替换成下列方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(mse)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="为训练算法提供数据"><a href="#为训练算法提供数据" class="headerlink" title="为训练算法提供数据"></a>为训练算法提供数据</h2><p>首先，我们将之前的批梯度下降算法改为<strong>小批量梯度下降算法</strong>（Mini-batch Gradient Descent）。<br>那么对于每个batch，我们都需要不断地替换<code>X</code>和<code>y</code>常量节点，来向训练算法提供数据。</p>
<p>在TensorFlow中，一个典型的做法是使用<code>placeholder</code><strong>占位符节点</strong>。</p>
<p>占位符节点在创建的时候，只需指定其存放的数据类型（如，floa32等），以及存放的数据维度大小即可。<br>然后，等到训练运行时才真正往占位符里放数据（使用<code>feed_dict</code>参数放数据）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># None 意味着在该维度不限制大小</span></span><br><span class="line">A = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">3</span>))</span><br><span class="line">B = A + <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 由于B依赖于A，因此在对B求值的时候，必须先用feed_dict向占位符A提供数据</span></span><br><span class="line">    B_val_1 = B.eval(feed_dict=&#123;A: [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]&#125;)</span><br><span class="line">    <span class="comment"># 占位符A的第0维度可以是任意大小</span></span><br><span class="line">    B_val_2 = B.eval(feed_dict=&#123;A: [[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(B_val_1)</span><br><span class="line">&gt;&gt; [[ <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>]]</span><br><span class="line"></span><br><span class="line">print(B_val_2)</span><br><span class="line">&gt;&gt; [[  <span class="number">9.</span>  <span class="number">10.</span>  <span class="number">11.</span>]</span><br><span class="line">    [ <span class="number">12.</span>  <span class="number">13.</span>  <span class="number">14.</span>]]</span><br></pre></td></tr></table></figure>
<hr>
<p>使用<code>placeholder</code>实现小批量梯度下降算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">10</span> <span class="comment"># 遍历10次数据集</span></span><br><span class="line">learning_rate = <span class="number">0.01</span> <span class="comment"># 学习率</span></span><br><span class="line">batch_size = <span class="number">100</span> <span class="comment"># 批数据大小</span></span><br><span class="line">n_batches = int(np.ceil(m / batch_size)) <span class="comment"># 完成一次epoch所需要的批次数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_batch</span><span class="params">(epoch, batch_index, batch_size)</span>:</span></span><br><span class="line">    <span class="string">''' 获取批数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param epoch: 当前epoch的次数</span></span><br><span class="line"><span class="string">    :param batch_index: 当前batch的序号</span></span><br><span class="line"><span class="string">    :param batch_size: 每个batch的大小</span></span><br><span class="line"><span class="string">    :return: 样本和标签的批数据</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># seed的种子设为：当前batch的总序号</span></span><br><span class="line">    <span class="comment"># 比如，完成一个epoch需要5个batch</span></span><br><span class="line">    <span class="comment"># 当前是第3个epoch的第2个batch</span></span><br><span class="line">    <span class="comment"># 那么这个batch总序号就是3 × 5 + 2 = 17</span></span><br><span class="line">    np.random.seed(epoch * n_batches + batch_index)</span><br><span class="line">    <span class="comment"># 在[0, m)范围内，产生batch_size个索引</span></span><br><span class="line">    indices = np.random.randint(m, size=batch_size)</span><br><span class="line">    <span class="comment"># 根据索引，获取样本的批数据</span></span><br><span class="line">    X_batch = scaled_housing_data_plus_bias[indices]</span><br><span class="line">    <span class="comment"># 根据索引，获取标签的批数据</span></span><br><span class="line">    y_batch = housing.target.reshape(<span class="number">-1</span>, <span class="number">1</span>)[indices]</span><br><span class="line">    <span class="keyword">return</span> X_batch, y_batch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建TensorFlow的占位符节点X和y，分别用来存放样本和标签</span></span><br><span class="line"><span class="comment"># 第0维度为None则不指定大小，因为样本数需要在运行时确定</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n + <span class="number">1</span>), name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的变量节点theta，用来存放待求解的参数（使用均匀分布初始化节点）</span></span><br><span class="line">theta = tf.Variable(tf.random_uniform([n + <span class="number">1</span>, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>, seed=<span class="number">42</span>), name=<span class="string">'thete'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建预测节点</span></span><br><span class="line">y_pred = tf.matmul(X, theta, name=<span class="string">'predictions'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建误差节点</span></span><br><span class="line">error = y_pred - y</span><br><span class="line"></span><br><span class="line">＃ 创建损失函数节点，使用均方根误差（mean square error）</span><br><span class="line">mse = tf.reduce_mean(tf.square(error), name=<span class="string">'mse'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建梯度下降优化器节点 及 对应的训练节点</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量初始化节点</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init) </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_index <span class="keyword">in</span> range(n_batches):</span><br><span class="line">            <span class="comment"># 获取批数据</span></span><br><span class="line">            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)</span><br><span class="line">            <span class="comment"># 由于训练节点依赖于X和y，因此使用feed_dict参数传送一个字典，分别为X和y提供数据</span></span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">    best_theta = theta.eval()</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="模型的存储及读取"><a href="#模型的存储及读取" class="headerlink" title="模型的存储及读取"></a>模型的存储及读取</h2><p>模型一旦训练好，那么应该将相关的信息（如，参数、计算图等）存储在硬盘里，方便以后的使用。<br>同样，在模型的训练过程中，应该定时地保存检查点（checkpoint）。这使得模型训练中断之后，可以直接从最后一次检查点开始训练而不是重头开始。</p>
<hr>
<h3 id="存储模型"><a href="#存储模型" class="headerlink" title="存储模型"></a>存储模型</h3><p>只需在计算图<strong>构造阶段</strong>（construction phase）的最后新建一个<code>Saver</code>保存节点即可。<br>然后在<strong>执行阶段</strong>（execution phase），调用其<code>save(sess, path)</code>方法即可将模型保存到<code>path</code>路径下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[...]</span><br><span class="line">theta = tf.Variable(tf.random_uniform([n + <span class="number">1</span>, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">"theta"</span>)</span><br><span class="line">[...]</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 在construction phase之后创建Saver node即可</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 每100个epoch保存checkpoint</span></span><br><span class="line">            save_path = saver.save(sess, <span class="string">'tmp/my_model.ckpt'</span>)</span><br><span class="line">        sess.run(training_op)</span><br><span class="line">    best_theta = theta.eval()</span><br><span class="line">    <span class="comment"># 训练完毕，保存最终模型</span></span><br><span class="line">    save_path = saver.save(sess, <span class="string">'tmp/my_model_final.ckpt'</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h3><p>首先，还是在构造阶段的最后新建一个<code>Saver</code>保存节点。<br>然后，在执行阶段的开始，调用其<code>restore(sess, path)</code>方法即可将path路径下的模型读取到当前的会话中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'tmp/my_model_final.ckpt'</span>)</span><br><span class="line">    [...]</span><br></pre></td></tr></table></figure>
<hr>
<p><code>Saver</code>默认情况下，会存储和读取模型的所有参数。<br>不过，我们也可以指定只保存哪些变量，以及使用什么名字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只保存theta变量，命名为weights</span></span><br><span class="line">saver = tf.train.Saver(&#123;<span class="string">"weights"</span>: theta&#125;)</span><br></pre></td></tr></table></figure>
<p><code>Saver</code>默认情况下，也会存储计算图的结构，保存在路径的<code>*.meta</code>文件中。<br>如果需要读取模型的计算图，可以调用<code>tf.train.import_meta_graph()</code>函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取模型的计算图</span></span><br><span class="line"><span class="comment"># 这样可以完整地恢复模型，不仅包括模型的参数，还包括模型的计算图结构</span></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"/tmp/my_model_final.ckpt.meta"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"/tmp/my_model_final.ckpt"</span>)</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="使用TensorBoard进行可视化"><a href="#使用TensorBoard进行可视化" class="headerlink" title="使用TensorBoard进行可视化"></a>使用TensorBoard进行可视化</h2><p>在此之前，我们都是用<code>print</code>函数打印出训练过程。然而，有一个更好的选择是：使用<code>TensorBoard</code>！</p>
<p>接下来，我们将对线性回归的loss值进行可视化。</p>
<hr>
<p>首先，新建一个存放数据的日志目录（使用时间戳作为目录名）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以指定格式获取当前时间</span></span><br><span class="line">now = datetime.utcnow().strftime(<span class="string">'%Y%m%d%H%M%S'</span>)</span><br><span class="line"><span class="comment"># 根目录</span></span><br><span class="line">root_logdir = <span class="string">'tf_logs'</span></span><br><span class="line"><span class="comment"># 日志文件目录</span></span><br><span class="line">logdir = <span class="string">'&#123;&#125;/run-&#123;&#125;/'</span>.format(root_logdir, now)</span><br></pre></td></tr></table></figure>
<p>其次，在构造阶段的末尾添加以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># summary是TensorBoard的一种二进制日志字符串</span></span><br><span class="line"><span class="comment"># 我们使用它的scalar标量类型（还有其他类型，如tf.summary.image可以可视化图像）</span></span><br><span class="line"><span class="comment"># 参数'MSE'：可视化时变量的名称</span></span><br><span class="line"><span class="comment"># 参数mse：loss function节点</span></span><br><span class="line">mse_summary = tf.summary.scalar(<span class="string">'MSE'</span>, mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FileWriter可以将summaries写进指定的日志文件中</span></span><br><span class="line"><span class="comment"># 参数logdir：指定的日志文件路径</span></span><br><span class="line"><span class="comment"># 参数tf.get_default_graph()：需要可视化的计算图结构</span></span><br><span class="line">file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())</span><br></pre></td></tr></table></figure>
<p>最后，在执行阶段添加以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[...]</span><br><span class="line"><span class="keyword">for</span> batch_index <span class="keyword">in</span> range(n_batches):</span><br><span class="line">    X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)</span><br><span class="line">    <span class="keyword">if</span> batch_index % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 计算mse_summary的值</span></span><br><span class="line">        summary_str = mse_summary.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        step = epoch * n_batches + batch_index</span><br><span class="line">        <span class="comment"># 将summary添加到日志文件中，同时需要指定当前的step（也就是可视化时的横轴坐标）</span></span><br><span class="line">        file_writer.add_summary(summary_str, step)</span><br><span class="line">    sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">[...]</span><br><span class="line"><span class="comment"># 关闭FileWriter</span></span><br><span class="line">file_writer.close()</span><br></pre></td></tr></table></figure>
<p>现在，让我们启动<code>TensorBoard</code>！</p>
<p>首先，在终端中输入:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ tensorboard --logdir tf_logs/</span><br><span class="line">&gt;&gt; Starting TensorBoard on port 6006</span><br><span class="line">   (You can navigate to http://0.0.0.0:6006)</span><br></pre></td></tr></table></figure></p>
<p>然后，在浏览器中打开<code>http://0.0.0.0:6006/</code> (或<code>http://localhost:6006/</code>)，即可访问TensorBoard！</p>
<p><img src="https://tuchuang001.com/images/2017/12/02/Selection_037.png" alt="可视化loss值"></p>
<p><img src="https://tuchuang001.com/images/2017/12/02/Selection_038.png" alt="可视化计算图"></p>
<hr>
<h2 id="命名域"><a href="#命名域" class="headerlink" title="命名域"></a>命名域</h2><p>当处理复杂模型的时候，图中可能有成千上万个节点。因此，非常有必要将相关的节点组织起来放到一起。<br>这就需要TensorFlow中的<strong>命名域</strong>（Name Scopes）来管理节点！</p>
<p>比如，我们可以将之前代码里的<code>error</code>和<code>mse</code>操作节点放在一个名叫<code>loss</code>的命名域里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    error = y_pred - y</span><br><span class="line">    mse = tf.reduce_mean(tf.square(error), name=<span class="string">'mse'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(error.op.name)</span><br><span class="line">&gt;&gt; loss/sub</span><br><span class="line">print(mse.op.name)</span><br><span class="line">&gt;&gt; loss/mse</span><br></pre></td></tr></table></figure>
<p>在TensorBoard中，error和mse将出现在loss命名域内。</p>
<p><img src="https://tuchuang001.com/images/2017/12/02/Selection_039.png" alt="TensorBoard中的命名域"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a1 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line">a2 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"param"</span>): </span><br><span class="line">    a3 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"param"</span>):     </span><br><span class="line">    a4 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> (a1, a2, a3, a4):</span><br><span class="line">    print(node.op.name)</span><br><span class="line">    </span><br><span class="line">&gt;&gt; a</span><br><span class="line">   a_1</span><br><span class="line">   param/a</span><br><span class="line">   param_1/a</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="模块化"><a href="#模块化" class="headerlink" title="模块化"></a>模块化</h2><p>假设现在要对多个<code>ReLU</code>（rectified linear units，修正线性单元）输出进行累加。</p>
<script type="math/tex; mode=display">ReLU_{x,b}(X) = max(X \cdot w + b, 0 )</script><p>由于要计算多次ReLU，所以基于模块化的思想，我们可以将实现ReLU功能的语句单独封装成一个函数以供调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">''' 实现ReLU</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param X: 输入样本</span></span><br><span class="line"><span class="string">    :return: 经过ReLU修正后的输出</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w_shape = (int(X.get_shape()[<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">    w = tf.Variable(tf.random_normal(w_shape), name=<span class="string">'weights'</span>)</span><br><span class="line">    b = tf.Variable(<span class="number">0.0</span>, name=<span class="string">'bias'</span>)</span><br><span class="line">    z = tf.add(tf.matmul(X, w), b, name=<span class="string">'z'</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.maximum(z, <span class="number">0.0</span>, name=<span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_features = <span class="number">3</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n_features), name=<span class="string">'X'</span>)</span><br><span class="line"><span class="comment"># 调用5次relu函数</span></span><br><span class="line">relus = [relu(X) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line"><span class="comment"># 将5次的结果累加</span></span><br><span class="line">output = tf.add_n(relus, name=<span class="string">'output'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://tuchuang001.com/images/2017/12/02/Selection_040.png" alt="ReLU的计算图(good)"></p>
<hr>
<p>还可以做得更好…</p>
<p>我们将之前讲过的命名域加进来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">''' 实现ReLU</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param X: 输入样本</span></span><br><span class="line"><span class="string">    :return: 经过ReLU修正后的输出</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'relu'</span>):</span><br><span class="line">        [...]</span><br></pre></td></tr></table></figure>
<p><img src="https://tuchuang001.com/images/2017/12/02/Selection_041.png" alt="ReLU的计算图(better)"></p>
<hr>
<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>如果你想让计算图中不同的部分共享一个变量（比如CNN中卷积核的权值），一个可能的操作是将变量作为参数传递过去。但是，当计算图中需要共享的变量非常多时，将变得十分麻烦。</p>
<p>TensorFlow有一个更好的解决方案是，使用<code>get_variable()</code>函数来创建（或复用）共享变量。<br>而选择创建还是选择复用则是由当前的变量域<code>variable_scope()</code>决定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在relu变量域内创建threshold变量</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>):</span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>, shape=(), initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复用，设置reuse=True</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 另一种复用方式</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    scope.reuse_variables() <span class="comment"># 调用scope的reuse_variables方法</span></span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="复用ReLU的阈值"><a href="#复用ReLU的阈值" class="headerlink" title="复用ReLU的阈值"></a>复用ReLU的阈值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="comment"># 存在则复用，不存在则创建</span></span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>, shape=(), initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    [...]                   </span><br><span class="line">    <span class="keyword">return</span> tf.maximum(z, threshold, name=<span class="string">"max"</span>)</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n_features), name=<span class="string">'X'</span>)</span><br><span class="line">relus = []</span><br><span class="line"><span class="keyword">for</span> relu_index <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="comment"># 第一次调用relu时reuse为0，则不复用选择创建变量</span></span><br><span class="line">    <span class="comment"># 后续调用relu则会选择复用变量</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>, reuse=(relu_index &gt;= <span class="number">1</span>)) <span class="keyword">as</span> scope:</span><br><span class="line">        relus.append(relu(X))</span><br><span class="line">output = tf.add_n(relus, name=<span class="string">'output'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://tuchuang001.com/images/2017/12/02/Selection_042.png" alt="5个ReLUs共享threshold变量"></p>
<hr>
<h3 id="复用CNN卷积层参数"><a href="#复用CNN卷积层参数" class="headerlink" title="复用CNN卷积层参数"></a>复用CNN卷积层参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_images = tf.placeholder(tf.float32, shape = (<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义了一层卷积神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></span><br><span class="line">    <span class="comment"># 创建名为weights的变量</span></span><br><span class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape, initializer=tf.random_normal_initializer())</span><br><span class="line">    <span class="comment"># 创建名为biases的变量</span></span><br><span class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape, initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    conv = tf.nn.conv2d(input, weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</span><br><span class="line">        <span class="comment"># 在名为conv1的variable scope下调用一层神经网络，对应的参数名为</span></span><br><span class="line">        <span class="comment"># "conv1/weights", "conv1/biases"</span></span><br><span class="line">        relu1 = conv_relu(input_images, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</span><br><span class="line">        <span class="comment"># 在名为conv2的variable scope下调用一层神经网络，对应的参数名为</span></span><br><span class="line">        <span class="comment"># "conv2/weights", "conv2/biases"</span></span><br><span class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"image_filter"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    result1 = my_image_filter(input_images)</span><br><span class="line">    scope.reuse_variables() <span class="comment"># 复用变量</span></span><br><span class="line">    result2 = my_image_filter(input_images)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer();</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    image = np.random.rand(<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">    result1 = sess.run(result1, feed_dict=&#123;input_images: image&#125;)</span><br><span class="line">    result2 = sess.run(result2, feed_dict=&#123;input_images: image&#125;)</span><br><span class="line">    print(result2.all() == result1.all())</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="keyword">True</span> </span><br><span class="line"><span class="comment"># 说明第二次的参数没有重新初始化，而是复用了第一次的参数</span></span><br></pre></td></tr></table></figure>

                
            
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://orzyt.cn/posts/hands-on-ml-up-and-running-tensorflow/" data-id="cjobive7m00lqu1siqetiji7i" class="article-share-link"><i class="fa fa-share"></i>分享到</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fa fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fa fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fa fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fa fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    
        <a href="https://orzyt.cn/posts/hands-on-ml-up-and-running-tensorflow/#comments" class="article-comment-link">评论</a>
    

        </footer>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/posts/use-tensorflow-to-implement-logistic-regression/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">上一篇</strong>
            <div class="article-nav-title">
                
                    使用TensorFlow实现逻辑回归
                
            </div>
        </a>
    
    
        <a href="/posts/use-hexo-on-coding-webIDE/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">下一篇</strong>
            <div class="article-nav-title">使用WebIDE搭建Hexo云端写作环境</div>
        </a>
    
</nav>


    
</article>


    
    
        <section id="comments">
    <div id="gitalkContainer"></div>
</section>
    

</section>
            
                
<aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/教程/">教程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法与数据结构/">算法与数据结构</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/软件工程/">软件工程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/ACM/" style="font-size: 13.08px;">ACM</a> <a href="/tags/BFS/" style="font-size: 11.54px;">BFS</a> <a href="/tags/BST/" style="font-size: 11.54px;">BST</a> <a href="/tags/DFS/" style="font-size: 16.92px;">DFS</a> <a href="/tags/DP/" style="font-size: 13.85px;">DP</a> <a href="/tags/GAN/" style="font-size: 10px;">GAN</a> <a href="/tags/Gale-Shapley算法/" style="font-size: 10px;">Gale-Shapley算法</a> <a href="/tags/KMP/" style="font-size: 10px;">KMP</a> <a href="/tags/LeetCode/" style="font-size: 20px;">LeetCode</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Matlab/" style="font-size: 10px;">Matlab</a> <a href="/tags/SG函数/" style="font-size: 10px;">SG函数</a> <a href="/tags/TensorFlow/" style="font-size: 10.77px;">TensorFlow</a> <a href="/tags/anaconda/" style="font-size: 10px;">anaconda</a> <a href="/tags/cgan/" style="font-size: 10px;">cgan</a> <a href="/tags/conditional-gan/" style="font-size: 10px;">conditional gan</a> <a href="/tags/greedy/" style="font-size: 10px;">greedy</a> <a href="/tags/hands-on-ML/" style="font-size: 11.54px;">hands-on ML</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/jupyter-notebook/" style="font-size: 10px;">jupyter notebook</a> <a href="/tags/npm/" style="font-size: 10px;">npm</a> <a href="/tags/pip/" style="font-size: 10px;">pip</a> <a href="/tags/scp/" style="font-size: 10px;">scp</a> <a href="/tags/ssh/" style="font-size: 10.77px;">ssh</a> <a href="/tags/tensorboard/" style="font-size: 10px;">tensorboard</a> <a href="/tags/webIDE/" style="font-size: 10px;">webIDE</a> <a href="/tags/二分/" style="font-size: 16.15px;">二分</a> <a href="/tags/二叉搜索树/" style="font-size: 10px;">二叉搜索树</a> <a href="/tags/二叉树/" style="font-size: 10px;">二叉树</a> <a href="/tags/位运算/" style="font-size: 17.69px;">位运算</a> <a href="/tags/分治/" style="font-size: 10px;">分治</a> <a href="/tags/单调栈/" style="font-size: 10.77px;">单调栈</a> <a href="/tags/博弈论/" style="font-size: 10px;">博弈论</a> <a href="/tags/双指针/" style="font-size: 14.62px;">双指针</a> <a href="/tags/哈希/" style="font-size: 16.15px;">哈希</a> <a href="/tags/哈希表/" style="font-size: 10px;">哈希表</a> <a href="/tags/回文/" style="font-size: 10px;">回文</a> <a href="/tags/字典树/" style="font-size: 10px;">字典树</a> <a href="/tags/字符串/" style="font-size: 19.23px;">字符串</a> <a href="/tags/排序/" style="font-size: 12.31px;">排序</a> <a href="/tags/数学/" style="font-size: 19.23px;">数学</a> <a href="/tags/数组/" style="font-size: 18.46px;">数组</a> <a href="/tags/数论/" style="font-size: 10px;">数论</a> <a href="/tags/机器学习/" style="font-size: 15.38px;">机器学习</a> <a href="/tags/条件生成对抗网络/" style="font-size: 10px;">条件生成对抗网络</a> <a href="/tags/栈/" style="font-size: 11.54px;">栈</a> <a href="/tags/树/" style="font-size: 16.92px;">树</a> <a href="/tags/树状数组/" style="font-size: 10px;">树状数组</a> <a href="/tags/牛顿迭代法/" style="font-size: 10px;">牛顿迭代法</a> <a href="/tags/生成对抗网络/" style="font-size: 10px;">生成对抗网络</a> <a href="/tags/端口转发/" style="font-size: 10px;">端口转发</a> <a href="/tags/算法/" style="font-size: 12.31px;">算法</a> <a href="/tags/结对编程/" style="font-size: 10px;">结对编程</a> <a href="/tags/贪心/" style="font-size: 11.54px;">贪心</a> <a href="/tags/软件工程/" style="font-size: 12.31px;">软件工程</a> <a href="/tags/逻辑回归/" style="font-size: 10px;">逻辑回归</a> <a href="/tags/链表/" style="font-size: 13.08px;">链表</a> <a href="/tags/镜像源/" style="font-size: 10px;">镜像源</a>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">最新文章</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/posts/LeetCode575-Distribute-Candies/" class="title">LeetCode575 Distribute Candies</a></p>
                            <p class="item-date"><time datetime="2018-01-14T05:21:28.000Z" itemprop="datePublished">2018-01-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/posts/LeetCode566-Reshape-the-Matrix/" class="title">LeetCode566 Reshape the Matrix</a></p>
                            <p class="item-date"><time datetime="2018-01-14T05:03:19.000Z" itemprop="datePublished">2018-01-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/posts/LeetCode563-Binary-Tree-Tilt/" class="title">LeetCode563 Binary Tree Tilt</a></p>
                            <p class="item-date"><time datetime="2018-01-14T04:51:07.000Z" itemprop="datePublished">2018-01-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/posts/LeetCode561-Array-Partition-I/" class="title">LeetCode561 Array Partition I</a></p>
                            <p class="item-date"><time datetime="2018-01-14T04:31:32.000Z" itemprop="datePublished">2018-01-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-title"><a href="/posts/LeetCode557-Reverse-Words-in-a-String-III/" class="title">LeetCode557 Reverse Words in a String III</a></p>
                            <p class="item-date"><time datetime="2018-01-14T04:17:22.000Z" itemprop="datePublished">2018-01-14</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
</aside>
<div id="toTop" class="fa fa-angle-up"></div>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2018 orzyt<br>
            Powered by <a rel="”nofollow”" href="http://hexo.io/" target="_blank">Hexo</a> | Hosted by <a rel="”nofollow”" href="https://pages.coding.me">Coding Pages</a>
        </div>
    </div>
</footer>
        
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
	<script>
        var gitalk = new Gitalk({
            id: location.pathname,
            clientID: '3628f46ff8965c297258', 
            clientSecret: 'da2ef51d9fe687db577e834867d253a697f19a9f',
            repo: 'comments',
            owner: 'orzyt',
            admin: ['orzyt'],
            distractionFreeMode: false,
            perPage: 20,
            createIssueManually: false,
            language: 'zh-CN', 
        })
        gitalk.render('gitalkContainer')
	</script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML.js"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>

