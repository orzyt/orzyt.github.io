<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>扬涛的博客</title>
  
  <subtitle>上善若水·大道至简</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://orzyt.cn/"/>
  <updated>2019-03-13T14:11:50.480Z</updated>
  <id>https://orzyt.cn/</id>
  
  <author>
    <name>orzyt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>强化学习实践（二）：Gridworld</title>
    <link href="https://orzyt.cn/posts/gridworld/"/>
    <id>https://orzyt.cn/posts/gridworld/</id>
    <published>2019-03-13T11:31:33.000Z</published>
    <updated>2019-03-13T14:11:50.480Z</updated>
    
    <content type="html"><![CDATA[<hr><p>《Reinforcement Learning: An Introduction》在第三章中给出了一个简单的例子:<code>Gridworld</code>, 以帮助我们理解<code>finite MDPs</code>, 同时也求解了该问题的<strong>贝尔曼期望方程</strong>和<strong>贝尔曼最优方程</strong>. 本文简要说明如何进行编程求解.</p><a id="more"></a><hr><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><blockquote><p>下图用一个矩形网格展示了一个简单finite MDP - <code>Gridworld</code>.<br>网格中的每一格对应于environment的一个state.<br>在每一格, 有四种可能的actions：<code>上/下/左/右</code>, 对应于agent往相应的方向移动一个单元格.<br>使agent离开网格的actions会使得agent留在原来的位置, 但是会有一个值为<code>-1</code>的reward.<br>除了那些使得agent离开state A和state B的action, 其他的actions对应的reward都是<code>0</code>.<br>处在<code>state A</code>时, 所有的actions会有值为<code>+10</code>的reward, 并且agent会移动到<code>state A&#39;</code>.<br>处在<code>state B</code>时, 所有的actions会有值为<code>+5</code>的reward, 并且agent会移动到<code>state B&#39;</code>.</p></blockquote><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g11f0p6x8vj20al05yq2y.jpg" alt="Girdworld示意图" width="40%" height="40%"></p><hr><h2 id="元素"><a href="#元素" class="headerlink" title="元素"></a>元素</h2><ul><li><strong>状态(State)</strong>: 网格的坐标, 共 $5 \times 5 = 25$ 个状态;</li><li><strong>动作(Action)</strong>: <code>上/下/左/右</code>四种动作;</li><li><strong>策略(Policy)</strong>: $\pi(a | s) = \frac14 \; \text{for} \; \forall s \in S, \text{and} \; \forall \; a \in \{↑,↓,←,→\}$;</li><li><strong>奖励(Reward)</strong>: 如题所述;</li><li><strong>折扣因子(Discount rate)</strong>: $\gamma \in [0, 1]$, 本文采用 $\gamma=0.9$.</li></ul><hr><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul><li>使用贝尔曼期望方程, 求解给定随机策略 $\pi(a | s) = \frac14$ 下的状态值函数.</li><li>使用贝尔曼最优方程, 求解最优状态值函数.</li></ul><hr><h2 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h2><iframe src="../../src/gridworld.html" scrolling="no" width="100%" height="3888px"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;《Reinforcement Learning: An Introduction》在第三章中给出了一个简单的例子:&lt;code&gt;Gridworld&lt;/code&gt;, 以帮助我们理解&lt;code&gt;finite MDPs&lt;/code&gt;, 同时也求解了该问题的&lt;strong&gt;贝尔曼期望方程&lt;/strong&gt;和&lt;strong&gt;贝尔曼最优方程&lt;/strong&gt;. 本文简要说明如何进行编程求解.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>强化学习实践（一）：Tic-Tac-Toe</title>
    <link href="https://orzyt.cn/posts/tic-tac-toe/"/>
    <id>https://orzyt.cn/posts/tic-tac-toe/</id>
    <published>2019-03-08T10:55:54.000Z</published>
    <updated>2019-03-08T13:29:43.661Z</updated>
    
    <content type="html"><![CDATA[<hr><p>为了对强化学习的基本概念有一个直观的认识,《Reinforcement Learning: An Introduction》第一章给出了一个简单的例子：<code>Tic-Tac-Toe</code>游戏.</p><h2 id="游戏规则"><a href="#游戏规则" class="headerlink" title="游戏规则"></a>游戏规则</h2><p>游戏的规则很简单, 两位玩家在 <code>3x3</code> 的棋盘上轮流下棋, 一位打 <code>X</code>, 另一位打 <code>O</code>, 若棋盘的任意一行、任意一列、正反对角线上有三个相同的棋, 则执该棋的玩家获胜. 若棋盘下满仍没有决出胜负, 则平局.</p><a id="more"></a><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0vltwtubvj20bb0b574c.jpg" alt="Tic-Tac-Toe示例" width="25%" height="25%"></p><hr><p>我们尝试使用强化学习的方法来训练一个Agent, 使其能够在该游戏上表现出色(即Agent在任何情况下都不会输, 最多平局).</p><p>由于没有外部经验, 因此我们需要同时训练两个Agent进行上万轮的对弈来寻找最优策略.</p><p><strong>注:下面的代码只给出部分关键实现过程, 完整代码见:<a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter01/tic_tac_toe.py" target="_blank" rel="noopener">tic_tac_toe.py</a>.</strong> </p><p><strong>版权归 <a href="https://github.com/ShangtongZhang" target="_blank" rel="noopener">@Shangtong Zhang</a> 等人所有, 仅添加中文注释便于理解.</strong></p><hr><h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>强化学习一个重要的概念就是——<strong>状态(State)</strong>. </p><p>状态是指Agent在一个特定时刻从环境中所感知的信号. </p><p>在<code>Tic-Tac-Toe</code>游戏中, 状态即为 <code>3*3</code> 棋盘的布局. </p><p>定义一个<code>State类</code>用来表示状态.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">State</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''状态初始化</span></span><br><span class="line"><span class="string">        棋盘使用 n * n 的数组进行表示</span></span><br><span class="line"><span class="string">        棋盘中的数字: 1代表先手, -1代表后手下, 0代表该位置无棋子</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 该状态的数组表示</span></span><br><span class="line">        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))</span><br><span class="line">        <span class="comment"># 该状态下的胜利者</span></span><br><span class="line">        self.winner = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># 该状态的哈希值表示</span></span><br><span class="line">        self.state_hash = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># 该状态是否为终结状态</span></span><br><span class="line">        self.end = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''计算状态的哈希值表示</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        int</span></span><br><span class="line"><span class="string">            状态的哈希值表示</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_end</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''判断当前状态是否为终结状态.</span></span><br><span class="line"><span class="string">        如果为终结状态, 同时判断胜利者是谁</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bool</span></span><br><span class="line"><span class="string">            当前状态是否为终结状态</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_state</span><span class="params">(self, i, j, symbol)</span>:</span></span><br><span class="line">        <span class="string">'''计算当前状态的后继状态</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        i : int</span></span><br><span class="line"><span class="string">            下一步动作的行坐标</span></span><br><span class="line"><span class="string">        j : int</span></span><br><span class="line"><span class="string">            下一步动作的列坐标</span></span><br><span class="line"><span class="string">        symbol : int</span></span><br><span class="line"><span class="string">            动作的执行者(1代表先手, -1代表后手)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        State</span></span><br><span class="line"><span class="string">            下一步棋盘的状态</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_state</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''打印状态信息</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><hr><p>根据游戏规则我们知道, 每个格子仅有三种状态, 即先手(1), 后手(-1), 空(0), 那么该游戏的状态数上限仅有 $3^9=19683$ 个.</p><p>因此, 我们可以预处理出所有合法的棋盘状态, 供后面强化学习算法使用.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_states = get_all_states()</span><br></pre></td></tr></table></figure><hr><h2 id="AgentPlayer相关"><a href="#AgentPlayer相关" class="headerlink" title="AgentPlayer相关"></a>AgentPlayer相关</h2><p>定义一个<code>AgentPlayer类</code>用来表示强化学习中和环境进行交互的智能体.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AgentPlayer</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, step_size=<span class="number">0.1</span>, epsilon=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">'''Agent初始化</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        step_size : float, optional</span></span><br><span class="line"><span class="string">            更新步长</span></span><br><span class="line"><span class="string">        epsilon : float, optional</span></span><br><span class="line"><span class="string">            探索概率</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 值函数</span></span><br><span class="line">        self.value = dict()</span><br><span class="line">        <span class="comment"># 值函数更新步长</span></span><br><span class="line">        self.step_size = step_size</span><br><span class="line">        <span class="comment"># Agent探索概率</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        <span class="comment"># Agent在一轮游戏中经历的所有状态</span></span><br><span class="line">        self.states = []</span><br><span class="line">        <span class="comment"># 记录每个状态是否采取贪心策略</span></span><br><span class="line">        self.greedy = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''重置Agent的状态, 开启新一轮游戏</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">'''将当前棋盘状态加到Agent的状态列表</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        state : State</span></span><br><span class="line"><span class="string">            当前棋盘的状态</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">        <span class="string">'''根据先后手, 初始化Agent的值函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        symbol : int</span></span><br><span class="line"><span class="string">            先手还是后手</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''值函数迭代</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''根据状态采取动作</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        list</span></span><br><span class="line"><span class="string">            采取的动作</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_policy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''保存策略</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_policy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''加载策略</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><hr><p>在AgentPlayer类中, 我们重点关注 <code>set_symbol</code>, <code>backup</code>, <code>act</code> 三个函数.</p><h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>Agent每次与环境进行交互的时候, 都会得到一个反馈信号称之为<strong>奖励(Reward)</strong>. </p><p>Agent的目标是<strong>最大化游戏过程中的奖励总和</strong>.</p><p>在 <code>Tic-Tac-Toe</code> 游戏中, 由于只有在游戏结束的时候才知道胜负, 故没有给出每一步显式的奖励, 而是直接评估状态的<strong>值函数(Value Function)</strong>.</p><p>根据我们的先验知识, 可以对不同的状态设置不同的初始值函数.</p><p>对于导致游戏结束的终结状态, 可分为胜利/平局/失败三种情况, 相应的值函数为1.0/0.5/0.0.</p><p>而对于非终结状态, 可以简单地将状态的值函数设为0.5, 代表无法判断胜负.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">    <span class="string">'''根据先后手, 初始化Agent的值函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    symbol : int</span></span><br><span class="line"><span class="string">        先手还是后手</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    self.symbol = symbol</span><br><span class="line">    <span class="keyword">for</span> state_hash <span class="keyword">in</span> all_states.keys():</span><br><span class="line">        (state, is_end) = all_states[state_hash]</span><br><span class="line">        <span class="keyword">if</span> is_end: <span class="comment"># 终结状态</span></span><br><span class="line">            <span class="keyword">if</span> state.winner == self.symbol: <span class="comment"># 获胜</span></span><br><span class="line">                self.value[state_hash] = <span class="number">1.0</span></span><br><span class="line">            <span class="keyword">elif</span> state.winner == <span class="number">0</span>: <span class="comment"># 平局</span></span><br><span class="line">                self.value[state_hash] = <span class="number">0.5</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 失败</span></span><br><span class="line">                self.value[state_hash] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 非终结状态</span></span><br><span class="line">            self.value[state_hash] = <span class="number">0.5</span></span><br></pre></td></tr></table></figure><hr><h3 id="值函数迭代"><a href="#值函数迭代" class="headerlink" title="值函数迭代"></a>值函数迭代</h3><p>使用<strong>时序差分学习(Temporal-Difference Learning)</strong>方法进行值函数的更新:</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left[ V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right) \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''值函数迭代</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#　获取状态的哈希值表示</span></span><br><span class="line">    self.states = [state.hash() <span class="keyword">for</span> state <span class="keyword">in</span> self.states]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 逆序遍历所有的状态, 并进行值函数的更新</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(len(self.states) - <span class="number">1</span>)):</span><br><span class="line">        state = self.states[i]  </span><br><span class="line">        <span class="comment"># TD误差 = V(s_&#123;t + 1&#125;) - V(s_&#123;t&#125;)</span></span><br><span class="line">        td_error = self.greedy[i] * (self.value[self.states[i + <span class="number">1</span>]] - self.value[state])</span><br><span class="line">        <span class="comment"># TD-Learning(时序差分学习)更新公式</span></span><br><span class="line">        self.value[state] += self.step_size * td_error</span><br></pre></td></tr></table></figure><hr><h3 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h3><p>采用 $\epsilon$-greedy 的贪心策略选择动作, 即有 $1 - \epsilon$ 的概率选择后继状态值函数最大的动作, 有 $\epsilon$ 概率进行随机选择动作.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''根据状态采取动作</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    list</span></span><br><span class="line"><span class="string">        采取的动作</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取当前状态</span></span><br><span class="line">    state = self.states[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># 下一步所有可能的状态</span></span><br><span class="line">    next_states = []</span><br><span class="line">    <span class="comment"># 下一步所有可能的位置</span></span><br><span class="line">    next_positions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(BOARD_ROWS):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(BOARD_COLS):</span><br><span class="line">            <span class="comment"># 当前棋盘位置上无棋子</span></span><br><span class="line">            <span class="keyword">if</span> state.data[i, j] == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 可行的位置</span></span><br><span class="line">                next_positions.append([i, j])</span><br><span class="line">                <span class="comment"># 可行的状态</span></span><br><span class="line">                next_states.append(state.next_state(i, j, self.symbol).hash())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 有epsilon概率采取随机动作</span></span><br><span class="line">    <span class="keyword">if</span> np.random.rand() &lt; self.epsilon:</span><br><span class="line">        action = next_positions[np.random.randint(len(next_positions))]</span><br><span class="line">        action.append(self.symbol)</span><br><span class="line">        self.greedy[<span class="number">-1</span>] = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    values = []</span><br><span class="line">    <span class="comment"># 遍历下一步所有可能的状态和位置</span></span><br><span class="line">    <span class="keyword">for</span> state_hash, pos <span class="keyword">in</span> zip(next_states, next_positions):</span><br><span class="line">        <span class="comment"># 获取对应状态的值函数</span></span><br><span class="line">        values.append((self.value[state_hash], pos))</span><br><span class="line">    <span class="comment"># 如果有多个状态的值函数相同,且都是最高的,shuffle则起到在这些状态中随机选择的作用</span></span><br><span class="line">    np.random.shuffle(values)</span><br><span class="line">    <span class="comment"># 按值函数从大到小排序</span></span><br><span class="line">    values.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># 选取最优动作</span></span><br><span class="line">    action = values[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    action.append(self.symbol)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><hr><h2 id="HumanPlayer相关"><a href="#HumanPlayer相关" class="headerlink" title="HumanPlayer相关"></a>HumanPlayer相关</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HumanPlayer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        self.symbol = <span class="keyword">None</span></span><br><span class="line">        self.keys = [<span class="string">'q'</span>, <span class="string">'w'</span>, <span class="string">'e'</span>, <span class="string">'a'</span>, <span class="string">'s'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>, <span class="string">'x'</span>, <span class="string">'c'</span>]</span><br><span class="line">        self.state = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        self.state = state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self, _)</span>:</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.state.print_state()</span><br><span class="line">        key = input(<span class="string">"Input your position:"</span>)</span><br><span class="line">        data = self.keys.index(key)</span><br><span class="line">        i = data // int(BOARD_COLS)</span><br><span class="line">        j = data % BOARD_COLS</span><br><span class="line">        <span class="keyword">return</span> [i, j, self.symbol]</span><br></pre></td></tr></table></figure><hr><h2 id="Agent训练"><a href="#Agent训练" class="headerlink" title="Agent训练"></a>Agent训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epochs, print_every_n=<span class="number">500</span>)</span>:</span></span><br><span class="line">    <span class="string">'''对Agent进行训练</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    epochs : int</span></span><br><span class="line"><span class="string">        训练轮数</span></span><br><span class="line"><span class="string">    print_every_n : int, optional</span></span><br><span class="line"><span class="string">        每多少轮输出训练信息</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义两个Agent</span></span><br><span class="line">    player1 = AgentPlayer(epsilon=<span class="number">0.01</span>)</span><br><span class="line">    player2 = AgentPlayer(epsilon=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 定义判决器</span></span><br><span class="line">    game = Game(player1, player2)</span><br><span class="line">    <span class="comment"># 先手赢的次数</span></span><br><span class="line">    player1_win = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 后手赢的次数</span></span><br><span class="line">    player2_win = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 新的一轮游戏</span></span><br><span class="line">        game.reset()</span><br><span class="line">        winner = game.play(print_state=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">1</span>:</span><br><span class="line">            player1_win += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">-1</span>:</span><br><span class="line">            player2_win += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 打印各自的胜率</span></span><br><span class="line">        <span class="keyword">if</span> i % print_every_n == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f'</span> % (i, player1_win / i, player2_win / i))</span><br><span class="line">        <span class="comment"># 在每轮游戏结束后,对Agent进行学习</span></span><br><span class="line">        player1.backup()</span><br><span class="line">        player2.backup()</span><br><span class="line">    <span class="comment"># 保存训练好的策略</span></span><br><span class="line">    player1.save_policy()</span><br><span class="line">    player2.save_policy()</span><br></pre></td></tr></table></figure><hr><h2 id="Agent对弈"><a href="#Agent对弈" class="headerlink" title="Agent对弈"></a>Agent对弈</h2><p>经过充分的训练后, 两个Agent对弈的胜率应该都为0%. 即任何局面都只能打成平手, 没有一方可以胜过另一方.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compete</span><span class="params">(turns)</span>:</span></span><br><span class="line">    <span class="string">'''将训练好的两个Agent进行对弈</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    turns : int</span></span><br><span class="line"><span class="string">        对弈轮数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对弈的时候不进行动作的探索, 故epsilon设为0.</span></span><br><span class="line">    player1 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">    player2 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">    game = Game(player1, player2)</span><br><span class="line">    player1.load_policy()</span><br><span class="line">    player2.load_policy()</span><br><span class="line">    player1_win = <span class="number">0.0</span></span><br><span class="line">    player2_win = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, turns):</span><br><span class="line">        game.reset()</span><br><span class="line">        winner = game.play()</span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">1</span>:</span><br><span class="line">            player1_win += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">-1</span>:</span><br><span class="line">            player2_win += <span class="number">1</span></span><br><span class="line">    print(<span class="string">'%d turns, player 1 win %.02f, player 2 win %.02f'</span> % (turns, player1_win / turns, player2_win / turns))</span><br></pre></td></tr></table></figure><hr><h2 id="Human-v-s-Agent"><a href="#Human-v-s-Agent" class="headerlink" title="Human v.s. Agent"></a>Human v.s. Agent</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''人类玩家和Agent进行对弈</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        player1 = HumanPlayer()</span><br><span class="line">        player2 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">        game = Game(player1, player2)</span><br><span class="line">        player2.load_policy()</span><br><span class="line">        winner = game.play()</span><br><span class="line">        <span class="keyword">if</span> winner == player2.symbol:</span><br><span class="line">            print(<span class="string">"失败!"</span>)</span><br><span class="line">        <span class="keyword">elif</span> winner == player1.symbol:</span><br><span class="line">            print(<span class="string">"胜利!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"平局!"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;为了对强化学习的基本概念有一个直观的认识,《Reinforcement Learning: An Introduction》第一章给出了一个简单的例子：&lt;code&gt;Tic-Tac-Toe&lt;/code&gt;游戏.&lt;/p&gt;
&lt;h2 id=&quot;游戏规则&quot;&gt;&lt;a href=&quot;#游戏规则&quot; class=&quot;headerlink&quot; title=&quot;游戏规则&quot;&gt;&lt;/a&gt;游戏规则&lt;/h2&gt;&lt;p&gt;游戏的规则很简单, 两位玩家在 &lt;code&gt;3x3&lt;/code&gt; 的棋盘上轮流下棋, 一位打 &lt;code&gt;X&lt;/code&gt;, 另一位打 &lt;code&gt;O&lt;/code&gt;, 若棋盘的任意一行、任意一列、正反对角线上有三个相同的棋, 则执该棋的玩家获胜. 若棋盘下满仍没有决出胜负, 则平局.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（三）：动态规划</title>
    <link href="https://orzyt.cn/posts/planning-by-dp/"/>
    <id>https://orzyt.cn/posts/planning-by-dp/</id>
    <published>2019-03-01T06:10:25.000Z</published>
    <updated>2019-03-13T12:07:10.945Z</updated>
    
    <content type="html"><![CDATA[<hr><p>在上一篇文章 <a href="https://orzyt.cn/posts/markov-decision-processes/">强化学习（二）：马尔可夫决策过程</a> 中, 我们介绍用来对强化学习问题进行建模的马尔可夫决策过程(Markov Decision Processes, MDPs). </p><p>由于MDPs的贝尔曼最优方程没有封闭解, 因此一般采用迭代的方法对其进行求解. </p><p>本文将介绍使用<strong>动态规划(Dynamic Programming)</strong>算法来求解MDPs.</p><a id="more"></a><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><ul><li><p><strong>动态(Dynamic)</strong>: 问题中的时序部分</p></li><li><p><strong>规划(Planning)</strong>: 对问题进行优化</p></li></ul><p>动态规划将问题分解为子问题, 从子问题的解中得到原始问题的解.</p><hr><h3 id="动态规划的性质"><a href="#动态规划的性质" class="headerlink" title="动态规划的性质"></a>动态规划的性质</h3><ul><li><p><strong>最优子结构(Optimal substructure)</strong></p><ul><li>应用最优性原则(Principle of optimality)</li><li>最优解可以从子问题的最优解中得到</li></ul></li><li><p><strong>重叠子问题(Overlapping subproblems)</strong></p><ul><li>相同的子问题出现多次</li><li>问题的解可以被缓存和复用</li></ul></li></ul><p>马尔可夫决策过程满足上面两种性质:</p><blockquote><p><em>贝尔曼方程</em> 给出了问题的递归分解表示, <em>值函数</em> 存储和复用了问题的解.</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi(a|s) (\mathcal{R}_s^a + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}v_{\pi}(s'))</script></blockquote><hr><h3 id="用动态规划进行Planning"><a href="#用动态规划进行Planning" class="headerlink" title="用动态规划进行Planning"></a>用动态规划进行Planning</h3><p>动态规划假设我们知道MDP的所有知识, 包括状态、行为、转移矩阵、奖励甚至策略等.</p><p>对于<strong>预测(Prediction)</strong>问题: </p><ul><li><p>输入: </p><ul><li>MDP $&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;$ 和 策略 $\pi$</li><li>MRP $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$</li></ul></li><li><p>输出: 值函数 $v_{\pi}$</p></li></ul><p>对于<strong>控制(Control)</strong>问题:</p><ul><li><p>输入:</p><ul><li>MDP $&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;$</li></ul></li><li><p>输出:</p><ul><li>最优值函数 $v_{*}$</li><li>最优策略 $\pi_{*}$</li></ul></li></ul><hr><h2 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h2><blockquote><p>问题: 评估一个给定的策略 $\pi$<br>求解: 对贝尔曼期望方程进行迭代, $v_1 \to v_2 \to \dots \to v_{\pi}$</p></blockquote><p>通常使用<strong>同步备份(synchronous backups)</strong>方法:</p><p>对于第 $k+1$ 次迭代, 所有状态 $s$ 在第 $k+1$ 时刻的价值 $v_{k+1}(s)$ 用 $v_k(s’)$ 进行更新, 其中 $s’$ 是 $s$ 的后继状态.</p><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0nb9rn3v4j20ar06eq31.jpg" alt="迭代策略评估" width="30%" height="30%"></p><script type="math/tex; mode=display">\begin{aligned} v _ { k + 1 } ( s ) & = \sum _ { a \in \mathcal { A } } \pi ( a | s ) \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { k } \left( s ^ { \prime } \right) \right) \\\mathbf { v } ^ { k + 1 } & = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } \mathbf { v } ^ { k } \end{aligned}</script><hr><p><strong>迭代策略评估算法</strong>:</p><p>迭代策略评估算法用来估计 $V \approx v_{\pi}$.</p><p>这里使用<code>in-place</code>版本, 即只保留一份 $v$ 数组, 没有新旧之分. </p><p>通常来说, 该方法也能收敛到 $v_{\pi}$, 而且收敛速度可能更快.</p><p>终止条件: $\max \limits_ { s \in \mathcal{S} } \left| v _ { k + 1 } ( s ) - v _ { k } ( s ) \right|$ 小于给定的误差 $\Delta$</p><p><img src="https://ws1.sinaimg.cn/large/8662e3cegy1g0nj071hc6j20km08hq3t.jpg" alt="迭代策略评估伪代码" width="60%" height="60%"></p><hr><p>例子: <strong>Small Gridworld</strong> <a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter04/grid_world.py" target="_blank" rel="noopener">[代码]</a></p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0nbvkvvotj20k00dkdgx.jpg" alt="Small Gridworld" width="50%" height="50%"></p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0nc5gkmd8j20e30lkwgd.jpg" alt="Small Gridworld Solution" width="50%" height="50%"></p><hr><h2 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h2><p>让我们考虑一个<strong>确定性策略</strong>(即对于一个状态来说, 其采取的动作是确定的, 而不是考虑每个动作的概率) $a = \pi(s)$.</p><blockquote><p>我们可以通过贪心选择来改进策略 $\pi$:</p><script type="math/tex; mode=display">\pi ^ { \prime } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } q _ { \pi } ( s , a )</script></blockquote><p>即状态 $s$ 的新策略为令动作值函数 $q_{\pi}(s, a)$ 取得最大值的动作.</p><p>相应地, 动作值函数 $q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right)$ 得到了改进:</p><script type="math/tex; mode=display">q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a ) \geq q _ { \pi } ( s , \pi ( s ) ) = v _ { \pi } ( s ) \\{\scriptsize 由于是确定性策略, 才会有 v_{\pi}(s) = q_{\pi}(s, \pi(s))}\tag{1}</script><p>注: 确定性策略下的动作值函数 $q_{\pi}(s, a)$ 为:</p><script type="math/tex; mode=display">\begin{aligned} q _ { \pi } ( s , a ) & = \mathbb { E } \left[ R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right) | S _ { t } = s , A _ { t } = a \right] \\ & = \sum _ { s ^ { \prime } , r } p \left( s ^ { \prime } , r | s , a \right) \left[ r + \gamma v _ { \pi } \left( s ^ { \prime } \right) \right] \end{aligned}\tag{2}</script><p>从而, 值函数 $v _ { \pi ^ { \prime } } ( s )$ 也得到了改进:</p><script type="math/tex; mode=display">\begin{aligned} v_\pi(s) & \le q_\pi(s,\pi^{'}(s)) {\scriptsize //公式(1)} \\ &={\Bbb E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t=s, A_t=\pi^{'}(s)] {\scriptsize //公式(2)} \\&={\Bbb E}_{\pi'}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\  {\scriptsize //注意外层是在新策略 \pi^{'} 下求期望} \\   & \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma q_\pi(S_{t+1},\pi'(S_{t+1}))|S_t=s] {\scriptsize //对状态S_{t+1}使用公式(1)} \\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma {\Bbb E}_{\pi'}\left[ R_{t+2}+\gamma v_{\pi}\left( S_{t+2}\right) | S_{t+1}, A_{t+1}=\pi^{'}(S_{t+1}) \right] | S_t=s]\\ &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_{\pi}\left( S_{t+2} \right)|S_t=s] {\scriptsize //去掉括号内的期望} \\ & \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma ^2 q_\pi(S_{t+2},\pi'(S_{t+2}))|S_t=s] {\scriptsize //对状态S_{t+2}使用公式(1)} \\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 {\Bbb E}_{\pi'}\left( R_{t+3}+\gamma v_{\pi}\left( S_{t+3} \right) \right)|S_t=s]\\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_{\pi}\left( S_{t+3} \right)|S_t=s]\\  & \vdots \\& \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4} + \dots |S_t=s]\\ &=v_{\pi^{'}}(s) \\ \end{aligned}</script><p>当改进停止时, 有如下等式:</p><script type="math/tex; mode=display">q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a ) = q _ { \pi } ( s , \pi ( s ) ) = v _ { \pi } ( s )\tag{3}</script><p>可以说, 此时公式(3)满足了贝尔曼最优方程:</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a )</script><p>从而, 对所有状态 $s$ 来说, 有$v_{\pi}(s) = v_{*}(s)$, 即策略 $\pi$ 改进到了最优策略.</p><hr><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><h3 id="策略迭代-1"><a href="#策略迭代-1" class="headerlink" title="策略迭代"></a>策略迭代</h3><p>给定一个策略 $\pi$, 我们可以首先对策略进行评估, 然后根据值函数 $v_{\pi}$ 进行贪心地改进策略.</p><script type="math/tex; mode=display">\pi _ { 0 } \stackrel { \mathrm { E } } { \longrightarrow } v _ { \pi _ { 0 } } \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { 1 } \stackrel { \mathrm { E } } { \longrightarrow } v _ { \pi _ { 1 } } \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { 2 } \stackrel { \mathrm { E } } { \longrightarrow } \cdots \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { * } \stackrel { \mathrm { E } } { \longrightarrow } v _ { * }</script><p>其中, $\stackrel { \mathrm { E } } { \longrightarrow }$ 表示策略评估, $\stackrel { \mathrm { I } } { \longrightarrow }$ 表示策略改进. </p><ul><li><p><strong>评估(Evaluate):</strong></p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots | S _ { t } = s \right]</script></li><li><p><strong>改进(Improve):</strong></p><script type="math/tex; mode=display">\pi^{'} = \text{greedy}(v_{\pi})</script></li></ul><p>由于每个策略都比前一个策略更优, 同时一个有限状态的马尔可夫决策过程(finite MDP)仅有有限个策略, 因此该过程一定能够在有限次的迭代中收敛到最优策略 $\pi_{*}$ 和最优值函数 $v_{*}$.</p><hr><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0ncddk93hj20ni0cc0uq.jpg" alt="策略迭代" width="50%" height="50%"></p><hr><p><strong>策略迭代算法:</strong></p><p>策略迭代算法分为: <strong>初始化</strong>, <strong>策略评估</strong> 以及 <strong>策略改进</strong> 三部分.</p><p>其中, 策略改进部分的终止条件为: <strong>是否所有状态的策略不再发生变化</strong>.</p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0njmn8jy5j20kq0e3760.jpg" alt="策略迭代算法" width="60%" height="60%"></p><hr><p>例子: <strong>Jack’s Car Rental</strong> <a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter04/car_rental.py" target="_blank" rel="noopener">[代码]</a>  (<em>先占个坑 , 等有时间把这个例子详细写下</em>)</p><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0ncjvapk8j20ke0ebn32.jpg" alt="Jack’s Car Rental" width="50%" height="50%"></p><p>策略迭代求解结果:</p><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0nclbnxl1j20jw0dgtam.jpg" alt="Jack’s Car Rental Solution" width="50%" height="50%"></p><p>图中纵坐标是位置 $1$ 的汽车数量, 横坐标是位置 $2$ 的汽车数量, 该问题共有 $21 \times 21$ 个状态. </p><p>图中的等高线将状态划分为不同的区域, 区域内的数值代表相应的策略(正数代表从位置 $1$ 移往位置 $2$ 的汽车数量, 负数则往反方向移动).</p><hr><h3 id="策略迭代的扩展"><a href="#策略迭代的扩展" class="headerlink" title="策略迭代的扩展"></a>策略迭代的扩展</h3><h4 id="改良策略迭代"><a href="#改良策略迭代" class="headerlink" title="改良策略迭代"></a>改良策略迭代</h4><p>策略评估并不需要真正的收敛到 $v_{\pi}$. (比如在 <code>Small Gridworld</code>例子中, 迭代 $k=3$次 即可以得到最优策略.)</p><p>为此我们可以引进终止条件, 如:</p><ul><li>值函数的 $\epsilon$ -收敛</li><li>简单地迭代 $k$ 次便停止策略评估</li></ul><p>或者每次迭代(即 $k=1$ )都对策略进行更新改进, 这种情况等价于<strong>值迭代(value iteration)</strong>.</p><hr><h4 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h4><p><strong>广义策略迭代</strong>(Generalized Policy iteration，GPI)指代让策略评估(policy-evaluation)和策略改进(policyimprovement)过程进行交互的一般概念, 其不依赖于两个过程的粒度(granularity)和其他细节.</p><p>几乎所有强化学习方法都可以很好地被描述为GPI. 也就是说, 它们都具有可辨识的策略与值函数. 其中, 策略 $\pi$ 通过相应的值函数 $v$ 进行改进, 而值函数 $V$ 总是趋向策略 $\pi$ 的值函数 $v^{\pi}$. 如下图所示,</p><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0nik26512j206y0a5dfz.jpg" alt="广义策略迭代" width="20%" height="20%"></p><hr><h2 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h2><p>策略迭代的一个缺点是它的每次迭代都涉及策略评估, 这本身就是一个需要对状态集进行多次扫描的耗时迭代计算. </p><p>而在值迭代的过程中, 并没有出现显式的策略, 并且中间过程的值函数可能也不和任何策略对应.</p><hr><h3 id="最优性原则"><a href="#最优性原则" class="headerlink" title="最优性原则"></a>最优性原则</h3><p>一个最优策略可以被分解为两部分:</p><ul><li>当前状态的最优动作 $A_{*}$</li><li>后继状态 $S^{\prime}$ 的最优策略</li></ul><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0nk02i8apj20mi06v75b.jpg" alt="最优性原则" width="60%" height="60%"></p><p>该原则的意思是说, 一个策略 $\pi(a|s)$ 在状态 $s$ 取到最优值函数 $v_{\pi}(s) = v_{*}(s)$ <strong>当且仅当</strong> 对于所有从状态 $s$ 出发可到达的状态 $s^{\prime}$, 策略 $\pi$ 也能够在状态 $s^{\prime}$ 取到最优值函数.</p><hr><h3 id="确定性值迭代"><a href="#确定性值迭代" class="headerlink" title="确定性值迭代"></a>确定性值迭代</h3><p>如果我们已经知道子问题的最优解 $v_{*}(s^{\prime})$, 那么状态 $s$ 的最优解可以通过向前看(lookahead)一步得到, 这称为<strong>值迭代(Value Iteration)</strong>:</p><script type="math/tex; mode=display">v_{*}(s) \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{*}(s') \right)</script><hr><p><strong>值迭代算法:</strong></p><p>值迭代算法和策略迭代算法一样, 是用来估计最优策略 $\pi_{*}$ 的, 它将策略评估和策略改进有效地结合在了一起.</p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0nkt7xfz0j20kn09k0ts.jpg" alt="值迭代算法" width="60%" height="60%"></p><hr><h2 id="同步动态规划算法总结"><a href="#同步动态规划算法总结" class="headerlink" title="同步动态规划算法总结"></a>同步动态规划算法总结</h2><div class="table-container"><table><thead><tr><th style="text-align:center">问题</th><th style="text-align:center">贝尔曼方程</th><th style="text-align:center">算法</th></tr></thead><tbody><tr><td style="text-align:center">预测(Prediction)</td><td style="text-align:center">贝尔曼期望方程</td><td style="text-align:center">迭代策略评估</td></tr><tr><td style="text-align:center">控制(Control)</td><td style="text-align:center">贝尔曼期望方程 + 贪心策略改进</td><td style="text-align:center">策略迭代</td></tr><tr><td style="text-align:center">控制(Control)</td><td style="text-align:center">贝尔曼最优方程</td><td style="text-align:center">值迭代</td></tr></tbody></table></div><p>对于有 $m$ 个动作和 $n$ 个状态 的MDP来说, 每次迭代的时间复杂度如下:</p><div class="table-container"><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">复杂度</th></tr></thead><tbody><tr><td style="text-align:center">$v_{\pi}(s)$ or $v_{*}(s)$</td><td style="text-align:center">$\mathcal{O}(mn^2)$</td></tr><tr><td style="text-align:center">$q_{\pi}(s, a)$ or $q_{*}(s, a)$</td><td style="text-align:center">$\mathcal{O}(m^2n^2)$</td></tr></tbody></table></div><hr><h2 id="动态规划的扩展"><a href="#动态规划的扩展" class="headerlink" title="动态规划的扩展"></a>动态规划的扩展</h2><h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>同步DP算法的主要缺点是每次迭代都需要对整个状态集进行扫描, 这对于状态数非常多的MDP来说耗费巨大. 而异步DP算法则将所有的状态独立地,以任意顺序进行备份, 并且每个状态的更新次数不一, 这可以显著地减少计算量.</p><p>为了保证算法的正确收敛, 异步动态规划算法必须保证<strong>所有状态都能够持续地被更新</strong>(continue to update the values of all the states), 也就是说在任何时刻任何状态都有可能被更新, 而不能忽略某个状态.</p><p>异步DP算法主要有三种简单的思想:</p><ul><li>就地动态规划(<em>In-place</em> dynamic programming)</li><li>优先扫描(<em>Prioritised sweeping</em>)</li><li>实时动态规划(<em>Real-time</em> dynamic programming)</li></ul><hr><h4 id="就地动态规划"><a href="#就地动态规划" class="headerlink" title="就地动态规划"></a>就地动态规划</h4><p>同步DP保留值函数的两个备份, $v_{new}$ 和 $v_{old}$</p><script type="math/tex; mode=display">{\color{red} {v_{new}(s)}} \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} {\color{red} {v_{old}(s')}} \right)</script><p>就地值迭代只保留值函数的一个备份.</p><script type="math/tex; mode=display">{\color{red} {v(s)}} \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} {\color{red} {v(s')}} \right)</script><hr><h4 id="优先扫描"><a href="#优先扫描" class="headerlink" title="优先扫描"></a>优先扫描</h4><p>使用贝尔曼误差的大小来进行状态的选择:</p><script type="math/tex; mode=display">\left| \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } \right) \right) - v ( s ) \right|</script><ul><li><p>仅备份有最大贝尔曼误差的状态</p></li><li><p>在每次备份后, 需要更新受到影响的状态(即备份状态的前驱状态)的贝尔曼误差</p></li><li><p>可以使用优先队列进行实现</p></li></ul><hr><h4 id="实时动态规划"><a href="#实时动态规划" class="headerlink" title="实时动态规划"></a>实时动态规划</h4><ul><li>思想: <strong>只使用和Agent相关的状态</strong></li><li>使用Agent的经验来进行状态的选择</li><li>在每个时间步 $S_t, A_t, R_{t+1}$ 对状态 $S_t$ 进行备份</li></ul><script type="math/tex; mode=display">{\color{red} {v \left( S _ { t } \right)}} \gets \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { {\color{red}{S _ { t }}} } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { {\color{red} {S _ { t }}} s ^ { \prime }}  ^ { a } {\color{red} {v \left( s ^ { \prime } \right)}} \right)</script><hr><h3 id="全宽和采样备份"><a href="#全宽和采样备份" class="headerlink" title="全宽和采样备份"></a>全宽和采样备份</h3><h4 id="全宽备份"><a href="#全宽备份" class="headerlink" title="全宽备份"></a>全宽备份</h4><ul><li><p>DP使用<strong>全宽备份</strong>(<em>full-width</em> backups)</p></li><li><p>对于每次备份(不管同步还是异步)</p><ul><li>每个后继状态和动作都会被考虑进去</li><li>需要知道MDP转移矩阵和奖励函数</li></ul></li><li><p>对于大规模DP问题会遇到维数灾难</p></li><li><p>进行一次备份都太奢侈了</p></li></ul><hr><h4 id="采样备份"><a href="#采样备份" class="headerlink" title="采样备份"></a>采样备份</h4><p><strong>采样备份(Sample Backups)</strong>使用采样的奖励和采样的转移 $&lt; S , A , R , S ^ { \prime } &gt;$ 来替代奖励函数 $\mathcal{R}$ 和 转移矩阵 $\mathcal{P}$. </p><p>采样备份的优点:</p><ul><li><strong>Model-free</strong>: 不需要知道MDP的先验知识</li><li>通过采样<strong>缓解维数灾难</strong></li><li><strong>备份代价成为常量</strong>, 独立于状态数 $n = |\mathcal{S}|$</li></ul><hr><h2 id="压缩映射"><a href="#压缩映射" class="headerlink" title="压缩映射"></a>压缩映射</h2><p>关于上面的种种算法, 我们可能会有如下疑问:</p><ul><li>值迭代是否会收敛到 $v_{*}$ ?</li><li>迭代策略评估是否会收敛到 $v_{\pi}$ ?</li><li>策略迭代是否会收敛到 $v_{*}$ ?</li><li>解唯一吗 ?</li><li>算法收敛速度有多快 ?</li></ul><p>为了解决这些问题, 需要引入压缩映射(contraction mapping)理论.<br>可以参考: <a href="https://zhuanlan.zhihu.com/p/39279611" target="_blank" rel="noopener">如何证明迭代式策略评价、值迭代和策略迭代的收敛性？</a></p><hr><p>(关于压缩映射理论有时间再补充, 先到这里吧…)</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第四章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf" target="_blank" rel="noopener">Lecture3: Planning by Dynamic Programming</a></li><li><a href="https://zhuanlan.zhihu.com/p/51393982" target="_blank" rel="noopener">David Silver 增强学习——Lecture 3 动态规划</a></li><li><a href="https://www.cnblogs.com/pinard/p/9463815.html" target="_blank" rel="noopener">强化学习（三）用动态规划（DP）求解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;在上一篇文章 &lt;a href=&quot;https://orzyt.cn/posts/markov-decision-processes/&quot;&gt;强化学习（二）：马尔可夫决策过程&lt;/a&gt; 中, 我们介绍用来对强化学习问题进行建模的马尔可夫决策过程(Markov Decision Processes, MDPs). &lt;/p&gt;
&lt;p&gt;由于MDPs的贝尔曼最优方程没有封闭解, 因此一般采用迭代的方法对其进行求解. &lt;/p&gt;
&lt;p&gt;本文将介绍使用&lt;strong&gt;动态规划(Dynamic Programming)&lt;/strong&gt;算法来求解MDPs.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="动态规划" scheme="https://orzyt.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（二）：马尔可夫决策过程</title>
    <link href="https://orzyt.cn/posts/markov-decision-processes/"/>
    <id>https://orzyt.cn/posts/markov-decision-processes/</id>
    <published>2019-02-27T07:38:18.000Z</published>
    <updated>2019-03-13T12:07:14.581Z</updated>
    
    <content type="html"><![CDATA[<hr><p>在上一篇文章 <a href="https://orzyt.cn/posts/introduction-to-rl">强化学习（一）：强化学习简介</a> 中, 我们介绍了强化学习的一些基本概念.</p><p>本文将介绍用来对强化学习问题进行建模的<strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>.</p><a id="more"></a><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><h3 id="马尔可夫决策过程简介"><a href="#马尔可夫决策过程简介" class="headerlink" title="马尔可夫决策过程简介"></a>马尔可夫决策过程简介</h3><p><strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>形式上用来描述强化学习中的环境.</p><p>其中,环境是<strong>完全可观测的(fully observable)</strong>,即当前状态可以完全表征过程.</p><p>几乎所有的强化学习问题都能用MDPs来描述：</p><ul><li>最优控制问题可以描述成连续MDPs;</li><li>部分观测环境可以转化成MDPs;</li><li>赌博机问题是只有一个状态的MDPs.</li></ul><hr><h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3nzaa8yj20mn0593ym.jpg" alt="马尔科夫性质" width="60%" height="60%"></p><p>马尔科夫性质(Markov Property)表明: <strong>未来只与现在有关,而与过去无关.</strong></p><hr><h3 id="状态转移矩阵"><a href="#状态转移矩阵" class="headerlink" title="状态转移矩阵"></a>状态转移矩阵</h3><p>对于一个马尔可夫状态$S$及其后继状态$S’$,其状态转移概率由下式定义:</p><script type="math/tex; mode=display">\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]</script><p><strong>状态转移矩阵(State Transition Matrix)$\mathcal{P}$</strong>定义了从所有状态$S$转移到所有后继状态$S’$的概率.</p><script type="math/tex; mode=display">\mathcal { P } = \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \dots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n 1 } } & { \cdots } & { \mathcal { P } _ { n n } } \end{array} \right]</script><p>其中,$n$为状态个数,且矩阵的每行和为1.</p><hr><h3 id="马尔可夫过程-1"><a href="#马尔可夫过程-1" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h3><p><strong>马尔可夫过程(Markov Process)</strong>是一个无记忆的随机过程(memoryless random process).</p><p>即,随机状态$S_1, S_2, \dots$序列具有马尔可夫性质.</p><blockquote><p>马尔可夫过程(或马尔可夫链)是一个二元组$&lt;\mathcal{S}, \mathcal{P}&gt;$</p><ul><li>$\mathcal{S}$: (有限)状态集</li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li></ul></blockquote><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l1vm9xkzj20c80act96.jpg" alt="Example: Student Markov Chain" width="50%" height="50%"></p><p>圆圈代表状态, 箭头代表状态之间的转移, 数值代表转移概率.</p><p>状态转移矩阵$\mathcal{P}$如下:</p><script type="math/tex; mode=display">{\mathcal P} =\begin{bmatrix}  & C1 & C2 & C3 &  Pass & Pub & FB & Sleep\\  C1 & &0.5 &  &   & & 0.5 & \\ C2  & & &  0.8 & & & &0.2\\ C3  & & &  & 0.6& 0.4& &\\ Pass  & & &  & & & &1.0\\ Pub  &0.2 & 0.4& 0.4 & & & &\\ FB  &0.1 & &  & & & 0.9 &\\ Sleep  & & &  & & & &1.0 \end{bmatrix}</script><hr><h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><p><strong>马尔可夫奖励过程(Markov Reward Process, MRP)</strong>是<em>带有奖励的马尔可夫链</em>.</p><blockquote><p>马尔可夫奖励过程是一个四元组&lt;$\mathcal{S}$, $\mathcal{P}$, <font color="red">$\mathcal{R}$</font>, <font color="red">$\mathcal{\gamma}$</font>&gt;</p><ul><li>$\mathcal{S}$: (有限)状态集</li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li><li><font color="red"> $\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right]$ </font></li><li><font color="red"> $\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </font></li></ul></blockquote><p><img src="http://wx1.sinaimg.cn/large/8662e3cegy1g0l2klvnixj20cf0aowf0.jpg" alt="Example: Student MRP" width="50%" height="50%"></p><h3 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h3><blockquote><p><strong>回报(Return)</strong> $G_t$ 是从时间 $t$ 开始的总折扣奖励.</p><script type="math/tex; mode=display">G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 }</script></blockquote><ul><li>折扣因子 $\gamma \in [ 0,1 ]$ 表示未来的奖励在当前的价值. 由于未来的奖励充满不确定性, 因此需要乘上折扣因子;</li><li>$\gamma$ 接近 $0$ 表明更注重当前的奖励(myopic);</li><li>$\gamma$ 接近 $1$ 表明更具有远见(far-sighted).</li></ul><hr><h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数(Value Function) $v(s)$ 表示一个状态 $s$ 的长期价值(long-term value).</p><blockquote><p>一个马尔可夫奖励过程(MRP)的<strong>状态值函数 $v(s)$</strong>是从状态 $s$ 开始的期望回报.</p><script type="math/tex; mode=display">v ( s ) = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right]</script></blockquote><hr><h3 id="MRPs的贝尔曼方程"><a href="#MRPs的贝尔曼方程" class="headerlink" title="MRPs的贝尔曼方程"></a>MRPs的贝尔曼方程</h3><p>值函数可以被分解为两部分:</p><ul><li>立即奖励 $R_{t+1}$</li><li>后继状态的折扣价值 $\gamma v(S_{t+1})$</li></ul><script type="math/tex; mode=display">\begin{aligned} v ( s ) & = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \gamma ^ { 2 } R _ { t + 3 } + \ldots | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma \left( R _ { t + 2 } + \gamma R _ { t + 3 } + \ldots \right) | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma G _ { t + 1 } | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \mathbb { E } \left[ \gamma G _ { t + 1 } | S _ { t } = s \right]\\ & = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \gamma v \left( S _ { t + 1 } \right)\\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma v \left( S _ { t + 1 } \right) | S _ { t } = s \right] \end{aligned}\tag{1}\label{eq:mrp-bellman-equation}</script><p>上式表明, $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 的值函数之间满足递推关系. </p><p>该递推式也称为<strong>贝尔曼方程(Bellman Equation)</strong>.</p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0l3fh3jb3j207802zglh.jpg" alt="Bellman Equation for MRPs" width="30%" height="30%"></p><p>如果已知概率转移矩阵 $\mathcal{P}$, 则可将公式\eqref{eq:mrp-bellman-equation}变形为:</p><script type="math/tex; mode=display">v ( s ) = \mathcal { R } _ { s } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } v \left( s ^ { \prime } \right)\tag{2}\label{eq:mrp-bellman-equation-2}</script><p>例子:</p><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l3pbm9ixj20c30b5mxp.jpg" alt="Example: Bellman Equation for Student MRP" width="40%" height="40%"></p><p><strong>贝尔曼方程的矩阵形式:</strong></p><p>可将公式\eqref{eq:mrp-bellman-equation-2}改写为矩阵形式:</p><script type="math/tex; mode=display">v = \mathcal { R } + \gamma \mathcal { P } v</script><p>其中, $v$ 为一个列向量, 向量的元素为每个状态的值函数.</p><script type="math/tex; mode=display">\left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right] = \left[ \begin{array} { c } { \mathcal { R } _ { 1 } } \\ { \vdots } \\ { \mathcal { R } _ { n } } \end{array} \right] + \gamma \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \ldots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n1 } } & { \ldots } & { \mathcal { P } _ { n n } } \end{array} \right] \left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right]</script><p>观测贝尔曼方程的矩阵形式, 可知其为线性方程, 可直接求解如下.</p><script type="math/tex; mode=display">\begin{aligned} v & = \mathcal { R } + \gamma \mathcal { P } v \\( I - \gamma \mathcal { P } ) v & = \mathcal { R } \\v & = ( I - \gamma \mathcal { P } ) ^ { - 1 } \mathcal { R }\end{aligned}</script><p>计算复杂度为: $\mathcal{O}(n^3)$. 因此, 只适合直接求解小规模的MRP问题.</p><p>对于大规模的MRP问题, 通常采取以下的迭代方法:</p><ul><li>动态规划(Dynamic programming)</li><li>蒙特卡洛评估(Monte-Carlo evaluation)</li><li>时序差分学习(Temporal-Difference learning)</li></ul><hr><h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p><strong>马尔可夫决策过程(Markov Decision Process, MDP)</strong>是<em>带有决策的马尔可夫奖励过程</em>.</p><blockquote><p>马尔可夫决策过程是一个五元组&lt;$\mathcal{S}$, <font color="red">$\mathcal{A}$</font>, $\mathcal{P}$, $\mathcal{R}$, $\mathcal{\gamma}$&gt;</p><ul><li>$\mathcal{S}$: 有限的状态集</li><li><font color="red"> $\mathcal{A}$: 有限的动作集</font></li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } ^ {a}= \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s, A _ { t } = a \right]$</li><li>$\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } ^ {a} = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s, A _ { t } = a \right]$</li><li>$\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </li></ul></blockquote><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0l47drh0vj20g30d93zc.jpg" alt="Example: Student MDP" width="45%" height="45%"></p><hr><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><blockquote><p><strong>策略(Policy) $\pi$</strong> 是给定状态的动作分布.</p><script type="math/tex; mode=display">\pi ( a | s ) = \mathbb { P } \left[ A _ { t } = a | S _ { t } = s \right]</script></blockquote><ul><li>策略完全决定智能体的行为;</li><li>MDP策略值依赖于当前状态(无关历史);</li><li>策略是固定的(与时间无关). $A _ { t } \sim \pi ( \cdot | S _ { t } ) , \forall t &gt; 0$</li></ul><p>给定一个马尔可夫决策过程 $M = &lt;\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma}&gt;$ 和 一个策略 $\pi$, 其可以转化为<em>马尔可夫过程</em>和<em>马尔可夫奖励过程</em>.</p><ul><li><p>状态序列 $S_1, S_2, \dots$ 是马尔科夫决策过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}&gt;$.</p></li><li><p>状态和奖励序列 $S_1, R_2, S_2, \dots$ 是马尔科夫奖励过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$.</p></li></ul><p>其中,</p><script type="math/tex; mode=display">\mathcal{P}_{s,s'}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{P}_{ss'}^{a}</script><script type="math/tex; mode=display">\mathcal{R}_{s}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{R}_{s}^{a}</script><hr><h3 id="值函数-1"><a href="#值函数-1" class="headerlink" title="值函数"></a>值函数</h3><p><strong>值函数(Value Function)</strong>可分为<strong>状态值函数(state-value function)</strong>和<strong>动作值函数(action-value function)</strong>.</p><blockquote><p>MDP的<strong>状态值函数 $v_{\pi}(s)$ </strong>是从状态 $s$ 开始, 然后按照策略 $\pi$ 决策所获得的期望回报.</p><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t | S_t = s \right]</script><p>MDP的<strong>动作值函数 $q_{\pi}(s, a)$ </strong>是从状态 $s$ 开始, 采取动作 $a$, 然后按照策略 $\pi$ 决策所获得的期望回报.</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t = a \right]</script></blockquote><hr><h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>状态值函数可以被分解为两部分, <strong>立即奖励 + 后继状态的折扣价值</strong>.</p><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s \right]</script><p>动作值函数也可以类似地分解.</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \right]</script><hr><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0ldl141fkj20bb04xq2x.jpg" width="40%" height="40%"></p><p>上图中, 空心圆圈代表状态, 实心圆圈代表动作.</p><p>在已知策略 $\pi$ 的情况下, 状态值函数 $v_{\pi}(s)$ 可以用动作值函数 $q_{\pi}(s, a)$ 进行表示:</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a) \tag{3}\label{eq:mdp-state-value-function}</script><hr><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0lds6jc80j20b004rmx6.jpg" width="40%" height="40%"></p><p>同理, 动作值函数 $q_{\pi}(s, a)$ 也可以用状态值函数 $v_{\pi}(s)$ 进行表示:</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}v_{\pi}(s') \tag{4}\label{eq:mdp-action-value-function}</script><hr><p><strong>状态值函数的贝尔曼期望方程:</strong></p><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0le5yxgeij20b706hdfx.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:mdp-action-value-function}代入公式\eqref{eq:mdp-state-value-function}中, 可得状态值函数的贝尔曼期望方程:</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{\pi}(s')  \right)</script><hr><p><strong>动作值函数的贝尔曼期望方程:</strong></p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0le9cf2u7j20bd05wwek.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:mdp-state-value-function}代入公式\eqref{eq:mdp-action-value-function}中, 可得动作值函数的贝尔曼期望方程:</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} \sum \limits_{a' \in \mathcal{A}} \pi (a' | s') q_{\pi}(s', a')</script><hr><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lecy0oxgj20h90dcwfj.jpg" alt="状态值函数的贝尔曼期望方程示例" width="55%" height="55%"></p><hr><p><strong>贝尔曼期望方程的矩阵形式:</strong></p><script type="math/tex; mode=display">v_{\pi} = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} v_{\pi}</script><p>可直接求解:</p><script type="math/tex; mode=display">v_{\pi} = (I - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}</script><hr><h3 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h3><blockquote><p><strong>最优状态值函数(optimal state-value function)</strong> $v_{*}(s)$ 是所有策略中最大的值函数.</p><script type="math/tex; mode=display">v_{*}(s) = \max \limits_{\pi}v_{\pi}(s)</script><p><strong>最优动作值函数(optimal action-value function)</strong> $q_{*}(s, a)$ 是所有策略中最大的动作值函数.</p><script type="math/tex; mode=display">q_{*}(s, a) = \max \limits_{\pi}q_{\pi}(s, a)</script></blockquote><ul><li>最优值函数代表了MDP的最好性能.</li><li>当得知最优值函数时, MDP可被认为”已解决”.</li></ul><hr><p>例子: </p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leoxfaylj20h70ee75c.jpg" alt="Student MDP中的最优状态值函数" width="50%" height="50%"></p><hr><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leqk38l4j20hh0eg75i.jpg" alt="Student MDP中的最优动作值函数" width="50%" height="50%"></p><p>注: 根据公式\eqref{eq:mdp-state-value-function}, Pub动作的最优值应为 $q_{*} = +1 + (0.2 \times 6 + 0.4 \times 8 + 0.4 \times 10) = 9.4$.</p><hr><h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p>首先定义策略之间的偏序关系, 使得策略之间可以进行比较:</p><script type="math/tex; mode=display">\pi \geq \pi ' \quad \text{if} \quad  v_{\pi}(s) \geq v_{\pi '}(s) , \forall s</script><p>对于任意的MDP来说:</p><ul><li>存在一个最优策略 $\pi_{*}$, 使得 $\pi_{*} \geq \pi, \forall \pi$</li><li>所有的最优策略都能取得最优值函数 $v_{\pi_{*}}(s) = v_{*}(s)$</li><li>所有的最优策略都能取得最优动作值函数 $q_{\pi_{*}}(s, a) = q_{*}(s, a)$</li></ul><hr><p><strong>寻找最优策略</strong></p><p>一个最优策略可以通过最大化所有的 $q_{*}(s, a)$ 得到:</p><script type="math/tex; mode=display">\pi_{*} \left( a | s \right) = \left \{ \begin{array}{ll}1 \ {\mathbb {if}} \ a = \operatorname*{argmax} \limits_{a \in \mathcal{A}} \ q_{*} \left( s,a \right) \\              0 \ {\mathbb {otherwise}}              \end{array} \right.</script><ul><li>对于任意的MDP, 总存在确定的最优策略</li><li>如果我们知道 $q_{*}(s, a)$, 则可以立即得到最优策略</li></ul><hr><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lfhg0710j20hn0ehjsl.jpg" alt="Student MDP的最优策略" width="50%" height="50%"></p><p>图中红色弧线表示每个状态的最优决策.</p><hr><h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>$v_{*}$可以通过贝尔曼最优方程递归得到:</p><p><img src="https://ws1.sinaimg.cn/large/8662e3cegy1g0lfkujh38j20b804uaa2.jpg" width="40%" height="40%"></p><script type="math/tex; mode=display">v_{*}(s) = \max \limits_{a} q_{*}(s, a)\tag{5}\label{eq:state-bellman-optimal-equation}</script><p>与公式\eqref{eq:mdp-state-value-function}的贝尔曼期望方程进行比较, 此时不再取均值, 而是取最大值.</p><hr><p>$q_{*}$与公式\eqref{eq:mdp-action-value-function}类似:</p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m10t6s7vj208003a747.jpg" width="40%" height="40%"></p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)\tag{6}\label{eq:action-bellman-optimal-equation}</script><hr><p><strong>状态值函数的贝尔曼最优方程</strong></p><p><img src="https://wx3.sinaimg.cn/large/8662e3cegy1g0m14a2fenj208m04xq2x.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:action-bellman-optimal-equation}代入公式\eqref{eq:state-bellman-optimal-equation}可得 $v_{*}$ 的贝尔曼最优方程:</p><script type="math/tex; mode=display">v _ { * } ( s ) = \max _ { a } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script><hr><p><strong>动作值函数的贝尔曼最优方程</strong></p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0m18irqg7j208804bgll.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:state-bellman-optimal-equation}代入公式\eqref{eq:action-bellman-optimal-equation}可得 $q_{*}$ 的贝尔曼最优方程:</p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \max _ { a ^ { \prime } } q _ { * } \left( s ^ { \prime } , a ^ { \prime } \right)</script><hr><p>例子:</p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m1ato6q2j20d70atjs3.jpg" alt="Student MDP贝尔曼最优方程" width="50%" height="50%"></p><hr><h3 id="贝尔曼最优方程的求解"><a href="#贝尔曼最优方程的求解" class="headerlink" title="贝尔曼最优方程的求解"></a>贝尔曼最优方程的求解</h3><p>贝尔曼最优方程<strong>不是线性的</strong>(因为有取$max$操作), 因此没有封闭解(Closed-form solution).</p><p>通常采用迭代求解方法:</p><ul><li>值迭代(Value Iteration)</li><li>策略迭代(Policy Iteration)</li><li>Q-Learning</li><li>Sarsa</li></ul><h2 id="MDP的扩展"><a href="#MDP的扩展" class="headerlink" title="MDP的扩展"></a>MDP的扩展</h2><ul><li>无穷和连续的MDPs</li><li>部分可观测的MDPs</li><li>不折扣, 平均奖励MDPs</li></ul><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第三章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf" target="_blank" rel="noopener">Lecture2: Markov Decision Processes</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;在上一篇文章 &lt;a href=&quot;https://orzyt.cn/posts/introduction-to-rl&quot;&gt;强化学习（一）：强化学习简介&lt;/a&gt; 中, 我们介绍了强化学习的一些基本概念.&lt;/p&gt;
&lt;p&gt;本文将介绍用来对强化学习问题进行建模的&lt;strong&gt;马尔可夫决策过程(Markov Decision Processes, MDPs)&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="马尔可夫决策过程" scheme="https://orzyt.cn/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（一）：强化学习简介</title>
    <link href="https://orzyt.cn/posts/introduction-to-rl/"/>
    <id>https://orzyt.cn/posts/introduction-to-rl/</id>
    <published>2019-02-25T14:56:13.000Z</published>
    <updated>2019-03-13T12:07:23.473Z</updated>
    
    <content type="html"><![CDATA[<hr><p>本文主要介绍强化学习中的一些基本概念.</p><a id="more"></a><h2 id="强化学习的特征"><a href="#强化学习的特征" class="headerlink" title="强化学习的特征"></a>强化学习的特征</h2><p>作为机器学习的一个分支，强化学习主要的特征为:</p><ul><li><p>无监督,仅有奖励信号；</p></li><li><p>反馈有延迟,不是瞬时的;</p></li><li><p>时间是重要的(由于是时序数据,不是独立同分布的);</p></li><li><p>Agent的动作会影响后续得到的数据;</p></li></ul><hr><h2 id="强化学习的概念"><a href="#强化学习的概念" class="headerlink" title="强化学习的概念"></a>强化学习的概念</h2><h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>奖励(Rewards) $R_t$ 是一个标量的反馈信号,表示Agent在 $t$ 时刻的表现如何.</p><p><strong>Agent的目标</strong>: 最大化累积奖励(maximise cumulative reward).</p><p>强化学习基于<strong>奖励假设(reward hypothesis)</strong>.</p><blockquote><p><strong>奖励假设(Reward Hypothesis)</strong>:<br>所有强化学习任务的目标都可以被描述为最大化期望累积奖励.</p></blockquote><hr><h3 id="序贯决策"><a href="#序贯决策" class="headerlink" title="序贯决策"></a>序贯决策</h3><p><strong>序贯决策(Sequential Decision Making)的目标</strong>: 选择合适的动作最大化将来的累积奖励.</p><ul><li>动作可能会产生长期后果；</li><li>奖励会有延迟性;</li><li>牺牲立即回报可能会获得更多的长期回报.</li></ul><hr><h3 id="智能体和环境"><a href="#智能体和环境" class="headerlink" title="智能体和环境"></a>智能体和环境</h3><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k2ozf0lzj20aq0bxtb1.jpg" alt="Agent和环境" width="35%" height="35%"></p><p>智能体(Agent)在每个时刻$t$会:</p><ul><li>执行动作(Action)$A_t$;</li><li>接收观测(Observation)$O_t$;</li><li>接收标量奖励(Reward)$R_t$.</li></ul><p>而环境(Environment)则会:</p><ul><li>接收动作(Action)$A_t$;</li><li>产生观测(Observation)$O_{t+1}$;</li><li>产生标量奖励(Reward)$R_{t+1}$.</li></ul><hr><h3 id="历史与状态"><a href="#历史与状态" class="headerlink" title="历史与状态"></a>历史与状态</h3><blockquote><p><strong>历史(History):</strong>由一系列观测,动作和奖励构成.</p></blockquote><script type="math/tex; mode=display">H_t = O_1, R_1, A_1, \dots, A_{t-1}, O_t, R_t</script><p>下一步将发生什么取决于历史:</p><ul><li>智能体选择的action;</li><li>环境选择的observations/rewards.</li></ul><blockquote><p><strong>状态(State)</strong>:用来决定接下来会发生什么的信息.</p></blockquote><p><strong>状态是历史的函数:</strong></p><script type="math/tex; mode=display">S_t = f(H_t)</script><hr><h4 id="环境状态"><a href="#环境状态" class="headerlink" title="环境状态"></a>环境状态</h4><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3akygqpj20b20cptb5.jpg" alt="环境状态" width="35%" height="35%"></p><p>环境状态 $S_{t}^{e}$ 是环境的私有表示,通常对于智能体来说该状态不可见.</p><p>即使$S_{t}^{e}$可见,也可能包含不相关信息.</p><hr><h4 id="智能体状态"><a href="#智能体状态" class="headerlink" title="智能体状态"></a>智能体状态</h4><p><img src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k3e8bw4aj20b00cx419.jpg" alt="智能体状态" width="35%" height="35%"></p><p>智能体状态 $S_{t}^{a}$ 是智能体的内部表示,包含其用来决定下一步动作的信息,也是强化学习算法使用的信息.</p><p>可以写成历史的函数: $S_{t}^{a} = f(H_t)$</p><hr><h4 id="信息状态"><a href="#信息状态" class="headerlink" title="信息状态"></a>信息状态</h4><p><strong>信息状态(也称为马尔科夫状态)</strong>: 包含历史中所有有用的信息.</p><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3nzaa8yj20mn0593ym.jpg" alt="马尔科夫状态定义" width="60%" height="60%"></p><p>马尔科夫状态表明: <strong>未来只与现在有关,而与过去无关.</strong></p><p>其中,<strong>环境状态$S_t^e$</strong>和<strong>历史$H_t$</strong>具有马尔科夫性质.</p><hr><h4 id="Rat-Example"><a href="#Rat-Example" class="headerlink" title="Rat Example"></a>Rat Example</h4><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0k3trc5qxj20ny0doq88.jpg" alt="Rat Example" width="60%" height="60%"></p><ul><li><p>假如个体状态=序列中的后三个事件(不包括电击、获得奶酪，下同),事件序列3的结果会是什么? (答案是：电击)</p></li><li><p>假如个体状态=亮灯、响铃和拉电闸各自事件发生的次数,那么事件序列3的结果又是什么? (答案是：奶酪)</p></li><li><p>假如个体状态=完整的事件序列,那结果又是什么? (答案是：未知)</p></li></ul><hr><h4 id="完全可观测环境"><a href="#完全可观测环境" class="headerlink" title="完全可观测环境"></a>完全可观测环境</h4><p><strong>完全可观测性(Full observability):</strong> 智能体可以直接观测到环境状态,即</p><script type="math/tex; mode=display">O_t = S_t^a = S_t^e</script><ul><li>智能体状态 = 环境状态 = 信息状态</li><li>实际上是马尔科夫决策过程(Markov Decision Process, MDP)</li></ul><hr><h4 id="部分可观测环境"><a href="#部分可观测环境" class="headerlink" title="部分可观测环境"></a>部分可观测环境</h4><p><strong>部分可观测性(Partial observability):</strong> 智能体不能够直接观测到环境.</p><p>如,机器人不能通过摄像头得知自身的绝对位置.</p><ul><li>智能体状态 $\neq$ 环境状态</li><li>部分可观测马尔科夫决策过程(POMDP)</li></ul><p>此时,智能体必须构建其自身的状态表示 $S_t^a$,比如:</p><ul><li>完全的历史: $S_t^a = H_t$;</li><li>环境状态的置信度: $S _ { t } ^ { a } = \left( \mathbb { P } \left[ S _ { t } ^ { e } = s ^ { 1 } \right] , \ldots , \mathbb { P } \left[ S _ { t } ^ { e } = s ^ { n } \right] \right)$;</li><li>循环神经网络: $S_t^a = \sigma \left(S_{t-1}^{a}W_{s} + O_{t}W_{o}\right)$</li></ul><hr><h2 id="智能体的构成"><a href="#智能体的构成" class="headerlink" title="智能体的构成"></a>智能体的构成</h2><p>智能体主要包含以下几种成分:</p><ul><li><strong>策略(Policy)</strong>: 智能体的行为函数;</li><li><strong>值函数(Value Function)</strong>: 每个state或action的好坏;</li><li><strong>模型(Model)</strong>: 智能体对环境的表示.</li></ul><hr><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><ul><li>策略(Policy)是智能体的行为;</li><li>是<strong>状态</strong>到<strong>动作</strong>的映射;</li><li>确定性策略: $a = \pi(s)$;</li><li>随机性策略: $\pi(a|s) = \mathbb{P} \left[ A_{t} = a | S_{t} = s\right]$</li></ul><hr><h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数(Value Function)是对于未来奖励的预测.</p><ul><li>用于评价状态的好坏;</li><li>因此可以用来选择动作.</li></ul><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_{t} = s \right]</script><hr><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型(Model)用来预测环境接下来会做什么.</p><ul><li>$\mathcal{P}$: 预测下一个状态.<script type="math/tex; mode=display">\mathcal{P}_{ss'}^{a} = \mathbb{P} \left[ S_{t+1} = s' | S_{t} = s, A_{t} = a\right]</script></li><li>$\mathcal{R}$: 预测下一个(立即)奖励.<script type="math/tex; mode=display">\mathcal{R}_{s}^{a} = \mathbb{E} \left[ R_{t+1} | S_{t} = s, A_{t} = a\right]</script></li></ul><hr><h3 id="Maze-Example"><a href="#Maze-Example" class="headerlink" title="Maze Example"></a>Maze Example</h3><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k4norup9j20mj092dg5.jpg" alt="Maze Example" width="60%" height="60%"></p><hr><p><strong>策略表示:</strong></p><p>箭头表示每个状态的策略 $\pi(s)$.</p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k4u9pdcdj20f10c5q38.jpg" alt="Maze Example: Policy" width="40%" height="40%"></p><hr><p><strong>值函数表示:</strong></p><p>数值表示每个状态的值 $v_{\pi}(s)$.</p><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k4w9vn7wj20f60cct8y.jpg" alt="Maze Example: Value Function" width="40%" height="40%"></p><hr><p><strong>模型表示:</strong></p><p>智能体可能对环境建立内部模型</p><ul><li>网格布局表示转移模型 $\mathcal{P}_{ss’}^{a}$;</li><li>数值表示每个状态的立即奖励 $\mathcal{R}_{s}^{a}$.</li></ul><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k51h023dj20c109kt8o.jpg" alt="Maze Example: Value Function" width="40%" height="40%"></p><hr><h3 id="智能体的分类"><a href="#智能体的分类" class="headerlink" title="智能体的分类"></a>智能体的分类</h3><p>按智能体的成分分类:</p><ul><li>基于值函数(Value Based)</li><li>基于策略(Policy Based)</li><li>演员-评论家(Actor Critic)</li></ul><p>或者按有无模型分类:</p><ul><li>无模型(Model Free)</li><li>基于模型(Model Based)</li></ul><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k55tidg0j20f30eaab9.jpg" alt="智能体分类" width="40%" height="40%"></p><hr><h2 id="强化学习的问题"><a href="#强化学习的问题" class="headerlink" title="强化学习的问题"></a>强化学习的问题</h2><h3 id="学习与规划"><a href="#学习与规划" class="headerlink" title="学习与规划"></a>学习与规划</h3><p><strong>强化学习</strong></p><ul><li>环境的初始状态未知;</li><li>智能体与环境进行交互;</li><li>智能体提升其策略.</li></ul><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0k5dprnn4j20p80ejn0q.jpg" alt="学习" width="60%" height="60%"></p><p><strong>规划</strong></p><ul><li>环境的模型已知;</li><li>智能体通过模型进行计算,无须与外部进行交互;</li><li>智能体提升其策略</li></ul><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k5eleerwj20ok0df0u3.jpg" alt="规划" width="60%" height="60%"></p><hr><h3 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h3><p>强化学习是一种试错(trial-and-error)学习.</p><p>智能体需要从与环境的交互中找到一种好的策略,同时不损失过多的奖励.</p><ul><li><strong>探索(Exploration):</strong> 从环境中寻找更多信息;</li><li><strong>利用(Exploitation):</strong> 利用已知信息使奖励最大化.</li></ul><p>探索和利用同等重要,即使根据已有信息选择出的最优动作可以得到不错的奖励,不妨尝试全新的动作对环境进行探索,也许可以得到更好的结果.</p><hr><h3 id="预测和控制"><a href="#预测和控制" class="headerlink" title="预测和控制"></a>预测和控制</h3><ul><li><strong>预测(Prediction):</strong> 对未来进行评估.</li></ul><p><img src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k5ryo676j20nz0ds0tn.jpg" alt="Gridworld Example: Prediction" width="60%" height="60%"></p><hr><ul><li><strong>控制(Control):</strong> 最优化未来的结果.</li></ul><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k5r6vdmgj20nv0eot9w.jpg" alt="Gridworld Example: Control" width="60%" height="60%"></p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第一章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf" target="_blank" rel="noopener">Lecture1: Introduction to Reinforcement Learning</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;本文主要介绍强化学习中的一些基本概念.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode637 Average of Levels in Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode637-average-of-levels-in-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode637-average-of-levels-in-binary-tree/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/average-of-levels-in-binary-tree/" target="_blank" rel="noopener">LeetCode637 Average of Levels in Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的层次遍历,使用空指针作为每层的分界.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; averageOfLevels(TreeNode* root) &#123;</span><br><span class="line">        <span class="built_in">queue</span>&lt;TreeNode*&gt; que;</span><br><span class="line">        que.push(root); que.push(<span class="literal">nullptr</span>);</span><br><span class="line">        <span class="keyword">double</span> sum = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; ret;</span><br><span class="line">        <span class="keyword">while</span> (!que.empty()) &#123;</span><br><span class="line">            TreeNode* u = que.front(); que.pop();</span><br><span class="line">            <span class="keyword">if</span> (u == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                ret.push_back(sum / cnt);</span><br><span class="line">                sum = cnt = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span> (!que.empty()) que.push(<span class="literal">nullptr</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                sum += u-&gt;val; cnt++;</span><br><span class="line">                <span class="keyword">if</span> (u-&gt;left) que.push(u-&gt;left);</span><br><span class="line">                <span class="keyword">if</span> (u-&gt;right) que.push(u-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/average-of-levels-
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode654 Maximum Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode654-maximum-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode654-maximum-binary-tree/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/maximum-binary-tree/" target="_blank" rel="noopener">LeetCode654 Maximum Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>递归构造二叉搜索树,树的左右儿子都比父结点小.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">helper</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        TreeNode* node = <span class="keyword">new</span> TreeNode(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">int</span> num = nums[l], id = l;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = l; i &lt;= r; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (num &lt; nums[i]) &#123;</span><br><span class="line">                num = nums[i];</span><br><span class="line">                id = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        node-&gt;val = num;</span><br><span class="line">        <span class="keyword">if</span> (l &lt;= id - <span class="number">1</span>) node-&gt;left = helper(nums, l, id - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (id + <span class="number">1</span> &lt;= r) node-&gt;right = helper(nums, id + <span class="number">1</span>, r);</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">TreeNode* <span class="title">constructMaximumBinaryTree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> helper(nums, <span class="number">0</span>, nums.size() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/maximum-binary-tre
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode657 Robot Return to Origin</title>
    <link href="https://orzyt.cn/posts/leetcode657-robot-return-to-origin/"/>
    <id>https://orzyt.cn/posts/leetcode657-robot-return-to-origin/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/robot-return-to-origin/" target="_blank" rel="noopener">LeetCode657 Robot Return to Origin</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>判断上和下,左和右的次数是否相同即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">judgeCircle</span><span class="params">(<span class="built_in">string</span> moves)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; f;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; c: moves) f[c]++;</span><br><span class="line">        <span class="keyword">return</span> f[<span class="string">'U'</span>] == f[<span class="string">'D'</span>] &amp;&amp; f[<span class="string">'L'</span>] == f[<span class="string">'R'</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/robot-return-to-or
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode661 Image Smoother</title>
    <link href="https://orzyt.cn/posts/leetcode661-image-smoother/"/>
    <id>https://orzyt.cn/posts/leetcode661-image-smoother/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/image-smoother/" target="_blank" rel="noopener">LeetCode661 Image Smoother</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>按题意模拟即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dx[<span class="number">9</span>] = &#123;<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dy[<span class="number">9</span>] = &#123;<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; imageSmoother(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; M) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = M.size(), m = M[<span class="number">0</span>].size();</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ret(n, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; m; ++j) &#123;</span><br><span class="line">                <span class="keyword">int</span> sum = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">9</span>; ++k) &#123;</span><br><span class="line">                    <span class="keyword">int</span> x = i + dx[k], y = j + dy[k];</span><br><span class="line">                    <span class="keyword">if</span> (x &lt; <span class="number">0</span> || x &gt;= n || y &lt; <span class="number">0</span> || y &gt;= m) <span class="keyword">continue</span>;</span><br><span class="line">                    sum += M[x][y];</span><br><span class="line">                    cnt++;</span><br><span class="line">                &#125;</span><br><span class="line">                ret[i].push_back(sum / cnt);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/image-smoother/&quot; t
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode665 Non-decreasing Array</title>
    <link href="https://orzyt.cn/posts/leetcode665-non-decreasing-array/"/>
    <id>https://orzyt.cn/posts/leetcode665-non-decreasing-array/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/non-decreasing-array/" target="_blank" rel="noopener">LeetCode665 Non-decreasing Array</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>判断只修改一个数能否使得数组非递减.</p><p>首先计算数组从左往右能延伸的距离$l$,从右往左能延伸的距离$r$.</p><p>可行的情况有:</p><ul><li>$ r \leq l $</li><li>$r == l + 1$ 且 满足下列情况之一<ul><li>r 为最后一位</li><li>l位置的值 $ \leq $ r + 1位置的值</li><li>l为首位 </li><li>l-1位置的值 $ \leq $ r 位置的值</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">checkPossibility</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> siz = nums.size(), l = <span class="number">0</span>, r = siz - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (l + <span class="number">1</span> &lt; siz &amp;&amp; nums[l] &lt;= nums[l + <span class="number">1</span>]) l++;</span><br><span class="line">        <span class="keyword">while</span> (r &gt; <span class="number">0</span> &amp;&amp; nums[r - <span class="number">1</span>] &lt;= nums[r]) r--;</span><br><span class="line">        <span class="keyword">return</span> r &lt;= l || (r == l + <span class="number">1</span> &amp;&amp; ((nums[l] &lt;= nums[r + <span class="number">1</span>] || r == siz - <span class="number">1</span>) || (l == <span class="number">0</span> || nums[l - <span class="number">1</span>] &lt;= nums[r])));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/non-decreasing-arr
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode617 Merge Two Binary Trees</title>
    <link href="https://orzyt.cn/posts/leetcode617-merge-two-binary-trees/"/>
    <id>https://orzyt.cn/posts/leetcode617-merge-two-binary-trees/</id>
    <published>2019-02-08T09:02:05.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/merge-two-binary-trees/" target="_blank" rel="noopener">LeetCode617 Merge Two Binary Trees</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的合并.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">mergeTrees</span><span class="params">(TreeNode* t1, TreeNode* t2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (t1 &amp;&amp; t2) &#123;</span><br><span class="line">            t1-&gt;val += t2-&gt;val;</span><br><span class="line">            t1-&gt;left = mergeTrees(t1-&gt;left, t2-&gt;left);</span><br><span class="line">            t1-&gt;right = mergeTrees(t1-&gt;right, t2-&gt;right);</span><br><span class="line">            <span class="keyword">return</span> t1;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">return</span> t1 ? t1 : t2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/merge-two-binary-t
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode606 Construct String from Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode606-construct-string-from-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode606-construct-string-from-binary-tree/</id>
    <published>2019-02-08T09:01:50.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/construct-string-from-binary-tree/" target="_blank" rel="noopener">LeetCode606 Construct String from Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的简单遍历.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">dfs</span><span class="params">(TreeNode* t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (t == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">if</span> (t-&gt;left == <span class="literal">NULL</span> &amp;&amp; t-&gt;right == <span class="literal">NULL</span>) <span class="keyword">return</span> to_string(t-&gt;val);</span><br><span class="line">        <span class="built_in">string</span> ret = to_string(t-&gt;val) + <span class="string">"("</span> + dfs(t-&gt;left) + <span class="string">")"</span>;</span><br><span class="line">        <span class="keyword">if</span> (t-&gt;right) ret += <span class="string">"("</span> + dfs(t-&gt;right) + <span class="string">")"</span>;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">tree2str</span><span class="params">(TreeNode* t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> dfs(t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/construct-string-f
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode599 Minimum Index Sum of Two Lists</title>
    <link href="https://orzyt.cn/posts/leetcode599-minimum-index-sum-of-two-lists/"/>
    <id>https://orzyt.cn/posts/leetcode599-minimum-index-sum-of-two-lists/</id>
    <published>2019-02-08T09:01:34.000Z</published>
    <updated>2019-02-08T10:25:59.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/minimum-index-sum-of-two-lists/" target="_blank" rel="noopener">LeetCode599 Minimum Index Sum of Two Lists</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>找出交集中下标和最小值.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; findRestaurant(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list1, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list2) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; ans;</span><br><span class="line">        <span class="keyword">int</span> n = list1.size(), m = list2.size();</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; hs;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) hs[list1[i]] = i;</span><br><span class="line">        <span class="keyword">int</span> minSum = INT_MAX;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (hs.count(list2[i])) &#123;</span><br><span class="line">                <span class="keyword">int</span> j = hs[list2[i]];</span><br><span class="line">                <span class="keyword">if</span> (i + j &lt; minSum) &#123;</span><br><span class="line">                    minSum = i + j;</span><br><span class="line">                    ans.clear();</span><br><span class="line">                    ans.push_back(list2[i]);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i + j == minSum) &#123;</span><br><span class="line">                    ans.push_back(list2[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/minimum-index-sum-
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode38 Count and Say</title>
    <link href="https://orzyt.cn/posts/leetcode38-count-and-say/"/>
    <id>https://orzyt.cn/posts/leetcode38-count-and-say/</id>
    <published>2019-02-08T09:00:37.000Z</published>
    <updated>2019-02-08T10:25:59.942Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/count-and-say/" target="_blank" rel="noopener">LeetCode38 Count and Say</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>按照规则模拟即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">countAndSay</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">string</span> s = <span class="string">"1"</span>, ans = s;</span><br><span class="line">        <span class="keyword">while</span> (--n) &#123;</span><br><span class="line">            ans.clear();</span><br><span class="line">            <span class="keyword">int</span> len = s.size();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; ++i) &#123;</span><br><span class="line">                <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">while</span> (s[i] == s[i + <span class="number">1</span>] &amp;&amp; i + <span class="number">1</span> &lt; len) &#123;</span><br><span class="line">                    count++;</span><br><span class="line">                    i++;</span><br><span class="line">                &#125;</span><br><span class="line">                ans += to_string(count) + s[i];</span><br><span class="line">            &#125;</span><br><span class="line">            s = ans;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/count-and-say/&quot; ta
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】深度人脸识别综述</title>
    <link href="https://orzyt.cn/posts/deep-face-recognition/"/>
    <id>https://orzyt.cn/posts/deep-face-recognition/</id>
    <published>2019-01-22T06:04:24.000Z</published>
    <updated>2019-02-26T11:20:51.367Z</updated>
    
    <content type="html"><![CDATA[<hr><p><strong>论文题目</strong>：《Deep Face Recognition: A Survey》</p><p><strong>论文作者</strong>：Mei Wang, Weihong Deng</p><p><strong>论文链接</strong>：<a href="http://cn.arxiv.org/pdf/1804.06655.pdf" target="_blank" rel="noopener">http://cn.arxiv.org/pdf/1804.06655.pdf</a></p><hr><a id="more"></a><p>随着2012年AlexNet赢得了ImageNet挑战赛的冠军后，深度学习技术在各个领域都发挥着重要的作用，极大地提升了许多任务的SOTA。2014年，DeepFace<sup><a href="#fn_1" id="reffn_1">1</a></sup>首次在著名的非受限环境人脸数据集——LFW上取得了与人类相媲美的准确率（DeepFace: 97.35% vs. Human: 97.53%）。因此，本文主要关注深度学习技术在人脸识别领域的应用与发展。</p><h2 id="概念和术语"><a href="#概念和术语" class="headerlink" title="概念和术语"></a>概念和术语</h2><p>人脸系统一般包括三个部分：</p><ul><li><p><strong>人脸检测（face detection）</strong>：对于一幅图像，检测其中人脸的位置；</p></li><li><p><strong>人脸对齐（face alignment）</strong>：根据人脸关键点，将人脸对齐到一个典型的角度；</p></li><li><p><strong>人脸识别（face recognition</strong>）：包括人脸处理、人脸表示和人脸匹配部分。</p></li></ul><p><img alt="人脸系统示意图" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k1j4up5oj21cb0ik489.jpg" width="100%" height="100%"></p><ul><li><p><strong>训练集（training set）</strong>：用于训练系统的人脸集；</p></li><li><p><strong>注册集（gallery set）</strong>：提前注册在系统中用于比对的标准人脸集；</p></li><li><p><strong>测试集（probe set）</strong>：用于测试的人脸集。</p></li></ul><p>人脸识别任务主要包括：</p><ul><li><p><strong>人脸认证（face identification）</strong>：为<strong>1:N</strong>的问题。通过计算测试个体与注册集个体的相似度，判断出当前测试个体的身份。根据测试集中的个体是否出现在注册集中，可分为<strong>闭集（closed-set）</strong>和<strong>开集（open-set）</strong>问题。</p></li><li><p><strong>人脸验证（face verification）</strong>：为<strong>1:1</strong>的问题。对测试集和验证集中的个体进行两两比对，判断是否是同一个体。</p></li></ul><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="主流结构"><a href="#主流结构" class="headerlink" title="主流结构"></a>主流结构</h3><p>在人脸识别问题中，主流的网络结构基本上都借鉴于物体分类问题，一直从AlexNet到SENet。</p><p>在2014年，DeepFace<sup><a href="#fn_1" id="reffn_1">1</a></sup>首次使用九层的卷积神经网络，经过3D人脸对齐处理，在LFW上达到了97.35%的准确率。在2015年，FaceNet<sup><a href="#fn_9" id="reffn_9">9</a></sup>在一个很大的私人数据集上训练GoogLeNet，采用triplet loss，得到99.63%的准确率。同年，VGGface<sup><a href="#fn_10" id="reffn_10">10</a></sup>从互联网中收集了一个大的数据集，并在其上训练VGGNet，得到了98.95%的准确率。在2017年，SphereFace<sup><a href="#fn_11" id="reffn_11">11</a></sup>使用64层的ResNet结构，采用angular softmax（A-softmax）loss，得到99.42%的准确率。在2017年末，VGGFace2<sup><a href="#fn_12" id="reffn_12">12</a></sup>作为一个新人脸的数据集被引入，同时使用SENet进行训练，在IJB-A和IJB-B上都取得SOTA。</p><p><img alt="主流网络结构的演变" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1jnv2szj210109oacr.jpg" width="90%" height="90%"></p><ul><li><p><code>AlexNet</code><sup><a href="#fn_2" id="reffn_2">2</a></sup>：AlexNet包括五个卷积层和三个全连接层，并且集成了如ReLU、dropout、数据增强等技术；</p></li><li><p><code>VGGNet</code><sup><a href="#fn_3" id="reffn_3">3</a></sup>：使用3×3卷积核，且每经过2×2的池化后特征图数量加倍，网络深度为16-19层；</p></li><li><p><code>GoogLeNet</code><sup><a href="#fn_4" id="reffn_4">4</a></sup>：提出了inception module，对不同尺度的特征图进行混合；</p></li><li><p><code>ResNet</code><sup><a href="#fn_5" id="reffn_5">5</a></sup>：通过学习残差表示，使得训练更深网络成为可能；</p></li><li><p><code>SENet</code><sup><a href="#fn_6" id="reffn_6">6</a></sup>：提出了Squeeze-and-Excitation操作，通过显式建模channel之间的相互依赖性，自适应地重新校准channel间的特征响应。</p></li></ul><p><img alt="主流网络结构示意图" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1jytblsj20of0kgwhp.jpg" width="60%" height="60%"></p><h3 id="特殊结构"><a href="#特殊结构" class="headerlink" title="特殊结构"></a>特殊结构</h3><ul><li><p><code>Light CNN</code><sup><a href="#fn_7" id="reffn_7">7</a></sup></p></li><li><p><code>bilinear CNN</code><sup><a href="#fn_8" id="reffn_8">8</a></sup></p></li><li><p>…</p></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在一开始，人们使用和物体分类同样的基于交叉熵的softmax loss，后来发现其不适用于人脸特征的学习，于是开始探索更具有判别性的loss。</p><p><img alt="人脸损失函数的演变" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1kfbt7kj217u0e8dlk.jpg" width="90%" height="90%"></p><p><img alt="不同方法在LFW数据集上的准确率" src="https://wx2.sinaimg.cn/large/8662e3cegy1g0k1kr4sx8j215r0k9wlm.jpg" width="90%" height="90%"></p><h3 id="基于欧几里德距离"><a href="#基于欧几里德距离" class="headerlink" title="基于欧几里德距离"></a>基于欧几里德距离</h3><hr><h4 id="contrastive-loss"><a href="#contrastive-loss" class="headerlink" title="contrastive loss"></a>contrastive loss</h4><p>相关文献：</p><ul><li><strong>《Deep learning face representation by joint identification-verification》</strong></li><li><strong>《Deepid3: Face recognition with very deep neural networks》</strong></li></ul><p>DeepID系列使用的loss。</p><script type="math/tex; mode=display">\operatorname { Verif } \left( f _ { i } , f _ { j } , y _ { i j } , \theta _ { v e } \right) = \left\{ \begin{array} { l l } { \frac { 1 } { 2 } \left\| f _ { i } - f _ { j } \right\| _ { 2 } ^ { 2 } } & { \text { if } y _ { i j } = 1 } \\ { \frac { 1 } { 2 } \max \left( 0 , m - \left\| f _ { i } - f _ { j } \right\| _ { 2 } \right) ^ { 2 } } & { \text { if } y _ { i j } = - 1 } \end{array} \right.</script><hr><h4 id="triplet-loss"><a href="#triplet-loss" class="headerlink" title="triplet loss"></a>triplet loss</h4><p>相关文献：</p><ul><li><strong>《Facenet: A unified embedding for face recognition and clustering》</strong></li></ul><script type="math/tex; mode=display">\mathcal{L} = \sum _ { i } ^ { N } \left[ \left\| f \left( x _ { i } ^ { a } \right) - f \left( x _ { i } ^ { p } \right) \right\| _ { 2 } ^ { 2 } - \left\| f \left( x _ { i } ^ { a } \right) - f \left( x _ { i } ^ { n } \right) \right\| _ { 2 } ^ { 2 } + \alpha \right] _ { + }</script><p><img alt="triplet loss示意图" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k1l9pm84j20x007ht9n.jpg" width="70%" height="70%"></p><hr><h4 id="center-loss"><a href="#center-loss" class="headerlink" title="center loss"></a>center loss</h4><p>相关文献：</p><ul><li><strong>《A Discriminative Feature Learning Approach for Deep Face Recognition》</strong></li></ul><script type="math/tex; mode=display">\begin{aligned} \mathcal { L } & = \mathcal { L } _ { S } + \lambda \mathcal { L } _ { C } \\ & = - \sum _ { i = 1 } ^ { m } \log \frac { e ^ { W _ { y _ { i } } ^ { T } \boldsymbol { x } _ { i } + b _ { y _ { i } } } } { \sum _ { j = 1 } ^ { n } e ^ { W _ { j } ^ { T } \boldsymbol { x } _ { i } + b _ { j } } } + \frac { \lambda } { 2 } \sum _ { i = 1 } ^ { m } \left\| \boldsymbol { x } _ { i } - \boldsymbol { c } _ { y _ { i } } \right\| _ { 2 } ^ { 2 } \end{aligned}</script><p><img alt="center loss示意图" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k1lnsllgj20tr0m1qdq.jpg" width="60%" height="60%"></p><hr><h4 id="range-loss"><a href="#range-loss" class="headerlink" title="range loss"></a>range loss</h4><p>相关文献：</p><ul><li><strong>《Range loss for deep face recognition with long-tail》</strong></li></ul><script type="math/tex; mode=display">\mathcal { L } _ { R } = \alpha \mathcal { L } _ { R _ { i n t r a } } + \beta \mathcal { L } _ { R _ { i n t e r } }</script><script type="math/tex; mode=display">\mathcal { L } _ { R _ { i n t r a } } = \sum _ { i \subseteq I } \mathcal { L } _ { R _ { i n t r a }}^ { i } = \sum _ { i \subseteq I } \frac { k } { \sum _ { j = 1 } ^ { k } \frac { 1 } { \mathcal { D } _ { j } } }</script><script type="math/tex; mode=display">\begin{aligned} \mathcal { L } _ { R _ { \text {inter} } } & = \max \left( m - \mathcal { D } _ { C e n t e r } , 0 \right) \\ & = \max \left( m - \left\| \overline { x } _ { \mathcal { Q } } - \overline { x } _ { \mathcal { R } } \right\| _ { 2 } ^ { 2 } , 0 \right) \end{aligned}</script><script type="math/tex; mode=display">\mathcal { L } = \mathcal { L } _ { M } + \lambda \mathcal { L } _ { R } = - \sum _ { i = 1 } ^ { M } \log \frac { e ^ { W _ { y _ { i } } ^ { T } x _ { i } + b _ { v _ { i } } } } { \sum _ { j = 1 } ^ { n } e ^ { W _ { j } ^ { T } x _ { i } + b _ { j } } } + \lambda \mathcal { L } _ { R }</script><hr><h4 id="center-invariant-loss"><a href="#center-invariant-loss" class="headerlink" title="center-invariant loss"></a>center-invariant loss</h4><p>相关文献：</p><ul><li><strong>《Deep face recognition with center invariant loss》</strong></li></ul><script type="math/tex; mode=display">\begin{aligned} L = & L _ { s } + \gamma L _ { I } + \lambda L _ { c } \\ = & - \log \left( \frac { e ^ { \mathbf { w } _ { y } ^ { T } \mathbf { x } _ { i } + b _ { y } } } { \sum _ { j = 1 } ^ { m } e ^ { \mathbf { w } _ { j } ^ { T } \mathbf { x } _ { i } + b _ { j } } } \right) + \frac { \gamma } { 4 } \left( \left\| \mathbf { c } _ { y } \right\| _ { 2 } ^ { 2 } - \frac { 1 } { m } \sum _ { k = 1 } ^ { m } \left\| \mathbf { c } _ { k } \right\| _ { 2 } ^ { 2 } \right) ^ { 2 } \\ & + \frac { \lambda } { 2 } \left\| \mathbf { x } _ { i } - \mathbf { c } _ { y } \right\| ^ { 2 } \end{aligned}</script><p><img alt="center invariant loss示意图" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k1lvxp55j20zt0i4n3g.jpg" width="60%" height="60%"></p><hr><h3 id="基于角度-余弦间隔"><a href="#基于角度-余弦间隔" class="headerlink" title="基于角度/余弦间隔"></a>基于角度/余弦间隔</h3><hr><h4 id="L-Softmax-loss"><a href="#L-Softmax-loss" class="headerlink" title="L-Softmax loss"></a>L-Softmax loss</h4><p>相关文献：</p><ul><li><strong>《Large-margin softmax loss for convolutional neural networks》</strong></li></ul><script type="math/tex; mode=display">L _ { i } = - \log \left( \frac { e ^ { \left\| \boldsymbol { W } _ { y _ { i } } \right\| \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { y _ { i } } \right) } } { e ^ { \left\| \boldsymbol { W } _ { y _ { i } } \right\| \boldsymbol { w } \left( \theta _ { \boldsymbol { y } _ { i } } \right) } + \sum _ { j \neq y _ { i } } e ^ { \left\| \boldsymbol { W } _ { j } \right\| \left\| \boldsymbol { x } _ { i } \right\| \cos \left( \theta _ { j } \right) } } \right)</script><script type="math/tex; mode=display">\psi ( \theta ) = ( - 1 ) ^ { k } \cos ( m \theta ) - 2 k , \quad \theta \in \left[ \frac { k \pi } { m } , \frac { ( k + 1 ) \pi } { m } \right]</script><script type="math/tex; mode=display">f _ { y _ { i } } = \frac { \lambda \left\| \boldsymbol { W } _ { y _ { i } } \right\| \left\| \boldsymbol { x } _ { i } \right\| \cos \left( \theta _ { y _ { i } } \right) + \left\| \boldsymbol { W } _ { y _ { i } } \right\| \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { \boldsymbol { y } _ { i } } \right) } { 1 + \lambda }</script><p><img alt="L-Softmax loss二分类示意图" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k1m4b63ij20fv0ib40x.jpg" width="50%" height="50%"></p><hr><h4 id="A-Softmax-loss"><a href="#A-Softmax-loss" class="headerlink" title="A-Softmax loss"></a>A-Softmax loss</h4><p>相关文献：</p><ul><li><strong>《Sphereface: Deep hypersphere embedding for face recognition》</strong></li></ul><script type="math/tex; mode=display">L _ { \mathrm { ang } } = \frac { 1 } { N } \sum _ { i } - \log \left( \frac { e ^ { \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { y _ { i } , i } \right) } } { e ^ { \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { y _ { i } } , i \right) } + \sum _ { j \neq y _ { i } } e ^ { \left\| \boldsymbol { x } _ { i } \right\| \cos \left( \theta _ { j , i } \right) } } \right)</script><script type="math/tex; mode=display">\psi \left( \theta _ { y _ { i } , i } \right) = ( - 1 ) ^ { k } \cos \left( m \theta _ { y _ { i } , i } \right) - 2 k</script><script type="math/tex; mode=display">\theta _ { y _ { i } , i } \in \left[ \frac { k \pi } { m } , \frac { ( k + 1 ) \pi } { m } \right] \text { and } k \in [ 0 , m - 1 ]</script><p><img alt="A-Softmax loss示意图" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k1mg2yrlj20hk0ca0v4.jpg" width="50%" height="50%"></p><hr><h4 id="AM-Softmax-loss"><a href="#AM-Softmax-loss" class="headerlink" title="AM-Softmax loss"></a>AM-Softmax loss</h4><p>相关文献：</p><ul><li><strong>《Additive margin softmax for face verification》</strong></li></ul><script type="math/tex; mode=display">\begin{aligned} \mathcal { L } _ { A M S } & = - \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \log \frac { e ^ { s \cdot \left( \cos \theta _ { y _ { i } } - m \right) } } { e ^ { s \cdot \left( \cos \theta _ { y _ { i } } - m \right) } + \sum _ { j = 1 , j \neq y _ { i } } ^ { c } e ^ { s \cdot c o s \theta _ { j } } } \\ & = - \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \log \frac { e ^ { s \cdot \left( W _ { y _ { i } } ^ { T } f _ { i } - m \right) } } { e ^ { s \cdot \left( W _ { y _ { i } } ^ { T } \boldsymbol { f } _ { i } - m \right) } + \sum _ { j = 1 , j \neq y _ { i } } ^ { c } e ^ { S W _ { j } ^ { T } \boldsymbol { f } _ { i } } } \end{aligned}</script><p><img alt="AM-Softmax loss示意图" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1mmsrz1j20n808dwfx.jpg" width="60%" height="60%"></p><hr><h4 id="CosFace"><a href="#CosFace" class="headerlink" title="CosFace"></a>CosFace</h4><p>相关文献：</p><ul><li><strong>《Cosface: Large margin cosine loss for deep face recognition》</strong></li></ul><script type="math/tex; mode=display">L _ { l m c } = \frac { 1 } { N } \sum _ { i } - \log \frac { e ^ { s \left( \cos \left( \theta _ { y _ { i } , i } \right) - m \right) } } { e ^ { s \left( \cos \left( \theta _ { y _ { i } } , i \right) - m \right) } + \sum _ { j \neq y _ { i } } e ^ { s \cos \left( \theta _ { j , i } \right) } }</script><script type="math/tex; mode=display">\begin{aligned} \text { subject to } \\  W & = \frac { W ^ { * } } { \left\| W ^ { * } \right\| } \\  x & = \frac { x ^ { * } } { \left\| x ^ { * } \right\| } \\ \cos \left( \theta _ { j } , i \right) & = W _ { j } ^ { T } x _ { i } \end{aligned}</script><p><img alt="CosFace示意图" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k1mtlmpmj20ck04sq3j.jpg" width="60%" height="60%"></p><hr><h4 id="ArcFace"><a href="#ArcFace" class="headerlink" title="ArcFace"></a>ArcFace</h4><p>相关文献：</p><ul><li><strong>《Arcface: Additive angular margin loss for deep face recognition》</strong></li></ul><script type="math/tex; mode=display">L  = - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \log \frac { e ^ { s \left( \cos \left( \theta _ { y _ { i } } + m \right) \right) } } { e ^ { s \left( \cos \left( \theta _ { y _ { i } } + m \right) \right) } + \sum _ { j = 1 , j \neq y _ { i } } ^ { n } e ^ { s \cos \theta _ { j } } }</script><p><img alt="ArcFace示意图" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1mz40h8j21640ad0wf.jpg" width="100%" height="100%"></p><hr><h3 id="Softmax及其变种"><a href="#Softmax及其变种" class="headerlink" title="Softmax及其变种"></a>Softmax及其变种</h3><hr><h4 id="L2-Softmax"><a href="#L2-Softmax" class="headerlink" title="L2-Softmax"></a>L2-Softmax</h4><p>相关文献：</p><ul><li><strong>《L2-constrained softmax loss for discriminative face verification》</strong></li></ul><script type="math/tex; mode=display">\begin{array} { l l } { \text { minimize } } & { - \frac { 1 } { M } \sum _ { i = 1 } ^ { M } \log \frac { e ^ { W _ { y _ { i } } ^ { T } f \left( \mathbf { x } _ { i } \right) + b _ { y _ { i } } } } { \sum _ { j = 1 } ^ { C } e ^ { W _ { j } ^ { T } f \left( \mathbf { x } _ { i } \right) + b _ { j } } } } \\ { \text { subject to } } & { \left\| f \left( \mathbf { x } _ { i } \right) \right\| _ { 2 } = \alpha , \forall i = 1,2 , \ldots M } \end{array}</script><hr><h4 id="Normface"><a href="#Normface" class="headerlink" title="Normface"></a>Normface</h4><p>相关文献：</p><ul><li><strong>《NormFace: L2 Hypersphere Embedding for Face Verification》</strong></li></ul><script type="math/tex; mode=display">\mathcal { L } _ { S' }  = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \log \frac { e ^ { s \tilde { W } _ { y _ { i } } ^ { T } \tilde { \mathbf { f } } _ { i } } } { \sum _ { j = 1 } ^ { n } e ^ { s \tilde { W } _ { j } ^ { T } \mathbf { f } _ { i } } }</script><script type="math/tex; mode=display">\tilde { \mathbf { x } } = \frac { \mathbf { x } } { \| \mathbf { x } \| _ { 2 } } = \frac { \mathbf { x } } { \sqrt { \sum _ { i } \mathbf { x } _ { i } ^ { 2 } + \epsilon } }</script><hr><h4 id="CoCo-loss"><a href="#CoCo-loss" class="headerlink" title="CoCo loss"></a>CoCo loss</h4><p>相关文献：</p><ul><li><strong>《Rethinking feature discrimination and polymerization for large-scale recognition》</strong></li></ul><script type="math/tex; mode=display">\mathcal { L } ^ { C O C O } \left( \boldsymbol { f } ^ { ( i ) } , \boldsymbol { c } _ { k } \right) = - \sum _ { i \in \mathcal { B } , k } t _ { k } ^ { ( i ) } \log p _ { k } ^ { ( i ) } = - \sum _ { i \in \mathcal { B } } \log p _ { l _ { i } } ^ { ( i ) }</script><script type="math/tex; mode=display">\hat { \boldsymbol { c } } _ { k } = \frac { \boldsymbol { c } _ { k } } { \left\| \boldsymbol { c } _ { k } \right\| } , \hat { \boldsymbol { f } } ^ { ( i ) } = \frac { \alpha \boldsymbol { f } ^ { ( i ) } } { \left\| \boldsymbol { f } ^ { ( i ) } \right\| } , p _ { k } ^ { ( i ) } = \frac { \exp \left( \hat { \boldsymbol { c } } _ { k } ^ { T } \cdot \hat { \boldsymbol { f } } ^ { ( i ) } \right) } { \sum _ { m } \exp \left( \hat { \boldsymbol { c } } _ { m } ^ { T } \cdot \hat { \boldsymbol { f } } ^ { ( i ) } \right) }</script><hr><h4 id="Ring-loss"><a href="#Ring-loss" class="headerlink" title="Ring loss"></a>Ring loss</h4><p>相关文献：</p><ul><li><strong>《Ring loss: Convex feature normalization for face recognition》</strong></li></ul><script type="math/tex; mode=display">L _ { R } = \frac { \lambda } { 2 m } \sum _ { i = 1 } ^ { m } \left( \left\| \mathcal { F } \left( \mathbf { x } _ { i } \right) \right\| _ { 2 } - R \right) ^ { 2 }</script><p><img alt="Ring loss示意图" src="https://ws1.sinaimg.cn/large/8662e3cegy1g0k1n6hmq9j20lo09g0wz.jpg" width="60%" height="60%"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. <strong>Deepface: Closing the gap to human-level performance in face verification</strong>. In CVPR, pages 1701–1708, 2014.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. A. Krizhevsky, I. Sutskever, and G. E. Hinton. <strong>Imagenet classification with deep convolutional neural networks</strong>. In NIPS, pages 1097–1105, 2012.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. K. Simonyan and A. Zisserman. <strong>Very deep convolutional networks for large-scale image recognition</strong>. arXiv preprint arXiv:1409.1556, 2014.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, et al. <strong>Going deeper with convolutions</strong>. In CVPR, 2015.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. K. He, X. Zhang, S. Ren, and J. Sun. <strong>Deep residual learning for image recognition</strong>. In CVPR, pages 770–778, 2016.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. J. Hu, L. Shen, and G. Sun. <strong>Squeeze-and-excitation networks</strong>. arXiv preprint arXiv:1709.01507, 2017.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. X. Wu, R. He, Z. Sun, and T. Tan. <strong>A light cnn for deep face representation with noisy labels</strong>. arXiv preprint arXiv:1511.02683, 2015.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. A. R. Chowdhury, T.-Y. Lin, S. Maji, and E. Learned-Miller. <strong>One-to-many face recognition with bilinear cnns</strong>. In WACV, pages 1–9. IEEE, 2016.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. F. Schroff, D. Kalenichenko, and J. Philbin. <strong>Facenet: A unified embedding for face recognition and clustering</strong>. In CVPR, pages 815–823, 2015.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. O. M. Parkhi, A. Vedaldi, A. Zisserman, et al. <strong>Deep face recognition</strong>. In BMVC, volume 1, page 6, 2015.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. <strong>Sphereface: Deep hypersphere embedding for face recognition</strong>. In CVPR, volume 1, 2017.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. <strong>Vggface2: A dataset for recognising faces across pose and age</strong>. arXiv preprint arXiv:1710.08092, 2017.<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;&lt;strong&gt;论文题目&lt;/strong&gt;：《Deep Face Recognition: A Survey》&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文作者&lt;/strong&gt;：Mei Wang, Weihong Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文链接&lt;/strong&gt;：&lt;a href=&quot;http://cn.arxiv.org/pdf/1804.06655.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://cn.arxiv.org/pdf/1804.06655.pdf&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="https://orzyt.cn/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://orzyt.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="人脸识别" scheme="https://orzyt.cn/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>2018-2019学年研一上学期总结</title>
    <link href="https://orzyt.cn/posts/2018-2019-fall-semester-summary/"/>
    <id>https://orzyt.cn/posts/2018-2019-fall-semester-summary/</id>
    <published>2019-01-21T05:43:42.000Z</published>
    <updated>2019-01-22T06:49:10.651Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文暂未公开，请输入密码访问" />    <label for="pass">本文暂未公开，请输入密码访问</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">∑(っ°Д°;)っ 密码错误！</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;本文暂未公开，请输入密码访问
    
    </summary>
    
      <category term="随笔" scheme="https://orzyt.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="研究生学习" scheme="https://orzyt.cn/tags/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅谈支持向量机的推导</title>
    <link href="https://orzyt.cn/posts/svm/"/>
    <id>https://orzyt.cn/posts/svm/</id>
    <published>2018-12-19T16:00:00.000Z</published>
    <updated>2019-03-13T11:40:08.825Z</updated>
    
    <content type="html"><![CDATA[<hr><iframe src="../../src/svm.html" scrolling="no" width="100%" height="11100px"></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;iframe src=&quot;../../src/svm.html&quot; scrolling=&quot;no&quot; width=&quot;100%&quot; height=&quot;11100px&quot;&gt;&lt;/iframe&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://orzyt.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="支持向量机" scheme="https://orzyt.cn/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
      <category term="SVM" scheme="https://orzyt.cn/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode575 Distribute Candies</title>
    <link href="https://orzyt.cn/posts/leetcode575-distribute-candies/"/>
    <id>https://orzyt.cn/posts/leetcode575-distribute-candies/</id>
    <published>2018-01-14T05:21:28.000Z</published>
    <updated>2019-02-08T10:26:00.494Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given an integer array with <strong>even</strong> length, where different numbers in this array represent different <strong>kinds</strong> of candies. Each number means one candy of the corresponding kind. You need to distribute these candies <strong>equally</strong> in number to brother and sister. Return the maximum number of <strong>kinds</strong> of candies the sister could gain.</p><p><strong>Note:</strong></p><ol><li>The length of the given array is in range [2, 10,000], and will be even.</li><li>The number in given array is in range [-100,000, 100,000].</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input: candies = [1,1,2,2,3,3]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation:</span><br><span class="line">There are three different kinds of candies (1, 2 and 3), and two candies for each kind.</span><br><span class="line">Optimal distribution: The sister has candies [1,2,3] and the brother has candies [1,2,3], too. </span><br><span class="line">The sister has three different kinds of candies.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: candies = [1,1,2,3]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: For example, the sister has candies [2,3] and the brother has candies [1,1]. </span><br><span class="line">The sister has two different kinds of candies, the brother has only one kind of candies.</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>有偶数个不同种类的糖果，将其平均分给两个人，问某人能够得到最多的种类数是多少</p><p>首先，用哈希表记录种类数，这是答案的上限，而一个人只能获得一半的糖果，所以这又是一个上限。</p><p>最终的答案为二者取最小值。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">distributeCandies</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candies)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; count;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> x: candies) count[x]++;</span><br><span class="line">        <span class="keyword">return</span> min(count.size(), candies.size() / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given an integer array with &lt;strong&gt;even&lt;/strong&gt; length, where differ
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="哈希表" scheme="https://orzyt.cn/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode566 Reshape the Matrix</title>
    <link href="https://orzyt.cn/posts/leetcode566-reshape-the-matrix/"/>
    <id>https://orzyt.cn/posts/leetcode566-reshape-the-matrix/</id>
    <published>2018-01-14T05:03:19.000Z</published>
    <updated>2019-02-08T10:26:00.386Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>In MATLAB, there is a very useful function called ‘reshape’, which can reshape a matrix into a new one with different size but keep its original data.</p><p>You’re given a matrix represented by a two-dimensional array, and two <strong>positive</strong> integers <strong>r</strong> and <strong>c</strong> representing the <strong>row</strong> number and <strong>column</strong> number of the wanted reshaped matrix, respectively.</p><p>The reshaped matrix need to be filled with all the elements of the original matrix in the same <strong>row-traversing</strong> order as they were.</p><p>If the ‘reshape’ operation with given parameters is possible and legal, output the new reshaped matrix; Otherwise, output the original matrix.</p><p><strong>Note:</strong></p><ol><li>The height and width of the given matrix is in range [1, 100].</li><li>The given r and c are all positive.</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">nums = </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">r = 1, c = 4</span><br><span class="line">Output: </span><br><span class="line">[[1,2,3,4]]</span><br><span class="line">Explanation:</span><br><span class="line">The row-traversing of nums is [1,2,3,4]. The new reshaped matrix is a 1 * 4 matrix, fill it row by row by using the previous list.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">nums = </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">r = 2, c = 4</span><br><span class="line">Output: </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">Explanation:</span><br><span class="line">There is no way to reshape a 2 * 2 matrix to a 2 * 4 matrix. So output the original matrix.</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>给定一个二维数组，模拟 MATLAB 中 <code>reshape</code> 函数的操作，若无法完成，则输出原数组</p><p>题中要求元素以<code>row-traversing</code>顺序访问，则$r$行$n$列的二维数组第$i$个访问到的元素所在的位置为($i / c$, $i \% c$)</p><p>利用这一关系，可以得到从原数组$nums$（$n$行$m$列）<code>reshape</code>成 新数组$vec$（$r$行$c$列）后的位置关系，$vec[i / c][i \% c] = nums[i / m][i \% m]$</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; matrixReshape(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; nums, <span class="keyword">int</span> r, <span class="keyword">int</span> c) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size(), m = nums[<span class="number">0</span>].size();</span><br><span class="line">        <span class="comment">// 元素数量不匹配，reshape失败</span></span><br><span class="line">        <span class="keyword">if</span> (n * m != r * c) <span class="keyword">return</span> nums;</span><br><span class="line">        <span class="comment">// 初始化二维vector</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; vec(r, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(c));</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; r * c; ++i) vec[i / c][i % c] = nums[i / m][i % m];</span><br><span class="line">        <span class="keyword">return</span> vec;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;In MATLAB, there is a very useful function called ‘reshape’, which can
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="数组" scheme="https://orzyt.cn/tags/%E6%95%B0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode563 Binary Tree Tilt</title>
    <link href="https://orzyt.cn/posts/leetcode563-binary-tree-tilt/"/>
    <id>https://orzyt.cn/posts/leetcode563-binary-tree-tilt/</id>
    <published>2018-01-14T04:51:07.000Z</published>
    <updated>2019-02-08T10:26:00.486Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given a binary tree, return the tilt of the <strong>whole tree</strong>.</p><p>The tilt of a <strong>tree node</strong> is defined as the <strong>absolute difference</strong> between the sum of all left subtree node values and the sum of all right subtree node values. Null node has tilt 0.</p><p>The tilt of the <strong>whole tree</strong> is defined as the sum of all nodes’ tilt.</p><a id="more"></a><p><strong>Note:</strong></p><ol><li>The sum of node values in any subtree won’t exceed the range of 32-bit integer.</li><li>All the tilt values won’t exceed the range of 32-bit integer.</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">         1</span><br><span class="line">       /   \</span><br><span class="line">      2     3</span><br><span class="line">Output: 1</span><br><span class="line">Explanation: </span><br><span class="line">Tilt of node 2 : 0</span><br><span class="line">Tilt of node 3 : 0</span><br><span class="line">Tilt of node 1 : |2-3| = 1</span><br><span class="line">Tilt of binary tree : 0 + 0 + 1 = 1</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>求二叉树的倾斜度。</p><p>一个节点的倾斜度是指：该节点 <strong>左子树所有节点值之和</strong> 与 <strong>右子树所有节点值之和</strong> 的 <code>绝对差值</code></p><p>一棵树的倾斜度是指：该棵树所有节点的倾斜度之和</p><p>对二叉树dfs一遍即可求出答案</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 以root为根的子树所有节点值之和</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> lsum = dfs(root-&gt;left), rsum = dfs(root-&gt;right);</span><br><span class="line">        <span class="comment">// 添加节点root的倾斜度</span></span><br><span class="line">        ans += <span class="built_in">abs</span>(lsum - rsum);</span><br><span class="line">        <span class="keyword">return</span> lsum + rsum + root-&gt;val;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findTilt</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        dfs(root);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given a binary tree, return the tilt of the &lt;strong&gt;whole tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The tilt of a &lt;strong&gt;tree node&lt;/strong&gt; is defined as the &lt;strong&gt;absolute difference&lt;/strong&gt; between the sum of all left subtree node values and the sum of all right subtree node values. Null node has tilt 0.&lt;/p&gt;
&lt;p&gt;The tilt of the &lt;strong&gt;whole tree&lt;/strong&gt; is defined as the sum of all nodes’ tilt.&lt;/p&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="DFS" scheme="https://orzyt.cn/tags/DFS/"/>
    
      <category term="二叉树" scheme="https://orzyt.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    
  </entry>
  
</feed>
