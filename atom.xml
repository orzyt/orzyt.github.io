<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>扬涛的博客</title>
  
  <subtitle>上善若水·大道至简</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://orzyt.cn/"/>
  <updated>2019-03-08T13:29:43.661Z</updated>
  <id>https://orzyt.cn/</id>
  
  <author>
    <name>orzyt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>强化学习实践（一）：Tic-Tac-Toe</title>
    <link href="https://orzyt.cn/posts/tic-tac-toe/"/>
    <id>https://orzyt.cn/posts/tic-tac-toe/</id>
    <published>2019-03-08T10:55:54.000Z</published>
    <updated>2019-03-08T13:29:43.661Z</updated>
    
    <content type="html"><![CDATA[<hr><p>为了对强化学习的基本概念有一个直观的认识,《Reinforcement Learning: An Introduction》第一章给出了一个简单的例子：<code>Tic-Tac-Toe</code>游戏.</p><h2 id="游戏规则"><a href="#游戏规则" class="headerlink" title="游戏规则"></a>游戏规则</h2><p>游戏的规则很简单, 两位玩家在 <code>3x3</code> 的棋盘上轮流下棋, 一位打 <code>X</code>, 另一位打 <code>O</code>, 若棋盘的任意一行、任意一列、正反对角线上有三个相同的棋, 则执该棋的玩家获胜. 若棋盘下满仍没有决出胜负, 则平局.</p><a id="more"></a><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0vltwtubvj20bb0b574c.jpg" alt="Tic-Tac-Toe示例" width="25%" height="25%"></p><hr><p>我们尝试使用强化学习的方法来训练一个Agent, 使其能够在该游戏上表现出色(即Agent在任何情况下都不会输, 最多平局).</p><p>由于没有外部经验, 因此我们需要同时训练两个Agent进行上万轮的对弈来寻找最优策略.</p><p><strong>注:下面的代码只给出部分关键实现过程, 完整代码见:<a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter01/tic_tac_toe.py" target="_blank" rel="noopener">tic_tac_toe.py</a>.</strong> </p><p><strong>版权归 <a href="https://github.com/ShangtongZhang" target="_blank" rel="noopener">@Shangtong Zhang</a> 等人所有, 仅添加中文注释便于理解.</strong></p><hr><h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>强化学习一个重要的概念就是——<strong>状态(State)</strong>. </p><p>状态是指Agent在一个特定时刻从环境中所感知的信号. </p><p>在<code>Tic-Tac-Toe</code>游戏中, 状态即为 <code>3*3</code> 棋盘的布局. </p><p>定义一个<code>State类</code>用来表示状态.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">State</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''状态初始化</span></span><br><span class="line"><span class="string">        棋盘使用 n * n 的数组进行表示</span></span><br><span class="line"><span class="string">        棋盘中的数字: 1代表先手, -1代表后手下, 0代表该位置无棋子</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 该状态的数组表示</span></span><br><span class="line">        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))</span><br><span class="line">        <span class="comment"># 该状态下的胜利者</span></span><br><span class="line">        self.winner = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># 该状态的哈希值表示</span></span><br><span class="line">        self.state_hash = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># 该状态是否为终结状态</span></span><br><span class="line">        self.end = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''计算状态的哈希值表示</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        int</span></span><br><span class="line"><span class="string">            状态的哈希值表示</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_end</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''判断当前状态是否为终结状态.</span></span><br><span class="line"><span class="string">        如果为终结状态, 同时判断胜利者是谁</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bool</span></span><br><span class="line"><span class="string">            当前状态是否为终结状态</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_state</span><span class="params">(self, i, j, symbol)</span>:</span></span><br><span class="line">        <span class="string">'''计算当前状态的后继状态</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        i : int</span></span><br><span class="line"><span class="string">            下一步动作的行坐标</span></span><br><span class="line"><span class="string">        j : int</span></span><br><span class="line"><span class="string">            下一步动作的列坐标</span></span><br><span class="line"><span class="string">        symbol : int</span></span><br><span class="line"><span class="string">            动作的执行者(1代表先手, -1代表后手)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        State</span></span><br><span class="line"><span class="string">            下一步棋盘的状态</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_state</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''打印状态信息</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><hr><p>根据游戏规则我们知道, 每个格子仅有三种状态, 即先手(1), 后手(-1), 空(0), 那么该游戏的状态数上限仅有 $3^9=19683$ 个.</p><p>因此, 我们可以预处理出所有合法的棋盘状态, 供后面强化学习算法使用.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_states = get_all_states()</span><br></pre></td></tr></table></figure><hr><h2 id="AgentPlayer相关"><a href="#AgentPlayer相关" class="headerlink" title="AgentPlayer相关"></a>AgentPlayer相关</h2><p>定义一个<code>AgentPlayer类</code>用来表示强化学习中和环境进行交互的智能体.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AgentPlayer</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, step_size=<span class="number">0.1</span>, epsilon=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">'''Agent初始化</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        step_size : float, optional</span></span><br><span class="line"><span class="string">            更新步长</span></span><br><span class="line"><span class="string">        epsilon : float, optional</span></span><br><span class="line"><span class="string">            探索概率</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 值函数</span></span><br><span class="line">        self.value = dict()</span><br><span class="line">        <span class="comment"># 值函数更新步长</span></span><br><span class="line">        self.step_size = step_size</span><br><span class="line">        <span class="comment"># Agent探索概率</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        <span class="comment"># Agent在一轮游戏中经历的所有状态</span></span><br><span class="line">        self.states = []</span><br><span class="line">        <span class="comment"># 记录每个状态是否采取贪心策略</span></span><br><span class="line">        self.greedy = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''重置Agent的状态, 开启新一轮游戏</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">'''将当前棋盘状态加到Agent的状态列表</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        state : State</span></span><br><span class="line"><span class="string">            当前棋盘的状态</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">        <span class="string">'''根据先后手, 初始化Agent的值函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        symbol : int</span></span><br><span class="line"><span class="string">            先手还是后手</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''值函数迭代</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''根据状态采取动作</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        list</span></span><br><span class="line"><span class="string">            采取的动作</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_policy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''保存策略</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_policy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''加载策略</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><hr><p>在AgentPlayer类中, 我们重点关注 <code>set_symbol</code>, <code>backup</code>, <code>act</code> 三个函数.</p><h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>Agent每次与环境进行交互的时候, 都会得到一个反馈信号称之为<strong>奖励(Reward)</strong>. </p><p>Agent的目标是<strong>最大化游戏过程中的奖励总和</strong>.</p><p>在 <code>Tic-Tac-Toe</code> 游戏中, 由于只有在游戏结束的时候才知道胜负, 故没有给出每一步显式的奖励, 而是直接评估状态的<strong>值函数(Value Function)</strong>.</p><p>根据我们的先验知识, 可以对不同的状态设置不同的初始值函数.</p><p>对于导致游戏结束的终结状态, 可分为胜利/平局/失败三种情况, 相应的值函数为1.0/0.5/0.0.</p><p>而对于非终结状态, 可以简单地将状态的值函数设为0.5, 代表无法判断胜负.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">    <span class="string">'''根据先后手, 初始化Agent的值函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    symbol : int</span></span><br><span class="line"><span class="string">        先手还是后手</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    self.symbol = symbol</span><br><span class="line">    <span class="keyword">for</span> state_hash <span class="keyword">in</span> all_states.keys():</span><br><span class="line">        (state, is_end) = all_states[state_hash]</span><br><span class="line">        <span class="keyword">if</span> is_end: <span class="comment"># 终结状态</span></span><br><span class="line">            <span class="keyword">if</span> state.winner == self.symbol: <span class="comment"># 获胜</span></span><br><span class="line">                self.value[state_hash] = <span class="number">1.0</span></span><br><span class="line">            <span class="keyword">elif</span> state.winner == <span class="number">0</span>: <span class="comment"># 平局</span></span><br><span class="line">                self.value[state_hash] = <span class="number">0.5</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 失败</span></span><br><span class="line">                self.value[state_hash] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 非终结状态</span></span><br><span class="line">            self.value[state_hash] = <span class="number">0.5</span></span><br></pre></td></tr></table></figure><hr><h3 id="值函数迭代"><a href="#值函数迭代" class="headerlink" title="值函数迭代"></a>值函数迭代</h3><p>使用<strong>时序差分学习(Temporal-Difference Learning)</strong>方法进行值函数的更新:</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left[ V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right) \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''值函数迭代</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#　获取状态的哈希值表示</span></span><br><span class="line">    self.states = [state.hash() <span class="keyword">for</span> state <span class="keyword">in</span> self.states]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 逆序遍历所有的状态, 并进行值函数的更新</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(len(self.states) - <span class="number">1</span>)):</span><br><span class="line">        state = self.states[i]  </span><br><span class="line">        <span class="comment"># TD误差 = V(s_&#123;t + 1&#125;) - V(s_&#123;t&#125;)</span></span><br><span class="line">        td_error = self.greedy[i] * (self.value[self.states[i + <span class="number">1</span>]] - self.value[state])</span><br><span class="line">        <span class="comment"># TD-Learning(时序差分学习)更新公式</span></span><br><span class="line">        self.value[state] += self.step_size * td_error</span><br></pre></td></tr></table></figure><hr><h3 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h3><p>采用 $\epsilon$-greedy 的贪心策略选择动作, 即有 $1 - \epsilon$ 的概率选择后继状态值函数最大的动作, 有 $\epsilon$ 概率进行随机选择动作.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''根据状态采取动作</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    list</span></span><br><span class="line"><span class="string">        采取的动作</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取当前状态</span></span><br><span class="line">    state = self.states[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># 下一步所有可能的状态</span></span><br><span class="line">    next_states = []</span><br><span class="line">    <span class="comment"># 下一步所有可能的位置</span></span><br><span class="line">    next_positions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(BOARD_ROWS):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(BOARD_COLS):</span><br><span class="line">            <span class="comment"># 当前棋盘位置上无棋子</span></span><br><span class="line">            <span class="keyword">if</span> state.data[i, j] == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 可行的位置</span></span><br><span class="line">                next_positions.append([i, j])</span><br><span class="line">                <span class="comment"># 可行的状态</span></span><br><span class="line">                next_states.append(state.next_state(i, j, self.symbol).hash())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 有epsilon概率采取随机动作</span></span><br><span class="line">    <span class="keyword">if</span> np.random.rand() &lt; self.epsilon:</span><br><span class="line">        action = next_positions[np.random.randint(len(next_positions))]</span><br><span class="line">        action.append(self.symbol)</span><br><span class="line">        self.greedy[<span class="number">-1</span>] = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    values = []</span><br><span class="line">    <span class="comment"># 遍历下一步所有可能的状态和位置</span></span><br><span class="line">    <span class="keyword">for</span> state_hash, pos <span class="keyword">in</span> zip(next_states, next_positions):</span><br><span class="line">        <span class="comment"># 获取对应状态的值函数</span></span><br><span class="line">        values.append((self.value[state_hash], pos))</span><br><span class="line">    <span class="comment"># 如果有多个状态的值函数相同,且都是最高的,shuffle则起到在这些状态中随机选择的作用</span></span><br><span class="line">    np.random.shuffle(values)</span><br><span class="line">    <span class="comment"># 按值函数从大到小排序</span></span><br><span class="line">    values.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># 选取最优动作</span></span><br><span class="line">    action = values[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    action.append(self.symbol)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><hr><h2 id="HumanPlayer相关"><a href="#HumanPlayer相关" class="headerlink" title="HumanPlayer相关"></a>HumanPlayer相关</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HumanPlayer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        self.symbol = <span class="keyword">None</span></span><br><span class="line">        self.keys = [<span class="string">'q'</span>, <span class="string">'w'</span>, <span class="string">'e'</span>, <span class="string">'a'</span>, <span class="string">'s'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>, <span class="string">'x'</span>, <span class="string">'c'</span>]</span><br><span class="line">        self.state = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        self.state = state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self, _)</span>:</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.state.print_state()</span><br><span class="line">        key = input(<span class="string">"Input your position:"</span>)</span><br><span class="line">        data = self.keys.index(key)</span><br><span class="line">        i = data // int(BOARD_COLS)</span><br><span class="line">        j = data % BOARD_COLS</span><br><span class="line">        <span class="keyword">return</span> [i, j, self.symbol]</span><br></pre></td></tr></table></figure><hr><h2 id="Agent训练"><a href="#Agent训练" class="headerlink" title="Agent训练"></a>Agent训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epochs, print_every_n=<span class="number">500</span>)</span>:</span></span><br><span class="line">    <span class="string">'''对Agent进行训练</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    epochs : int</span></span><br><span class="line"><span class="string">        训练轮数</span></span><br><span class="line"><span class="string">    print_every_n : int, optional</span></span><br><span class="line"><span class="string">        每多少轮输出训练信息</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义两个Agent</span></span><br><span class="line">    player1 = AgentPlayer(epsilon=<span class="number">0.01</span>)</span><br><span class="line">    player2 = AgentPlayer(epsilon=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 定义判决器</span></span><br><span class="line">    game = Game(player1, player2)</span><br><span class="line">    <span class="comment"># 先手赢的次数</span></span><br><span class="line">    player1_win = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 后手赢的次数</span></span><br><span class="line">    player2_win = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 新的一轮游戏</span></span><br><span class="line">        game.reset()</span><br><span class="line">        winner = game.play(print_state=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">1</span>:</span><br><span class="line">            player1_win += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">-1</span>:</span><br><span class="line">            player2_win += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 打印各自的胜率</span></span><br><span class="line">        <span class="keyword">if</span> i % print_every_n == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f'</span> % (i, player1_win / i, player2_win / i))</span><br><span class="line">        <span class="comment"># 在每轮游戏结束后,对Agent进行学习</span></span><br><span class="line">        player1.backup()</span><br><span class="line">        player2.backup()</span><br><span class="line">    <span class="comment"># 保存训练好的策略</span></span><br><span class="line">    player1.save_policy()</span><br><span class="line">    player2.save_policy()</span><br></pre></td></tr></table></figure><hr><h2 id="Agent对弈"><a href="#Agent对弈" class="headerlink" title="Agent对弈"></a>Agent对弈</h2><p>经过充分的训练后, 两个Agent对弈的胜率应该都为0%. 即任何局面都只能打成平手, 没有一方可以胜过另一方.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compete</span><span class="params">(turns)</span>:</span></span><br><span class="line">    <span class="string">'''将训练好的两个Agent进行对弈</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    turns : int</span></span><br><span class="line"><span class="string">        对弈轮数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对弈的时候不进行动作的探索, 故epsilon设为0.</span></span><br><span class="line">    player1 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">    player2 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">    game = Game(player1, player2)</span><br><span class="line">    player1.load_policy()</span><br><span class="line">    player2.load_policy()</span><br><span class="line">    player1_win = <span class="number">0.0</span></span><br><span class="line">    player2_win = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, turns):</span><br><span class="line">        game.reset()</span><br><span class="line">        winner = game.play()</span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">1</span>:</span><br><span class="line">            player1_win += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">-1</span>:</span><br><span class="line">            player2_win += <span class="number">1</span></span><br><span class="line">    print(<span class="string">'%d turns, player 1 win %.02f, player 2 win %.02f'</span> % (turns, player1_win / turns, player2_win / turns))</span><br></pre></td></tr></table></figure><hr><h2 id="Human-v-s-Agent"><a href="#Human-v-s-Agent" class="headerlink" title="Human v.s. Agent"></a>Human v.s. Agent</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''人类玩家和Agent进行对弈</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        player1 = HumanPlayer()</span><br><span class="line">        player2 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">        game = Game(player1, player2)</span><br><span class="line">        player2.load_policy()</span><br><span class="line">        winner = game.play()</span><br><span class="line">        <span class="keyword">if</span> winner == player2.symbol:</span><br><span class="line">            print(<span class="string">"失败!"</span>)</span><br><span class="line">        <span class="keyword">elif</span> winner == player1.symbol:</span><br><span class="line">            print(<span class="string">"胜利!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"平局!"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;为了对强化学习的基本概念有一个直观的认识,《Reinforcement Learning: An Introduction》第一章给出了一个简单的例子：&lt;code&gt;Tic-Tac-Toe&lt;/code&gt;游戏.&lt;/p&gt;
&lt;h2 id=&quot;游戏规则&quot;&gt;&lt;a href=&quot;#游戏规则&quot; class=&quot;headerlink&quot; title=&quot;游戏规则&quot;&gt;&lt;/a&gt;游戏规则&lt;/h2&gt;&lt;p&gt;游戏的规则很简单, 两位玩家在 &lt;code&gt;3x3&lt;/code&gt; 的棋盘上轮流下棋, 一位打 &lt;code&gt;X&lt;/code&gt;, 另一位打 &lt;code&gt;O&lt;/code&gt;, 若棋盘的任意一行、任意一列、正反对角线上有三个相同的棋, 则执该棋的玩家获胜. 若棋盘下满仍没有决出胜负, 则平局.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（三）：动态规划</title>
    <link href="https://orzyt.cn/posts/planning-by-dp/"/>
    <id>https://orzyt.cn/posts/planning-by-dp/</id>
    <published>2019-03-01T06:10:25.000Z</published>
    <updated>2019-03-08T13:34:20.557Z</updated>
    
    <content type="html"><![CDATA[<hr><p>在上一篇文章 <a href="https://orzyt.cn/posts/markov-decision-processes/">强化学习（二）：马尔可夫决策过程</a> 中, 我们介绍用来对强化学习问题进行建模的马尔可夫决策过程(Markov Decision Processes, MDPs). </p><p>由于MDPs的贝尔曼最优方程没有封闭解, 因此一般采用迭代的方法对其进行求解. </p><p>本文将介绍使用<strong>动态规划(Dynamic Programming)</strong>算法来求解MDPs.</p><hr><a id="more"></a><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><ul><li><p><strong>动态(Dynamic)</strong>: 问题中的时序部分</p></li><li><p><strong>规划(Planning)</strong>: 对问题进行优化</p></li></ul><p>动态规划将问题分解为子问题, 从子问题的解中得到原始问题的解.</p><hr><h3 id="动态规划的性质"><a href="#动态规划的性质" class="headerlink" title="动态规划的性质"></a>动态规划的性质</h3><ul><li><p><strong>最优子结构(Optimal substructure)</strong></p><ul><li>应用最优性原则(Principle of optimality)</li><li>最优解可以从子问题的最优解中得到</li></ul></li><li><p><strong>重叠子问题(Overlapping subproblems)</strong></p><ul><li>相同的子问题出现多次</li><li>问题的解可以被缓存和复用</li></ul></li></ul><p>马尔可夫决策过程满足上面两种性质:</p><blockquote><p><em>贝尔曼方程</em> 给出了问题的递归分解表示, <em>值函数</em> 存储和复用了问题的解.</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi(a|s) (\mathcal{R}_s^a + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}v_{\pi}(s'))</script></blockquote><hr><h3 id="用动态规划进行Planning"><a href="#用动态规划进行Planning" class="headerlink" title="用动态规划进行Planning"></a>用动态规划进行Planning</h3><p>动态规划假设我们知道MDP的所有知识, 包括状态、行为、转移矩阵、奖励甚至策略等.</p><p>对于<strong>预测(Prediction)</strong>问题: </p><ul><li><p>输入: </p><ul><li>MDP $&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;$ 和 策略 $\pi$</li><li>MRP $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$</li></ul></li><li><p>输出: 值函数 $v_{\pi}$</p></li></ul><p>对于<strong>控制(Control)</strong>问题:</p><ul><li><p>输入:</p><ul><li>MDP $&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;$</li></ul></li><li><p>输出:</p><ul><li>最优值函数 $v_{*}$</li><li>最优策略 $\pi_{*}$</li></ul></li></ul><hr><h2 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h2><blockquote><p>问题: 评估一个给定的策略 $\pi$<br>求解: 对贝尔曼期望方程进行迭代, $v_1 \to v_2 \to \dots \to v_{\pi}$</p></blockquote><p>通常使用<strong>同步备份(synchronous backups)</strong>方法:</p><p>对于第 $k+1$ 次迭代, 所有状态 $s$ 在第 $k+1$ 时刻的价值 $v_{k+1}(s)$ 用 $v_k(s’)$ 进行更新, 其中 $s’$ 是 $s$ 的后继状态.</p><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0nb9rn3v4j20ar06eq31.jpg" alt="迭代策略评估" width="30%" height="30%"></p><script type="math/tex; mode=display">\begin{aligned} v _ { k + 1 } ( s ) & = \sum _ { a \in \mathcal { A } } \pi ( a | s ) \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { k } \left( s ^ { \prime } \right) \right) \\\mathbf { v } ^ { k + 1 } & = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } \mathbf { v } ^ { k } \end{aligned}</script><hr><p><strong>迭代策略评估算法</strong>:</p><p>迭代策略评估算法用来估计 $V \approx v_{\pi}$.</p><p>这里使用<code>in-place</code>版本, 即只保留一份 $v$ 数组, 没有新旧之分. </p><p>通常来说, 该方法也能收敛到 $v_{\pi}$, 而且收敛速度可能更快.</p><p>终止条件: $\max \limits_ { s \in \mathcal{S} } \left| v _ { k + 1 } ( s ) - v _ { k } ( s ) \right|$ 小于给定的误差 $\Delta$</p><p><img src="https://ws1.sinaimg.cn/large/8662e3cegy1g0nj071hc6j20km08hq3t.jpg" alt="迭代策略评估伪代码" width="60%" height="60%"></p><hr><p>例子: <strong>Small Gridworld</strong> <a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter04/grid_world.py" target="_blank" rel="noopener">[代码]</a></p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0nbvkvvotj20k00dkdgx.jpg" alt="Small Gridworld" width="50%" height="50%"></p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0nc5gkmd8j20e30lkwgd.jpg" alt="Small Gridworld Solution" width="50%" height="50%"></p><hr><h2 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h2><p>让我们考虑一个<strong>确定性策略</strong>(即对于一个状态来说, 其采取的动作是确定的, 而不是考虑每个动作的概率) $a = \pi(s)$.</p><blockquote><p>我们可以通过贪心选择来改进策略 $\pi$:</p><script type="math/tex; mode=display">\pi ^ { \prime } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } q _ { \pi } ( s , a )</script></blockquote><p>即状态 $s$ 的新策略为令动作值函数 $q_{\pi}(s, a)$ 取得最大值的动作.</p><p>相应地, 动作值函数 $q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right)$ 得到了改进:</p><script type="math/tex; mode=display">q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a ) \geq q _ { \pi } ( s , \pi ( s ) ) = v _ { \pi } ( s ) \\{\scriptsize 由于是确定性策略, 才会有 v_{\pi}(s) = q_{\pi}(s, \pi(s))}\tag{1}</script><p>注: 确定性策略下的动作值函数 $q_{\pi}(s, a)$ 为:</p><script type="math/tex; mode=display">\begin{aligned} q _ { \pi } ( s , a ) & = \mathbb { E } \left[ R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right) | S _ { t } = s , A _ { t } = a \right] \\ & = \sum _ { s ^ { \prime } , r } p \left( s ^ { \prime } , r | s , a \right) \left[ r + \gamma v _ { \pi } \left( s ^ { \prime } \right) \right] \end{aligned}\tag{2}</script><p>从而, 值函数 $v _ { \pi ^ { \prime } } ( s )$ 也得到了改进:</p><script type="math/tex; mode=display">\begin{aligned} v_\pi(s) & \le q_\pi(s,\pi^{'}(s)) {\scriptsize //公式(1)} \\ &={\Bbb E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t=s, A_t=\pi^{'}(s)] {\scriptsize //公式(2)} \\&={\Bbb E}_{\pi'}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\  {\scriptsize //注意外层是在新策略 \pi^{'} 下求期望} \\   & \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma q_\pi(S_{t+1},\pi'(S_{t+1}))|S_t=s] {\scriptsize //对状态S_{t+1}使用公式(1)} \\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma {\Bbb E}_{\pi'}\left[ R_{t+2}+\gamma v_{\pi}\left( S_{t+2}\right) | S_{t+1}, A_{t+1}=\pi^{'}(S_{t+1}) \right] | S_t=s]\\ &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_{\pi}\left( S_{t+2} \right)|S_t=s] {\scriptsize //去掉括号内的期望} \\ & \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma ^2 q_\pi(S_{t+2},\pi'(S_{t+2}))|S_t=s] {\scriptsize //对状态S_{t+2}使用公式(1)} \\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 {\Bbb E}_{\pi'}\left( R_{t+3}+\gamma v_{\pi}\left( S_{t+3} \right) \right)|S_t=s]\\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_{\pi}\left( S_{t+3} \right)|S_t=s]\\  & \vdots \\& \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4} + \dots |S_t=s]\\ &=v_{\pi^{'}}(s) \\ \end{aligned}</script><p>当改进停止时, 有如下等式:</p><script type="math/tex; mode=display">q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a ) = q _ { \pi } ( s , \pi ( s ) ) = v _ { \pi } ( s )\tag{3}</script><p>可以说, 此时公式(3)满足了贝尔曼最优方程:</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a )</script><p>从而, 对所有状态 $s$ 来说, 有$v_{\pi}(s) = v_{*}(s)$, 即策略 $\pi$ 改进到了最优策略.</p><hr><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><h3 id="策略迭代-1"><a href="#策略迭代-1" class="headerlink" title="策略迭代"></a>策略迭代</h3><p>给定一个策略 $\pi$, 我们可以首先对策略进行评估, 然后根据值函数 $v_{\pi}$ 进行贪心地改进策略.</p><script type="math/tex; mode=display">\pi _ { 0 } \stackrel { \mathrm { E } } { \longrightarrow } v _ { \pi _ { 0 } } \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { 1 } \stackrel { \mathrm { E } } { \longrightarrow } v _ { \pi _ { 1 } } \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { 2 } \stackrel { \mathrm { E } } { \longrightarrow } \cdots \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { * } \stackrel { \mathrm { E } } { \longrightarrow } v _ { * }</script><p>其中, $\stackrel { \mathrm { E } } { \longrightarrow }$ 表示策略评估, $\stackrel { \mathrm { I } } { \longrightarrow }$ 表示策略改进. </p><ul><li><p><strong>评估(Evaluate):</strong></p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots | S _ { t } = s \right]</script></li><li><p><strong>改进(Improve):</strong></p><script type="math/tex; mode=display">\pi^{'} = \text{greedy}(v_{\pi})</script></li></ul><p>由于每个策略都比前一个策略更优, 同时一个有限状态的马尔可夫决策过程(finite MDP)仅有有限个策略, 因此该过程一定能够在有限次的迭代中收敛到最优策略 $\pi_{*}$ 和最优值函数 $v_{*}$.</p><hr><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0ncddk93hj20ni0cc0uq.jpg" alt="策略迭代" width="50%" height="50%"></p><hr><p><strong>策略迭代算法:</strong></p><p>策略迭代算法分为: <strong>初始化</strong>, <strong>策略评估</strong> 以及 <strong>策略改进</strong> 三部分.</p><p>其中, 策略改进部分的终止条件为: <strong>是否所有状态的策略不再发生变化</strong>.</p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0njmn8jy5j20kq0e3760.jpg" alt="策略迭代算法" width="60%" height="60%"></p><hr><p>例子: <strong>Jack’s Car Rental</strong> <a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter04/car_rental.py" target="_blank" rel="noopener">[代码]</a>  (<em>先占个坑 , 等有时间把这个例子详细写下</em>)</p><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0ncjvapk8j20ke0ebn32.jpg" alt="Jack’s Car Rental" width="50%" height="50%"></p><p>策略迭代求解结果:</p><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0nclbnxl1j20jw0dgtam.jpg" alt="Jack’s Car Rental Solution" width="50%" height="50%"></p><p>图中纵坐标是位置 $1$ 的汽车数量, 横坐标是位置 $2$ 的汽车数量, 该问题共有 $21 \times 21$ 个状态. </p><p>图中的等高线将状态划分为不同的区域, 区域内的数值代表相应的策略(正数代表从位置 $1$ 移往位置 $2$ 的汽车数量, 负数则往反方向移动).</p><hr><h3 id="策略迭代的扩展"><a href="#策略迭代的扩展" class="headerlink" title="策略迭代的扩展"></a>策略迭代的扩展</h3><h4 id="改良策略迭代"><a href="#改良策略迭代" class="headerlink" title="改良策略迭代"></a>改良策略迭代</h4><p>策略评估并不需要真正的收敛到 $v_{\pi}$. (比如在 <code>Small Gridworld</code>例子中, 迭代 $k=3$次 即可以得到最优策略.)</p><p>为此我们可以引进终止条件, 如:</p><ul><li>值函数的 $\epsilon$ -收敛</li><li>简单地迭代 $k$ 次便停止策略评估</li></ul><p>或者每次迭代(即 $k=1$ )都对策略进行更新改进, 这种情况等价于<strong>值迭代(value iteration)</strong>.</p><hr><h4 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h4><p><strong>广义策略迭代</strong>(Generalized Policy iteration，GPI)指代让策略评估(policy-evaluation)和策略改进(policyimprovement)过程进行交互的一般概念, 其不依赖于两个过程的粒度(granularity)和其他细节.</p><p>几乎所有强化学习方法都可以很好地被描述为GPI. 也就是说, 它们都具有可辨识的策略与值函数. 其中, 策略 $\pi$ 通过相应的值函数 $v$ 进行改进, 而值函数 $V$ 总是趋向策略 $\pi$ 的值函数 $v^{\pi}$. 如下图所示,</p><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0nik26512j206y0a5dfz.jpg" alt="广义策略迭代" width="20%" height="20%"></p><hr><h2 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h2><p>策略迭代的一个缺点是它的每次迭代都涉及策略评估, 这本身就是一个需要对状态集进行多次扫描的耗时迭代计算. </p><p>而在值迭代的过程中, 并没有出现显式的策略, 并且中间过程的值函数可能也不和任何策略对应.</p><hr><h3 id="最优性原则"><a href="#最优性原则" class="headerlink" title="最优性原则"></a>最优性原则</h3><p>一个最优策略可以被分解为两部分:</p><ul><li>当前状态的最优动作 $A_{*}$</li><li>后继状态 $S^{\prime}$ 的最优策略</li></ul><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0nk02i8apj20mi06v75b.jpg" alt="最优性原则" width="60%" height="60%"></p><p>该原则的意思是说, 一个策略 $\pi(a|s)$ 在状态 $s$ 取到最优值函数 $v_{\pi}(s) = v_{*}(s)$ <strong>当且仅当</strong> 对于所有从状态 $s$ 出发可到达的状态 $s^{\prime}$, 策略 $\pi$ 也能够在状态 $s^{\prime}$ 取到最优值函数.</p><hr><h3 id="确定性值迭代"><a href="#确定性值迭代" class="headerlink" title="确定性值迭代"></a>确定性值迭代</h3><p>如果我们已经知道子问题的最优解 $v_{*}(s^{\prime})$, 那么状态 $s$ 的最优解可以通过向前看(lookahead)一步得到, 这称为<strong>值迭代(Value Iteration)</strong>:</p><script type="math/tex; mode=display">v_{*}(s) \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{*}(s') \right)</script><hr><p><strong>值迭代算法:</strong></p><p>值迭代算法和策略迭代算法一样, 是用来估计最优策略 $\pi_{*}$ 的, 它将策略评估和策略改进有效地结合在了一起.</p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0nkt7xfz0j20kn09k0ts.jpg" alt="值迭代算法" width="60%" height="60%"></p><hr><h2 id="同步动态规划算法总结"><a href="#同步动态规划算法总结" class="headerlink" title="同步动态规划算法总结"></a>同步动态规划算法总结</h2><div class="table-container"><table><thead><tr><th style="text-align:center">问题</th><th style="text-align:center">贝尔曼方程</th><th style="text-align:center">算法</th></tr></thead><tbody><tr><td style="text-align:center">预测(Prediction)</td><td style="text-align:center">贝尔曼期望方程</td><td style="text-align:center">迭代策略评估</td></tr><tr><td style="text-align:center">控制(Control)</td><td style="text-align:center">贝尔曼期望方程 + 贪心策略改进</td><td style="text-align:center">策略迭代</td></tr><tr><td style="text-align:center">控制(Control)</td><td style="text-align:center">贝尔曼最优方程</td><td style="text-align:center">值迭代</td></tr></tbody></table></div><p>对于有 $m$ 个动作和 $n$ 个状态 的MDP来说, 每次迭代的时间复杂度如下:</p><div class="table-container"><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">复杂度</th></tr></thead><tbody><tr><td style="text-align:center">$v_{\pi}(s)$ or $v_{*}(s)$</td><td style="text-align:center">$\mathcal{O}(mn^2)$</td></tr><tr><td style="text-align:center">$q_{\pi}(s, a)$ or $q_{*}(s, a)$</td><td style="text-align:center">$\mathcal{O}(m^2n^2)$</td></tr></tbody></table></div><hr><h2 id="动态规划的扩展"><a href="#动态规划的扩展" class="headerlink" title="动态规划的扩展"></a>动态规划的扩展</h2><h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>同步DP算法的主要缺点是每次迭代都需要对整个状态集进行扫描, 这对于状态数非常多的MDP来说耗费巨大. 而异步DP算法则将所有的状态独立地,以任意顺序进行备份, 并且每个状态的更新次数不一, 这可以显著地减少计算量.</p><p>为了保证算法的正确收敛, 异步动态规划算法必须保证<strong>所有状态都能够持续地被更新</strong>(continue to update the values of all the states), 也就是说在任何时刻任何状态都有可能被更新, 而不能忽略某个状态.</p><p>异步DP算法主要有三种简单的思想:</p><ul><li>就地动态规划(<em>In-place</em> dynamic programming)</li><li>优先扫描(<em>Prioritised sweeping</em>)</li><li>实时动态规划(<em>Real-time</em> dynamic programming)</li></ul><hr><h4 id="就地动态规划"><a href="#就地动态规划" class="headerlink" title="就地动态规划"></a>就地动态规划</h4><p>同步DP保留值函数的两个备份, $v_{new}$ 和 $v_{old}$</p><script type="math/tex; mode=display">{\color{red} {v_{new}(s)}} \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} {\color{red} {v_{old}(s')}} \right)</script><p>就地值迭代只保留值函数的一个备份.</p><script type="math/tex; mode=display">{\color{red} {v(s)}} \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} {\color{red} {v(s')}} \right)</script><hr><h4 id="优先扫描"><a href="#优先扫描" class="headerlink" title="优先扫描"></a>优先扫描</h4><p>使用贝尔曼误差的大小来进行状态的选择:</p><script type="math/tex; mode=display">\left| \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } \right) \right) - v ( s ) \right|</script><ul><li><p>仅备份有最大贝尔曼误差的状态</p></li><li><p>在每次备份后, 需要更新受到影响的状态(即备份状态的前驱状态)的贝尔曼误差</p></li><li><p>可以使用优先队列进行实现</p></li></ul><hr><h4 id="实时动态规划"><a href="#实时动态规划" class="headerlink" title="实时动态规划"></a>实时动态规划</h4><ul><li>思想: <strong>只使用和Agent相关的状态</strong></li><li>使用Agent的经验来进行状态的选择</li><li>在每个时间步 $S_t, A_t, R_{t+1}$ 对状态 $S_t$ 进行备份</li></ul><script type="math/tex; mode=display">{\color{red} {v \left( S _ { t } \right)}} \gets \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { {\color{red}{S _ { t }}} } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { {\color{red} {S _ { t }}} s ^ { \prime }}  ^ { a } {\color{red} {v \left( s ^ { \prime } \right)}} \right)</script><hr><h3 id="全宽和采样备份"><a href="#全宽和采样备份" class="headerlink" title="全宽和采样备份"></a>全宽和采样备份</h3><h4 id="全宽备份"><a href="#全宽备份" class="headerlink" title="全宽备份"></a>全宽备份</h4><ul><li><p>DP使用<strong>全宽备份</strong>(<em>full-width</em> backups)</p></li><li><p>对于每次备份(不管同步还是异步)</p><ul><li>每个后继状态和动作都会被考虑进去</li><li>需要知道MDP转移矩阵和奖励函数</li></ul></li><li><p>对于大规模DP问题会遇到维数灾难</p></li><li><p>进行一次备份都太奢侈了</p></li></ul><hr><h4 id="采样备份"><a href="#采样备份" class="headerlink" title="采样备份"></a>采样备份</h4><p><strong>采样备份(Sample Backups)</strong>使用采样的奖励和采样的转移 $&lt; S , A , R , S ^ { \prime } &gt;$ 来替代奖励函数 $\mathcal{R}$ 和 转移矩阵 $\mathcal{P}$. </p><p>采样备份的优点:</p><ul><li><strong>Model-free</strong>: 不需要知道MDP的先验知识</li><li>通过采样<strong>缓解维数灾难</strong></li><li><strong>备份代价成为常量</strong>, 独立于状态数 $n = |\mathcal{S}|$</li></ul><hr><h2 id="压缩映射"><a href="#压缩映射" class="headerlink" title="压缩映射"></a>压缩映射</h2><p>关于上面的种种算法, 我们可能会有如下疑问:</p><ul><li>值迭代是否会收敛到 $v_{*}$ ?</li><li>迭代策略评估是否会收敛到 $v_{\pi}$ ?</li><li>策略迭代是否会收敛到 $v_{*}$ ?</li><li>解唯一吗 ?</li><li>算法收敛速度有多快 ?</li></ul><p>为了解决这些问题, 需要引入压缩映射(contraction mapping)理论.<br>可以参考: <a href="https://zhuanlan.zhihu.com/p/39279611" target="_blank" rel="noopener">如何证明迭代式策略评价、值迭代和策略迭代的收敛性？</a></p><hr><p>(关于压缩映射理论有时间再补充, 先到这里吧…)</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第四章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf" target="_blank" rel="noopener">Lecture3: Planning by Dynamic Programming</a></li><li><a href="https://zhuanlan.zhihu.com/p/51393982" target="_blank" rel="noopener">David Silver 增强学习——Lecture 3 动态规划</a></li><li><a href="https://www.cnblogs.com/pinard/p/9463815.html" target="_blank" rel="noopener">强化学习（三）用动态规划（DP）求解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;在上一篇文章 &lt;a href=&quot;https://orzyt.cn/posts/markov-decision-processes/&quot;&gt;强化学习（二）：马尔可夫决策过程&lt;/a&gt; 中, 我们介绍用来对强化学习问题进行建模的马尔可夫决策过程(Markov Decision Processes, MDPs). &lt;/p&gt;
&lt;p&gt;由于MDPs的贝尔曼最优方程没有封闭解, 因此一般采用迭代的方法对其进行求解. &lt;/p&gt;
&lt;p&gt;本文将介绍使用&lt;strong&gt;动态规划(Dynamic Programming)&lt;/strong&gt;算法来求解MDPs.&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="动态规划" scheme="https://orzyt.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（二）：马尔可夫决策过程</title>
    <link href="https://orzyt.cn/posts/markov-decision-processes/"/>
    <id>https://orzyt.cn/posts/markov-decision-processes/</id>
    <published>2019-02-27T07:38:18.000Z</published>
    <updated>2019-03-08T13:36:24.253Z</updated>
    
    <content type="html"><![CDATA[<hr><p>在上一篇文章 <a href="https://orzyt.cn/posts/introduction-to-rl">强化学习（一）：强化学习简介</a> 中, 我们介绍了强化学习的一些基本概念.</p><p>本文将介绍用来对强化学习问题进行建模的<strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>.</p><hr><a id="more"></a><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><h3 id="马尔可夫决策过程简介"><a href="#马尔可夫决策过程简介" class="headerlink" title="马尔可夫决策过程简介"></a>马尔可夫决策过程简介</h3><p><strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>形式上用来描述强化学习中的环境.</p><p>其中,环境是<strong>完全可观测的(fully observable)</strong>,即当前状态可以完全表征过程.</p><p>几乎所有的强化学习问题都能用MDPs来描述：</p><ul><li>最优控制问题可以描述成连续MDPs;</li><li>部分观测环境可以转化成MDPs;</li><li>赌博机问题是只有一个状态的MDPs.</li></ul><hr><h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3nzaa8yj20mn0593ym.jpg" alt="马尔科夫性质" width="60%" height="60%"></p><p>马尔科夫性质(Markov Property)表明: <strong>未来只与现在有关,而与过去无关.</strong></p><hr><h3 id="状态转移矩阵"><a href="#状态转移矩阵" class="headerlink" title="状态转移矩阵"></a>状态转移矩阵</h3><p>对于一个马尔可夫状态$S$及其后继状态$S’$,其状态转移概率由下式定义:</p><script type="math/tex; mode=display">\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]</script><p><strong>状态转移矩阵(State Transition Matrix)$\mathcal{P}$</strong>定义了从所有状态$S$转移到所有后继状态$S’$的概率.</p><script type="math/tex; mode=display">\mathcal { P } = \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \dots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n 1 } } & { \cdots } & { \mathcal { P } _ { n n } } \end{array} \right]</script><p>其中,$n$为状态个数,且矩阵的每行和为1.</p><hr><h3 id="马尔可夫过程-1"><a href="#马尔可夫过程-1" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h3><p><strong>马尔可夫过程(Markov Process)</strong>是一个无记忆的随机过程(memoryless random process).</p><p>即,随机状态$S_1, S_2, \dots$序列具有马尔可夫性质.</p><blockquote><p>马尔可夫过程(或马尔可夫链)是一个二元组$&lt;\mathcal{S}, \mathcal{P}&gt;$</p><ul><li>$\mathcal{S}$: (有限)状态集</li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li></ul></blockquote><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l1vm9xkzj20c80act96.jpg" alt="Example: Student Markov Chain" width="50%" height="50%"></p><p>圆圈代表状态, 箭头代表状态之间的转移, 数值代表转移概率.</p><p>状态转移矩阵$\mathcal{P}$如下:</p><script type="math/tex; mode=display">{\mathcal P} =\begin{bmatrix}  & C1 & C2 & C3 &  Pass & Pub & FB & Sleep\\  C1 & &0.5 &  &   & & 0.5 & \\ C2  & & &  0.8 & & & &0.2\\ C3  & & &  & 0.6& 0.4& &\\ Pass  & & &  & & & &1.0\\ Pub  &0.2 & 0.4& 0.4 & & & &\\ FB  &0.1 & &  & & & 0.9 &\\ Sleep  & & &  & & & &1.0 \end{bmatrix}</script><hr><h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><p><strong>马尔可夫奖励过程(Markov Reward Process, MRP)</strong>是<em>带有奖励的马尔可夫链</em>.</p><blockquote><p>马尔可夫奖励过程是一个四元组&lt;$\mathcal{S}$, $\mathcal{P}$, <font color="red">$\mathcal{R}$</font>, <font color="red">$\mathcal{\gamma}$</font>&gt;</p><ul><li>$\mathcal{S}$: (有限)状态集</li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li><li><font color="red"> $\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right]$ </font></li><li><font color="red"> $\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </font></li></ul></blockquote><p><img src="http://wx1.sinaimg.cn/large/8662e3cegy1g0l2klvnixj20cf0aowf0.jpg" alt="Example: Student MRP" width="50%" height="50%"></p><h3 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h3><blockquote><p><strong>回报(Return)</strong> $G_t$ 是从时间 $t$ 开始的总折扣奖励.</p><script type="math/tex; mode=display">G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 }</script></blockquote><ul><li>折扣因子 $\gamma \in [ 0,1 ]$ 表示未来的奖励在当前的价值. 由于未来的奖励充满不确定性, 因此需要乘上折扣因子;</li><li>$\gamma$ 接近 $0$ 表明更注重当前的奖励(myopic);</li><li>$\gamma$ 接近 $1$ 表明更具有远见(far-sighted).</li></ul><hr><h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数(Value Function) $v(s)$ 表示一个状态 $s$ 的长期价值(long-term value).</p><blockquote><p>一个马尔可夫奖励过程(MRP)的<strong>状态值函数 $v(s)$</strong>是从状态 $s$ 开始的期望回报.</p><script type="math/tex; mode=display">v ( s ) = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right]</script></blockquote><hr><h3 id="MRPs的贝尔曼方程"><a href="#MRPs的贝尔曼方程" class="headerlink" title="MRPs的贝尔曼方程"></a>MRPs的贝尔曼方程</h3><p>值函数可以被分解为两部分:</p><ul><li>立即奖励 $R_{t+1}$</li><li>后继状态的折扣价值 $\gamma v(S_{t+1})$</li></ul><script type="math/tex; mode=display">\begin{aligned} v ( s ) & = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \gamma ^ { 2 } R _ { t + 3 } + \ldots | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma \left( R _ { t + 2 } + \gamma R _ { t + 3 } + \ldots \right) | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma G _ { t + 1 } | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \mathbb { E } \left[ \gamma G _ { t + 1 } | S _ { t } = s \right]\\ & = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \gamma v \left( S _ { t + 1 } \right)\\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma v \left( S _ { t + 1 } \right) | S _ { t } = s \right] \end{aligned}\tag{1}\label{eq:mrp-bellman-equation}</script><p>上式表明, $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 的值函数之间满足递推关系. </p><p>该递推式也称为<strong>贝尔曼方程(Bellman Equation)</strong>.</p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0l3fh3jb3j207802zglh.jpg" alt="Bellman Equation for MRPs" width="30%" height="30%"></p><p>如果已知概率转移矩阵 $\mathcal{P}$, 则可将公式\eqref{eq:mrp-bellman-equation}变形为:</p><script type="math/tex; mode=display">v ( s ) = \mathcal { R } _ { s } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } v \left( s ^ { \prime } \right)\tag{2}\label{eq:mrp-bellman-equation-2}</script><p>例子:</p><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l3pbm9ixj20c30b5mxp.jpg" alt="Example: Bellman Equation for Student MRP" width="40%" height="40%"></p><p><strong>贝尔曼方程的矩阵形式:</strong></p><p>可将公式\eqref{eq:mrp-bellman-equation-2}改写为矩阵形式:</p><script type="math/tex; mode=display">v = \mathcal { R } + \gamma \mathcal { P } v</script><p>其中, $v$ 为一个列向量, 向量的元素为每个状态的值函数.</p><script type="math/tex; mode=display">\left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right] = \left[ \begin{array} { c } { \mathcal { R } _ { 1 } } \\ { \vdots } \\ { \mathcal { R } _ { n } } \end{array} \right] + \gamma \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \ldots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n1 } } & { \ldots } & { \mathcal { P } _ { n n } } \end{array} \right] \left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right]</script><p>观测贝尔曼方程的矩阵形式, 可知其为线性方程, 可直接求解如下.</p><script type="math/tex; mode=display">\begin{aligned} v & = \mathcal { R } + \gamma \mathcal { P } v \\( I - \gamma \mathcal { P } ) v & = \mathcal { R } \\v & = ( I - \gamma \mathcal { P } ) ^ { - 1 } \mathcal { R }\end{aligned}</script><p>计算复杂度为: $\mathcal{O}(n^3)$. 因此, 只适合直接求解小规模的MRP问题.</p><p>对于大规模的MRP问题, 通常采取以下的迭代方法:</p><ul><li>动态规划(Dynamic programming)</li><li>蒙特卡洛评估(Monte-Carlo evaluation)</li><li>时序差分学习(Temporal-Difference learning)</li></ul><hr><h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p><strong>马尔可夫决策过程(Markov Decision Process, MDP)</strong>是<em>带有决策的马尔可夫奖励过程</em>.</p><blockquote><p>马尔可夫决策过程是一个五元组&lt;$\mathcal{S}$, <font color="red">$\mathcal{A}$</font>, $\mathcal{P}$, $\mathcal{R}$, $\mathcal{\gamma}$&gt;</p><ul><li>$\mathcal{S}$: 有限的状态集</li><li><font color="red"> $\mathcal{A}$: 有限的动作集</font></li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } ^ {a}= \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s, A _ { t } = a \right]$</li><li>$\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } ^ {a} = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s, A _ { t } = a \right]$</li><li>$\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </li></ul></blockquote><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0l47drh0vj20g30d93zc.jpg" alt="Example: Student MDP" width="45%" height="45%"></p><hr><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><blockquote><p><strong>策略(Policy) $\pi$</strong> 是给定状态的动作分布.</p><script type="math/tex; mode=display">\pi ( a | s ) = \mathbb { P } \left[ A _ { t } = a | S _ { t } = s \right]</script></blockquote><ul><li>策略完全决定智能体的行为;</li><li>MDP策略值依赖于当前状态(无关历史);</li><li>策略是固定的(与时间无关). $A _ { t } \sim \pi ( \cdot | S _ { t } ) , \forall t &gt; 0$</li></ul><p>给定一个马尔可夫决策过程 $M = &lt;\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma}&gt;$ 和 一个策略 $\pi$, 其可以转化为<em>马尔可夫过程</em>和<em>马尔可夫奖励过程</em>.</p><ul><li><p>状态序列 $S_1, S_2, \dots$ 是马尔科夫决策过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}&gt;$.</p></li><li><p>状态和奖励序列 $S_1, R_2, S_2, \dots$ 是马尔科夫奖励过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$.</p></li></ul><p>其中,</p><script type="math/tex; mode=display">\mathcal{P}_{s,s'}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{P}_{ss'}^{a}</script><script type="math/tex; mode=display">\mathcal{R}_{s}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{R}_{s}^{a}</script><hr><h3 id="值函数-1"><a href="#值函数-1" class="headerlink" title="值函数"></a>值函数</h3><p><strong>值函数(Value Function)</strong>可分为<strong>状态值函数(state-value function)</strong>和<strong>动作值函数(action-value function)</strong>.</p><blockquote><p>MDP的<strong>状态值函数 $v_{\pi}(s)$ </strong>是从状态 $s$ 开始, 然后按照策略 $\pi$ 决策所获得的期望回报.</p><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t | S_t = s \right]</script><p>MDP的<strong>动作值函数 $q_{\pi}(s, a)$ </strong>是从状态 $s$ 开始, 采取动作 $a$, 然后按照策略 $\pi$ 决策所获得的期望回报.</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t = a \right]</script></blockquote><hr><h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>状态值函数可以被分解为两部分, <strong>立即奖励 + 后继状态的折扣价值</strong>.</p><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s \right]</script><p>动作值函数也可以类似地分解.</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \right]</script><hr><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0ldl141fkj20bb04xq2x.jpg" width="40%" height="40%"></p><p>上图中, 空心圆圈代表状态, 实心圆圈代表动作.</p><p>在已知策略 $\pi$ 的情况下, 状态值函数 $v_{\pi}(s)$ 可以用动作值函数 $q_{\pi}(s, a)$ 进行表示:</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a) \tag{3}\label{eq:mdp-state-value-function}</script><hr><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0lds6jc80j20b004rmx6.jpg" width="40%" height="40%"></p><p>同理, 动作值函数 $q_{\pi}(s, a)$ 也可以用状态值函数 $v_{\pi}(s)$ 进行表示:</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}v_{\pi}(s') \tag{4}\label{eq:mdp-action-value-function}</script><hr><p><strong>状态值函数的贝尔曼期望方程:</strong></p><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0le5yxgeij20b706hdfx.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:mdp-action-value-function}代入公式\eqref{eq:mdp-state-value-function}中, 可得状态值函数的贝尔曼期望方程:</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{\pi}(s')  \right)</script><hr><p><strong>动作值函数的贝尔曼期望方程:</strong></p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0le9cf2u7j20bd05wwek.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:mdp-state-value-function}代入公式\eqref{eq:mdp-action-value-function}中, 可得动作值函数的贝尔曼期望方程:</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} \sum \limits_{a' \in \mathcal{A}} \pi (a' | s') q_{\pi}(s', a')</script><hr><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lecy0oxgj20h90dcwfj.jpg" alt="状态值函数的贝尔曼期望方程示例" width="55%" height="55%"></p><hr><p><strong>贝尔曼期望方程的矩阵形式:</strong></p><script type="math/tex; mode=display">v_{\pi} = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} v_{\pi}</script><p>可直接求解:</p><script type="math/tex; mode=display">v_{\pi} = (I - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}</script><hr><h3 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h3><blockquote><p><strong>最优状态值函数(optimal state-value function)</strong> $v_{*}(s)$ 是所有策略中最大的值函数.</p><script type="math/tex; mode=display">v_{*}(s) = \max \limits_{\pi}v_{\pi}(s)</script><p><strong>最优动作值函数(optimal action-value function)</strong> $q_{*}(s, a)$ 是所有策略中最大的动作值函数.</p><script type="math/tex; mode=display">q_{*}(s, a) = \max \limits_{\pi}q_{\pi}(s, a)</script></blockquote><ul><li>最优值函数代表了MDP的最好性能.</li><li>当得知最优值函数时, MDP可被认为”已解决”.</li></ul><hr><p>例子: </p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leoxfaylj20h70ee75c.jpg" alt="Student MDP中的最优状态值函数" width="50%" height="50%"></p><hr><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leqk38l4j20hh0eg75i.jpg" alt="Student MDP中的最优动作值函数" width="50%" height="50%"></p><p>注: 根据公式\eqref{eq:mdp-state-value-function}, Pub动作的最优值应为 $q_{*} = +1 + (0.2 \times 6 + 0.4 \times 8 + 0.4 \times 10) = 9.4$.</p><hr><h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p>首先定义策略之间的偏序关系, 使得策略之间可以进行比较:</p><script type="math/tex; mode=display">\pi \geq \pi ' \quad \text{if} \quad  v_{\pi}(s) \geq v_{\pi '}(s) , \forall s</script><p>对于任意的MDP来说:</p><ul><li>存在一个最优策略 $\pi_{*}$, 使得 $\pi_{*} \geq \pi, \forall \pi$</li><li>所有的最优策略都能取得最优值函数 $v_{\pi_{*}}(s) = v_{*}(s)$</li><li>所有的最优策略都能取得最优动作值函数 $q_{\pi_{*}}(s, a) = q_{*}(s, a)$</li></ul><hr><p><strong>寻找最优策略</strong></p><p>一个最优策略可以通过最大化所有的 $q_{*}(s, a)$ 得到:</p><script type="math/tex; mode=display">\pi_{*} \left( a | s \right) = \left \{ \begin{array}{ll}1 \ {\mathbb {if}} \ a = \operatorname*{argmax} \limits_{a \in \mathcal{A}} \ q_{*} \left( s,a \right) \\              0 \ {\mathbb {otherwise}}              \end{array} \right.</script><ul><li>对于任意的MDP, 总存在确定的最优策略</li><li>如果我们知道 $q_{*}(s, a)$, 则可以立即得到最优策略</li></ul><hr><p>例子:</p><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lfhg0710j20hn0ehjsl.jpg" alt="Student MDP的最优策略" width="50%" height="50%"></p><p>图中红色弧线表示每个状态的最优决策.</p><hr><h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>$v_{*}$可以通过贝尔曼最优方程递归得到:</p><p><img src="https://ws1.sinaimg.cn/large/8662e3cegy1g0lfkujh38j20b804uaa2.jpg" width="40%" height="40%"></p><script type="math/tex; mode=display">v_{*}(s) = \max \limits_{a} q_{*}(s, a)\tag{5}\label{eq:state-bellman-optimal-equation}</script><p>与公式\eqref{eq:mdp-state-value-function}的贝尔曼期望方程进行比较, 此时不再取均值, 而是取最大值.</p><hr><p>$q_{*}$与公式\eqref{eq:mdp-action-value-function}类似:</p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m10t6s7vj208003a747.jpg" width="40%" height="40%"></p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)\tag{6}\label{eq:action-bellman-optimal-equation}</script><hr><p><strong>状态值函数的贝尔曼最优方程</strong></p><p><img src="https://wx3.sinaimg.cn/large/8662e3cegy1g0m14a2fenj208m04xq2x.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:action-bellman-optimal-equation}代入公式\eqref{eq:state-bellman-optimal-equation}可得 $v_{*}$ 的贝尔曼最优方程:</p><script type="math/tex; mode=display">v _ { * } ( s ) = \max _ { a } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script><hr><p><strong>动作值函数的贝尔曼最优方程</strong></p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0m18irqg7j208804bgll.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:state-bellman-optimal-equation}代入公式\eqref{eq:action-bellman-optimal-equation}可得 $q_{*}$ 的贝尔曼最优方程:</p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \max _ { a ^ { \prime } } q _ { * } \left( s ^ { \prime } , a ^ { \prime } \right)</script><hr><p>例子:</p><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m1ato6q2j20d70atjs3.jpg" alt="Student MDP贝尔曼最优方程" width="50%" height="50%"></p><hr><h3 id="贝尔曼最优方程的求解"><a href="#贝尔曼最优方程的求解" class="headerlink" title="贝尔曼最优方程的求解"></a>贝尔曼最优方程的求解</h3><p>贝尔曼最优方程<strong>不是线性的</strong>(因为有取$max$操作), 因此没有封闭解(Closed-form solution).</p><p>通常采用迭代求解方法:</p><ul><li>值迭代(Value Iteration)</li><li>策略迭代(Policy Iteration)</li><li>Q-Learning</li><li>Sarsa</li></ul><h2 id="MDP的扩展"><a href="#MDP的扩展" class="headerlink" title="MDP的扩展"></a>MDP的扩展</h2><ul><li>无穷和连续的MDPs</li><li>部分可观测的MDPs</li><li>不折扣, 平均奖励MDPs</li></ul><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第三章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf" target="_blank" rel="noopener">Lecture2: Markov Decision Processes</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;在上一篇文章 &lt;a href=&quot;https://orzyt.cn/posts/introduction-to-rl&quot;&gt;强化学习（一）：强化学习简介&lt;/a&gt; 中, 我们介绍了强化学习的一些基本概念.&lt;/p&gt;
&lt;p&gt;本文将介绍用来对强化学习问题进行建模的&lt;strong&gt;马尔可夫决策过程(Markov Decision Processes, MDPs)&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="马尔可夫决策过程" scheme="https://orzyt.cn/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（一）：强化学习简介</title>
    <link href="https://orzyt.cn/posts/introduction-to-rl/"/>
    <id>https://orzyt.cn/posts/introduction-to-rl/</id>
    <published>2019-02-25T14:56:13.000Z</published>
    <updated>2019-03-08T13:40:47.760Z</updated>
    
    <content type="html"><![CDATA[<hr><p>本文主要介绍强化学习中的一些基本概念.</p><hr><a id="more"></a><h2 id="强化学习的特征"><a href="#强化学习的特征" class="headerlink" title="强化学习的特征"></a>强化学习的特征</h2><p>作为机器学习的一个分支，强化学习主要的特征为:</p><ul><li><p>无监督,仅有奖励信号；</p></li><li><p>反馈有延迟,不是瞬时的;</p></li><li><p>时间是重要的(由于是时序数据,不是独立同分布的);</p></li><li><p>Agent的动作会影响后续得到的数据;</p></li></ul><hr><h2 id="强化学习的概念"><a href="#强化学习的概念" class="headerlink" title="强化学习的概念"></a>强化学习的概念</h2><h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>奖励(Rewards) $R_t$ 是一个标量的反馈信号,表示Agent在 $t$ 时刻的表现如何.</p><p><strong>Agent的目标</strong>: 最大化累积奖励(maximise cumulative reward).</p><p>强化学习基于<strong>奖励假设(reward hypothesis)</strong>.</p><blockquote><p><strong>奖励假设(Reward Hypothesis)</strong>:<br>所有强化学习任务的目标都可以被描述为最大化期望累积奖励.</p></blockquote><hr><h3 id="序贯决策"><a href="#序贯决策" class="headerlink" title="序贯决策"></a>序贯决策</h3><p><strong>序贯决策(Sequential Decision Making)的目标</strong>: 选择合适的动作最大化将来的累积奖励.</p><ul><li>动作可能会产生长期后果；</li><li>奖励会有延迟性;</li><li>牺牲立即回报可能会获得更多的长期回报.</li></ul><hr><h3 id="智能体和环境"><a href="#智能体和环境" class="headerlink" title="智能体和环境"></a>智能体和环境</h3><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k2ozf0lzj20aq0bxtb1.jpg" alt="Agent和环境" width="35%" height="35%"></p><p>智能体(Agent)在每个时刻$t$会:</p><ul><li>执行动作(Action)$A_t$;</li><li>接收观测(Observation)$O_t$;</li><li>接收标量奖励(Reward)$R_t$.</li></ul><p>而环境(Environment)则会:</p><ul><li>接收动作(Action)$A_t$;</li><li>产生观测(Observation)$O_{t+1}$;</li><li>产生标量奖励(Reward)$R_{t+1}$.</li></ul><hr><h3 id="历史与状态"><a href="#历史与状态" class="headerlink" title="历史与状态"></a>历史与状态</h3><blockquote><p><strong>历史(History):</strong>由一系列观测,动作和奖励构成.</p></blockquote><script type="math/tex; mode=display">H_t = O_1, R_1, A_1, \dots, A_{t-1}, O_t, R_t</script><p>下一步将发生什么取决于历史:</p><ul><li>智能体选择的action;</li><li>环境选择的observations/rewards.</li></ul><blockquote><p><strong>状态(State)</strong>:用来决定接下来会发生什么的信息.</p></blockquote><p><strong>状态是历史的函数:</strong></p><script type="math/tex; mode=display">S_t = f(H_t)</script><hr><h4 id="环境状态"><a href="#环境状态" class="headerlink" title="环境状态"></a>环境状态</h4><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3akygqpj20b20cptb5.jpg" alt="环境状态" width="35%" height="35%"></p><p>环境状态 $S_{t}^{e}$ 是环境的私有表示,通常对于智能体来说该状态不可见.</p><p>即使$S_{t}^{e}$可见,也可能包含不相关信息.</p><hr><h4 id="智能体状态"><a href="#智能体状态" class="headerlink" title="智能体状态"></a>智能体状态</h4><p><img src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k3e8bw4aj20b00cx419.jpg" alt="智能体状态" width="35%" height="35%"></p><p>智能体状态 $S_{t}^{a}$ 是智能体的内部表示,包含其用来决定下一步动作的信息,也是强化学习算法使用的信息.</p><p>可以写成历史的函数: $S_{t}^{a} = f(H_t)$</p><hr><h4 id="信息状态"><a href="#信息状态" class="headerlink" title="信息状态"></a>信息状态</h4><p><strong>信息状态(也称为马尔科夫状态)</strong>: 包含历史中所有有用的信息.</p><p><img src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3nzaa8yj20mn0593ym.jpg" alt="马尔科夫状态定义" width="60%" height="60%"></p><p>马尔科夫状态表明: <strong>未来只与现在有关,而与过去无关.</strong></p><p>其中,<strong>环境状态$S_t^e$</strong>和<strong>历史$H_t$</strong>具有马尔科夫性质.</p><hr><h4 id="Rat-Example"><a href="#Rat-Example" class="headerlink" title="Rat Example"></a>Rat Example</h4><p><img src="https://wx2.sinaimg.cn/large/8662e3cegy1g0k3trc5qxj20ny0doq88.jpg" alt="Rat Example" width="60%" height="60%"></p><ul><li><p>假如个体状态=序列中的后三个事件(不包括电击、获得奶酪，下同),事件序列3的结果会是什么? (答案是：电击)</p></li><li><p>假如个体状态=亮灯、响铃和拉电闸各自事件发生的次数,那么事件序列3的结果又是什么? (答案是：奶酪)</p></li><li><p>假如个体状态=完整的事件序列,那结果又是什么? (答案是：未知)</p></li></ul><hr><h4 id="完全可观测环境"><a href="#完全可观测环境" class="headerlink" title="完全可观测环境"></a>完全可观测环境</h4><p><strong>完全可观测性(Full observability):</strong> 智能体可以直接观测到环境状态,即</p><script type="math/tex; mode=display">O_t = S_t^a = S_t^e</script><ul><li>智能体状态 = 环境状态 = 信息状态</li><li>实际上是马尔科夫决策过程(Markov Decision Process, MDP)</li></ul><hr><h4 id="部分可观测环境"><a href="#部分可观测环境" class="headerlink" title="部分可观测环境"></a>部分可观测环境</h4><p><strong>部分可观测性(Partial observability):</strong> 智能体不能够直接观测到环境.</p><p>如,机器人不能通过摄像头得知自身的绝对位置.</p><ul><li>智能体状态 $\neq$ 环境状态</li><li>部分可观测马尔科夫决策过程(POMDP)</li></ul><p>此时,智能体必须构建其自身的状态表示 $S_t^a$,比如:</p><ul><li>完全的历史: $S_t^a = H_t$;</li><li>环境状态的置信度: $S _ { t } ^ { a } = \left( \mathbb { P } \left[ S _ { t } ^ { e } = s ^ { 1 } \right] , \ldots , \mathbb { P } \left[ S _ { t } ^ { e } = s ^ { n } \right] \right)$;</li><li>循环神经网络: $S_t^a = \sigma \left(S_{t-1}^{a}W_{s} + O_{t}W_{o}\right)$</li></ul><hr><h2 id="智能体的构成"><a href="#智能体的构成" class="headerlink" title="智能体的构成"></a>智能体的构成</h2><p>智能体主要包含以下几种成分:</p><ul><li><strong>策略(Policy)</strong>: 智能体的行为函数;</li><li><strong>值函数(Value Function)</strong>: 每个state或action的好坏;</li><li><strong>模型(Model)</strong>: 智能体对环境的表示.</li></ul><hr><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><ul><li>策略(Policy)是智能体的行为;</li><li>是<strong>状态</strong>到<strong>动作</strong>的映射;</li><li>确定性策略: $a = \pi(s)$;</li><li>随机性策略: $\pi(a|s) = \mathbb{P} \left[ A_{t} = a | S_{t} = s\right]$</li></ul><hr><h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数(Value Function)是对于未来奖励的预测.</p><ul><li>用于评价状态的好坏;</li><li>因此可以用来选择动作.</li></ul><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_{t} = s \right]</script><hr><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型(Model)用来预测环境接下来会做什么.</p><ul><li>$\mathcal{P}$: 预测下一个状态.<script type="math/tex; mode=display">\mathcal{P}_{ss'}^{a} = \mathbb{P} \left[ S_{t+1} = s' | S_{t} = s, A_{t} = a\right]</script></li><li>$\mathcal{R}$: 预测下一个(立即)奖励.<script type="math/tex; mode=display">\mathcal{R}_{s}^{a} = \mathbb{E} \left[ R_{t+1} | S_{t} = s, A_{t} = a\right]</script></li></ul><hr><h3 id="Maze-Example"><a href="#Maze-Example" class="headerlink" title="Maze Example"></a>Maze Example</h3><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k4norup9j20mj092dg5.jpg" alt="Maze Example" width="60%" height="60%"></p><hr><p><strong>策略表示:</strong></p><p>箭头表示每个状态的策略 $\pi(s)$.</p><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k4u9pdcdj20f10c5q38.jpg" alt="Maze Example: Policy" width="40%" height="40%"></p><hr><p><strong>值函数表示:</strong></p><p>数值表示每个状态的值 $v_{\pi}(s)$.</p><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k4w9vn7wj20f60cct8y.jpg" alt="Maze Example: Value Function" width="40%" height="40%"></p><hr><p><strong>模型表示:</strong></p><p>智能体可能对环境建立内部模型</p><ul><li>网格布局表示转移模型 $\mathcal{P}_{ss’}^{a}$;</li><li>数值表示每个状态的立即奖励 $\mathcal{R}_{s}^{a}$.</li></ul><p><img src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k51h023dj20c109kt8o.jpg" alt="Maze Example: Value Function" width="40%" height="40%"></p><hr><h3 id="智能体的分类"><a href="#智能体的分类" class="headerlink" title="智能体的分类"></a>智能体的分类</h3><p>按智能体的成分分类:</p><ul><li>基于值函数(Value Based)</li><li>基于策略(Policy Based)</li><li>演员-评论家(Actor Critic)</li></ul><p>或者按有无模型分类:</p><ul><li>无模型(Model Free)</li><li>基于模型(Model Based)</li></ul><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k55tidg0j20f30eaab9.jpg" alt="智能体分类" width="40%" height="40%"></p><hr><h2 id="强化学习的问题"><a href="#强化学习的问题" class="headerlink" title="强化学习的问题"></a>强化学习的问题</h2><h3 id="学习与规划"><a href="#学习与规划" class="headerlink" title="学习与规划"></a>学习与规划</h3><p><strong>强化学习</strong></p><ul><li>环境的初始状态未知;</li><li>智能体与环境进行交互;</li><li>智能体提升其策略.</li></ul><p><img src="https://ws4.sinaimg.cn/large/8662e3cegy1g0k5dprnn4j20p80ejn0q.jpg" alt="学习" width="60%" height="60%"></p><p><strong>规划</strong></p><ul><li>环境的模型已知;</li><li>智能体通过模型进行计算,无须与外部进行交互;</li><li>智能体提升其策略</li></ul><p><img src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k5eleerwj20ok0df0u3.jpg" alt="规划" width="60%" height="60%"></p><hr><h3 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h3><p>强化学习是一种试错(trial-and-error)学习.</p><p>智能体需要从与环境的交互中找到一种好的策略,同时不损失过多的奖励.</p><ul><li><strong>探索(Exploration):</strong> 从环境中寻找更多信息;</li><li><strong>利用(Exploitation):</strong> 利用已知信息使奖励最大化.</li></ul><p>探索和利用同等重要,即使根据已有信息选择出的最优动作可以得到不错的奖励,不妨尝试全新的动作对环境进行探索,也许可以得到更好的结果.</p><hr><h3 id="预测和控制"><a href="#预测和控制" class="headerlink" title="预测和控制"></a>预测和控制</h3><ul><li><strong>预测(Prediction):</strong> 对未来进行评估.</li></ul><p><img src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k5ryo676j20nz0ds0tn.jpg" alt="Gridworld Example: Prediction" width="60%" height="60%"></p><hr><ul><li><strong>控制(Control):</strong> 最优化未来的结果.</li></ul><p><img src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k5r6vdmgj20nv0eot9w.jpg" alt="Gridworld Example: Control" width="60%" height="60%"></p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第一章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf" target="_blank" rel="noopener">Lecture1: Introduction to Reinforcement Learning</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;本文主要介绍强化学习中的一些基本概念.&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode637 Average of Levels in Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode637-average-of-levels-in-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode637-average-of-levels-in-binary-tree/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/average-of-levels-in-binary-tree/" target="_blank" rel="noopener">LeetCode637 Average of Levels in Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的层次遍历,使用空指针作为每层的分界.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; averageOfLevels(TreeNode* root) &#123;</span><br><span class="line">        <span class="built_in">queue</span>&lt;TreeNode*&gt; que;</span><br><span class="line">        que.push(root); que.push(<span class="literal">nullptr</span>);</span><br><span class="line">        <span class="keyword">double</span> sum = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; ret;</span><br><span class="line">        <span class="keyword">while</span> (!que.empty()) &#123;</span><br><span class="line">            TreeNode* u = que.front(); que.pop();</span><br><span class="line">            <span class="keyword">if</span> (u == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                ret.push_back(sum / cnt);</span><br><span class="line">                sum = cnt = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span> (!que.empty()) que.push(<span class="literal">nullptr</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                sum += u-&gt;val; cnt++;</span><br><span class="line">                <span class="keyword">if</span> (u-&gt;left) que.push(u-&gt;left);</span><br><span class="line">                <span class="keyword">if</span> (u-&gt;right) que.push(u-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/average-of-levels-
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode657 Robot Return to Origin</title>
    <link href="https://orzyt.cn/posts/leetcode657-robot-return-to-origin/"/>
    <id>https://orzyt.cn/posts/leetcode657-robot-return-to-origin/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/robot-return-to-origin/" target="_blank" rel="noopener">LeetCode657 Robot Return to Origin</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>判断上和下,左和右的次数是否相同即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">judgeCircle</span><span class="params">(<span class="built_in">string</span> moves)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; f;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; c: moves) f[c]++;</span><br><span class="line">        <span class="keyword">return</span> f[<span class="string">'U'</span>] == f[<span class="string">'D'</span>] &amp;&amp; f[<span class="string">'L'</span>] == f[<span class="string">'R'</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/robot-return-to-or
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode654 Maximum Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode654-maximum-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode654-maximum-binary-tree/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/maximum-binary-tree/" target="_blank" rel="noopener">LeetCode654 Maximum Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>递归构造二叉搜索树,树的左右儿子都比父结点小.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">helper</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        TreeNode* node = <span class="keyword">new</span> TreeNode(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">int</span> num = nums[l], id = l;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = l; i &lt;= r; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (num &lt; nums[i]) &#123;</span><br><span class="line">                num = nums[i];</span><br><span class="line">                id = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        node-&gt;val = num;</span><br><span class="line">        <span class="keyword">if</span> (l &lt;= id - <span class="number">1</span>) node-&gt;left = helper(nums, l, id - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (id + <span class="number">1</span> &lt;= r) node-&gt;right = helper(nums, id + <span class="number">1</span>, r);</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">TreeNode* <span class="title">constructMaximumBinaryTree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> helper(nums, <span class="number">0</span>, nums.size() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/maximum-binary-tre
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode661 Image Smoother</title>
    <link href="https://orzyt.cn/posts/leetcode661-image-smoother/"/>
    <id>https://orzyt.cn/posts/leetcode661-image-smoother/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/image-smoother/" target="_blank" rel="noopener">LeetCode661 Image Smoother</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>按题意模拟即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dx[<span class="number">9</span>] = &#123;<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dy[<span class="number">9</span>] = &#123;<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; imageSmoother(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; M) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = M.size(), m = M[<span class="number">0</span>].size();</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ret(n, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; m; ++j) &#123;</span><br><span class="line">                <span class="keyword">int</span> sum = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">9</span>; ++k) &#123;</span><br><span class="line">                    <span class="keyword">int</span> x = i + dx[k], y = j + dy[k];</span><br><span class="line">                    <span class="keyword">if</span> (x &lt; <span class="number">0</span> || x &gt;= n || y &lt; <span class="number">0</span> || y &gt;= m) <span class="keyword">continue</span>;</span><br><span class="line">                    sum += M[x][y];</span><br><span class="line">                    cnt++;</span><br><span class="line">                &#125;</span><br><span class="line">                ret[i].push_back(sum / cnt);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/image-smoother/&quot; t
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode665 Non-decreasing Array</title>
    <link href="https://orzyt.cn/posts/leetcode665-non-decreasing-array/"/>
    <id>https://orzyt.cn/posts/leetcode665-non-decreasing-array/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/non-decreasing-array/" target="_blank" rel="noopener">LeetCode665 Non-decreasing Array</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>判断只修改一个数能否使得数组非递减.</p><p>首先计算数组从左往右能延伸的距离$l$,从右往左能延伸的距离$r$.</p><p>可行的情况有:</p><ul><li>$ r \leq l $</li><li>$r == l + 1$ 且 满足下列情况之一<ul><li>r 为最后一位</li><li>l位置的值 $ \leq $ r + 1位置的值</li><li>l为首位 </li><li>l-1位置的值 $ \leq $ r 位置的值</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">checkPossibility</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> siz = nums.size(), l = <span class="number">0</span>, r = siz - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (l + <span class="number">1</span> &lt; siz &amp;&amp; nums[l] &lt;= nums[l + <span class="number">1</span>]) l++;</span><br><span class="line">        <span class="keyword">while</span> (r &gt; <span class="number">0</span> &amp;&amp; nums[r - <span class="number">1</span>] &lt;= nums[r]) r--;</span><br><span class="line">        <span class="keyword">return</span> r &lt;= l || (r == l + <span class="number">1</span> &amp;&amp; ((nums[l] &lt;= nums[r + <span class="number">1</span>] || r == siz - <span class="number">1</span>) || (l == <span class="number">0</span> || nums[l - <span class="number">1</span>] &lt;= nums[r])));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/non-decreasing-arr
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode617 Merge Two Binary Trees</title>
    <link href="https://orzyt.cn/posts/leetcode617-merge-two-binary-trees/"/>
    <id>https://orzyt.cn/posts/leetcode617-merge-two-binary-trees/</id>
    <published>2019-02-08T09:02:05.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/merge-two-binary-trees/" target="_blank" rel="noopener">LeetCode617 Merge Two Binary Trees</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的合并.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">mergeTrees</span><span class="params">(TreeNode* t1, TreeNode* t2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (t1 &amp;&amp; t2) &#123;</span><br><span class="line">            t1-&gt;val += t2-&gt;val;</span><br><span class="line">            t1-&gt;left = mergeTrees(t1-&gt;left, t2-&gt;left);</span><br><span class="line">            t1-&gt;right = mergeTrees(t1-&gt;right, t2-&gt;right);</span><br><span class="line">            <span class="keyword">return</span> t1;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">return</span> t1 ? t1 : t2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/merge-two-binary-t
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode606 Construct String from Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode606-construct-string-from-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode606-construct-string-from-binary-tree/</id>
    <published>2019-02-08T09:01:50.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/construct-string-from-binary-tree/" target="_blank" rel="noopener">LeetCode606 Construct String from Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的简单遍历.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">dfs</span><span class="params">(TreeNode* t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (t == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">if</span> (t-&gt;left == <span class="literal">NULL</span> &amp;&amp; t-&gt;right == <span class="literal">NULL</span>) <span class="keyword">return</span> to_string(t-&gt;val);</span><br><span class="line">        <span class="built_in">string</span> ret = to_string(t-&gt;val) + <span class="string">"("</span> + dfs(t-&gt;left) + <span class="string">")"</span>;</span><br><span class="line">        <span class="keyword">if</span> (t-&gt;right) ret += <span class="string">"("</span> + dfs(t-&gt;right) + <span class="string">")"</span>;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">tree2str</span><span class="params">(TreeNode* t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> dfs(t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/construct-string-f
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode599 Minimum Index Sum of Two Lists</title>
    <link href="https://orzyt.cn/posts/leetcode599-minimum-index-sum-of-two-lists/"/>
    <id>https://orzyt.cn/posts/leetcode599-minimum-index-sum-of-two-lists/</id>
    <published>2019-02-08T09:01:34.000Z</published>
    <updated>2019-02-08T10:25:59.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/minimum-index-sum-of-two-lists/" target="_blank" rel="noopener">LeetCode599 Minimum Index Sum of Two Lists</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>找出交集中下标和最小值.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; findRestaurant(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list1, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list2) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; ans;</span><br><span class="line">        <span class="keyword">int</span> n = list1.size(), m = list2.size();</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; hs;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) hs[list1[i]] = i;</span><br><span class="line">        <span class="keyword">int</span> minSum = INT_MAX;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (hs.count(list2[i])) &#123;</span><br><span class="line">                <span class="keyword">int</span> j = hs[list2[i]];</span><br><span class="line">                <span class="keyword">if</span> (i + j &lt; minSum) &#123;</span><br><span class="line">                    minSum = i + j;</span><br><span class="line">                    ans.clear();</span><br><span class="line">                    ans.push_back(list2[i]);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i + j == minSum) &#123;</span><br><span class="line">                    ans.push_back(list2[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/minimum-index-sum-
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode38 Count and Say</title>
    <link href="https://orzyt.cn/posts/leetcode38-count-and-say/"/>
    <id>https://orzyt.cn/posts/leetcode38-count-and-say/</id>
    <published>2019-02-08T09:00:37.000Z</published>
    <updated>2019-02-08T10:25:59.942Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/count-and-say/" target="_blank" rel="noopener">LeetCode38 Count and Say</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>按照规则模拟即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">countAndSay</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">string</span> s = <span class="string">"1"</span>, ans = s;</span><br><span class="line">        <span class="keyword">while</span> (--n) &#123;</span><br><span class="line">            ans.clear();</span><br><span class="line">            <span class="keyword">int</span> len = s.size();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; ++i) &#123;</span><br><span class="line">                <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">while</span> (s[i] == s[i + <span class="number">1</span>] &amp;&amp; i + <span class="number">1</span> &lt; len) &#123;</span><br><span class="line">                    count++;</span><br><span class="line">                    i++;</span><br><span class="line">                &#125;</span><br><span class="line">                ans += to_string(count) + s[i];</span><br><span class="line">            &#125;</span><br><span class="line">            s = ans;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/count-and-say/&quot; ta
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>【论文笔记】深度人脸识别综述</title>
    <link href="https://orzyt.cn/posts/deep-face-recognition/"/>
    <id>https://orzyt.cn/posts/deep-face-recognition/</id>
    <published>2019-01-22T06:04:24.000Z</published>
    <updated>2019-02-26T11:20:51.367Z</updated>
    
    <content type="html"><![CDATA[<hr><p><strong>论文题目</strong>：《Deep Face Recognition: A Survey》</p><p><strong>论文作者</strong>：Mei Wang, Weihong Deng</p><p><strong>论文链接</strong>：<a href="http://cn.arxiv.org/pdf/1804.06655.pdf" target="_blank" rel="noopener">http://cn.arxiv.org/pdf/1804.06655.pdf</a></p><hr><a id="more"></a><p>随着2012年AlexNet赢得了ImageNet挑战赛的冠军后，深度学习技术在各个领域都发挥着重要的作用，极大地提升了许多任务的SOTA。2014年，DeepFace<sup><a href="#fn_1" id="reffn_1">1</a></sup>首次在著名的非受限环境人脸数据集——LFW上取得了与人类相媲美的准确率（DeepFace: 97.35% vs. Human: 97.53%）。因此，本文主要关注深度学习技术在人脸识别领域的应用与发展。</p><h2 id="概念和术语"><a href="#概念和术语" class="headerlink" title="概念和术语"></a>概念和术语</h2><p>人脸系统一般包括三个部分：</p><ul><li><p><strong>人脸检测（face detection）</strong>：对于一幅图像，检测其中人脸的位置；</p></li><li><p><strong>人脸对齐（face alignment）</strong>：根据人脸关键点，将人脸对齐到一个典型的角度；</p></li><li><p><strong>人脸识别（face recognition</strong>）：包括人脸处理、人脸表示和人脸匹配部分。</p></li></ul><p><img alt="人脸系统示意图" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k1j4up5oj21cb0ik489.jpg" width="100%" height="100%"></p><ul><li><p><strong>训练集（training set）</strong>：用于训练系统的人脸集；</p></li><li><p><strong>注册集（gallery set）</strong>：提前注册在系统中用于比对的标准人脸集；</p></li><li><p><strong>测试集（probe set）</strong>：用于测试的人脸集。</p></li></ul><p>人脸识别任务主要包括：</p><ul><li><p><strong>人脸认证（face identification）</strong>：为<strong>1:N</strong>的问题。通过计算测试个体与注册集个体的相似度，判断出当前测试个体的身份。根据测试集中的个体是否出现在注册集中，可分为<strong>闭集（closed-set）</strong>和<strong>开集（open-set）</strong>问题。</p></li><li><p><strong>人脸验证（face verification）</strong>：为<strong>1:1</strong>的问题。对测试集和验证集中的个体进行两两比对，判断是否是同一个体。</p></li></ul><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><h3 id="主流结构"><a href="#主流结构" class="headerlink" title="主流结构"></a>主流结构</h3><p>在人脸识别问题中，主流的网络结构基本上都借鉴于物体分类问题，一直从AlexNet到SENet。</p><p>在2014年，DeepFace<sup><a href="#fn_1" id="reffn_1">1</a></sup>首次使用九层的卷积神经网络，经过3D人脸对齐处理，在LFW上达到了97.35%的准确率。在2015年，FaceNet<sup><a href="#fn_9" id="reffn_9">9</a></sup>在一个很大的私人数据集上训练GoogLeNet，采用triplet loss，得到99.63%的准确率。同年，VGGface<sup><a href="#fn_10" id="reffn_10">10</a></sup>从互联网中收集了一个大的数据集，并在其上训练VGGNet，得到了98.95%的准确率。在2017年，SphereFace<sup><a href="#fn_11" id="reffn_11">11</a></sup>使用64层的ResNet结构，采用angular softmax（A-softmax）loss，得到99.42%的准确率。在2017年末，VGGFace2<sup><a href="#fn_12" id="reffn_12">12</a></sup>作为一个新人脸的数据集被引入，同时使用SENet进行训练，在IJB-A和IJB-B上都取得SOTA。</p><p><img alt="主流网络结构的演变" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1jnv2szj210109oacr.jpg" width="90%" height="90%"></p><ul><li><p><code>AlexNet</code><sup><a href="#fn_2" id="reffn_2">2</a></sup>：AlexNet包括五个卷积层和三个全连接层，并且集成了如ReLU、dropout、数据增强等技术；</p></li><li><p><code>VGGNet</code><sup><a href="#fn_3" id="reffn_3">3</a></sup>：使用3×3卷积核，且每经过2×2的池化后特征图数量加倍，网络深度为16-19层；</p></li><li><p><code>GoogLeNet</code><sup><a href="#fn_4" id="reffn_4">4</a></sup>：提出了inception module，对不同尺度的特征图进行混合；</p></li><li><p><code>ResNet</code><sup><a href="#fn_5" id="reffn_5">5</a></sup>：通过学习残差表示，使得训练更深网络成为可能；</p></li><li><p><code>SENet</code><sup><a href="#fn_6" id="reffn_6">6</a></sup>：提出了Squeeze-and-Excitation操作，通过显式建模channel之间的相互依赖性，自适应地重新校准channel间的特征响应。</p></li></ul><p><img alt="主流网络结构示意图" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1jytblsj20of0kgwhp.jpg" width="60%" height="60%"></p><h3 id="特殊结构"><a href="#特殊结构" class="headerlink" title="特殊结构"></a>特殊结构</h3><ul><li><p><code>Light CNN</code><sup><a href="#fn_7" id="reffn_7">7</a></sup></p></li><li><p><code>bilinear CNN</code><sup><a href="#fn_8" id="reffn_8">8</a></sup></p></li><li><p>…</p></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在一开始，人们使用和物体分类同样的基于交叉熵的softmax loss，后来发现其不适用于人脸特征的学习，于是开始探索更具有判别性的loss。</p><p><img alt="人脸损失函数的演变" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1kfbt7kj217u0e8dlk.jpg" width="90%" height="90%"></p><p><img alt="不同方法在LFW数据集上的准确率" src="https://wx2.sinaimg.cn/large/8662e3cegy1g0k1kr4sx8j215r0k9wlm.jpg" width="90%" height="90%"></p><h3 id="基于欧几里德距离"><a href="#基于欧几里德距离" class="headerlink" title="基于欧几里德距离"></a>基于欧几里德距离</h3><hr><h4 id="contrastive-loss"><a href="#contrastive-loss" class="headerlink" title="contrastive loss"></a>contrastive loss</h4><p>相关文献：</p><ul><li><strong>《Deep learning face representation by joint identification-verification》</strong></li><li><strong>《Deepid3: Face recognition with very deep neural networks》</strong></li></ul><p>DeepID系列使用的loss。</p><script type="math/tex; mode=display">\operatorname { Verif } \left( f _ { i } , f _ { j } , y _ { i j } , \theta _ { v e } \right) = \left\{ \begin{array} { l l } { \frac { 1 } { 2 } \left\| f _ { i } - f _ { j } \right\| _ { 2 } ^ { 2 } } & { \text { if } y _ { i j } = 1 } \\ { \frac { 1 } { 2 } \max \left( 0 , m - \left\| f _ { i } - f _ { j } \right\| _ { 2 } \right) ^ { 2 } } & { \text { if } y _ { i j } = - 1 } \end{array} \right.</script><hr><h4 id="triplet-loss"><a href="#triplet-loss" class="headerlink" title="triplet loss"></a>triplet loss</h4><p>相关文献：</p><ul><li><strong>《Facenet: A unified embedding for face recognition and clustering》</strong></li></ul><script type="math/tex; mode=display">\mathcal{L} = \sum _ { i } ^ { N } \left[ \left\| f \left( x _ { i } ^ { a } \right) - f \left( x _ { i } ^ { p } \right) \right\| _ { 2 } ^ { 2 } - \left\| f \left( x _ { i } ^ { a } \right) - f \left( x _ { i } ^ { n } \right) \right\| _ { 2 } ^ { 2 } + \alpha \right] _ { + }</script><p><img alt="triplet loss示意图" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k1l9pm84j20x007ht9n.jpg" width="70%" height="70%"></p><hr><h4 id="center-loss"><a href="#center-loss" class="headerlink" title="center loss"></a>center loss</h4><p>相关文献：</p><ul><li><strong>《A Discriminative Feature Learning Approach for Deep Face Recognition》</strong></li></ul><script type="math/tex; mode=display">\begin{aligned} \mathcal { L } & = \mathcal { L } _ { S } + \lambda \mathcal { L } _ { C } \\ & = - \sum _ { i = 1 } ^ { m } \log \frac { e ^ { W _ { y _ { i } } ^ { T } \boldsymbol { x } _ { i } + b _ { y _ { i } } } } { \sum _ { j = 1 } ^ { n } e ^ { W _ { j } ^ { T } \boldsymbol { x } _ { i } + b _ { j } } } + \frac { \lambda } { 2 } \sum _ { i = 1 } ^ { m } \left\| \boldsymbol { x } _ { i } - \boldsymbol { c } _ { y _ { i } } \right\| _ { 2 } ^ { 2 } \end{aligned}</script><p><img alt="center loss示意图" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k1lnsllgj20tr0m1qdq.jpg" width="60%" height="60%"></p><hr><h4 id="range-loss"><a href="#range-loss" class="headerlink" title="range loss"></a>range loss</h4><p>相关文献：</p><ul><li><strong>《Range loss for deep face recognition with long-tail》</strong></li></ul><script type="math/tex; mode=display">\mathcal { L } _ { R } = \alpha \mathcal { L } _ { R _ { i n t r a } } + \beta \mathcal { L } _ { R _ { i n t e r } }</script><script type="math/tex; mode=display">\mathcal { L } _ { R _ { i n t r a } } = \sum _ { i \subseteq I } \mathcal { L } _ { R _ { i n t r a }}^ { i } = \sum _ { i \subseteq I } \frac { k } { \sum _ { j = 1 } ^ { k } \frac { 1 } { \mathcal { D } _ { j } } }</script><script type="math/tex; mode=display">\begin{aligned} \mathcal { L } _ { R _ { \text {inter} } } & = \max \left( m - \mathcal { D } _ { C e n t e r } , 0 \right) \\ & = \max \left( m - \left\| \overline { x } _ { \mathcal { Q } } - \overline { x } _ { \mathcal { R } } \right\| _ { 2 } ^ { 2 } , 0 \right) \end{aligned}</script><script type="math/tex; mode=display">\mathcal { L } = \mathcal { L } _ { M } + \lambda \mathcal { L } _ { R } = - \sum _ { i = 1 } ^ { M } \log \frac { e ^ { W _ { y _ { i } } ^ { T } x _ { i } + b _ { v _ { i } } } } { \sum _ { j = 1 } ^ { n } e ^ { W _ { j } ^ { T } x _ { i } + b _ { j } } } + \lambda \mathcal { L } _ { R }</script><hr><h4 id="center-invariant-loss"><a href="#center-invariant-loss" class="headerlink" title="center-invariant loss"></a>center-invariant loss</h4><p>相关文献：</p><ul><li><strong>《Deep face recognition with center invariant loss》</strong></li></ul><script type="math/tex; mode=display">\begin{aligned} L = & L _ { s } + \gamma L _ { I } + \lambda L _ { c } \\ = & - \log \left( \frac { e ^ { \mathbf { w } _ { y } ^ { T } \mathbf { x } _ { i } + b _ { y } } } { \sum _ { j = 1 } ^ { m } e ^ { \mathbf { w } _ { j } ^ { T } \mathbf { x } _ { i } + b _ { j } } } \right) + \frac { \gamma } { 4 } \left( \left\| \mathbf { c } _ { y } \right\| _ { 2 } ^ { 2 } - \frac { 1 } { m } \sum _ { k = 1 } ^ { m } \left\| \mathbf { c } _ { k } \right\| _ { 2 } ^ { 2 } \right) ^ { 2 } \\ & + \frac { \lambda } { 2 } \left\| \mathbf { x } _ { i } - \mathbf { c } _ { y } \right\| ^ { 2 } \end{aligned}</script><p><img alt="center invariant loss示意图" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k1lvxp55j20zt0i4n3g.jpg" width="60%" height="60%"></p><hr><h3 id="基于角度-余弦间隔"><a href="#基于角度-余弦间隔" class="headerlink" title="基于角度/余弦间隔"></a>基于角度/余弦间隔</h3><hr><h4 id="L-Softmax-loss"><a href="#L-Softmax-loss" class="headerlink" title="L-Softmax loss"></a>L-Softmax loss</h4><p>相关文献：</p><ul><li><strong>《Large-margin softmax loss for convolutional neural networks》</strong></li></ul><script type="math/tex; mode=display">L _ { i } = - \log \left( \frac { e ^ { \left\| \boldsymbol { W } _ { y _ { i } } \right\| \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { y _ { i } } \right) } } { e ^ { \left\| \boldsymbol { W } _ { y _ { i } } \right\| \boldsymbol { w } \left( \theta _ { \boldsymbol { y } _ { i } } \right) } + \sum _ { j \neq y _ { i } } e ^ { \left\| \boldsymbol { W } _ { j } \right\| \left\| \boldsymbol { x } _ { i } \right\| \cos \left( \theta _ { j } \right) } } \right)</script><script type="math/tex; mode=display">\psi ( \theta ) = ( - 1 ) ^ { k } \cos ( m \theta ) - 2 k , \quad \theta \in \left[ \frac { k \pi } { m } , \frac { ( k + 1 ) \pi } { m } \right]</script><script type="math/tex; mode=display">f _ { y _ { i } } = \frac { \lambda \left\| \boldsymbol { W } _ { y _ { i } } \right\| \left\| \boldsymbol { x } _ { i } \right\| \cos \left( \theta _ { y _ { i } } \right) + \left\| \boldsymbol { W } _ { y _ { i } } \right\| \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { \boldsymbol { y } _ { i } } \right) } { 1 + \lambda }</script><p><img alt="L-Softmax loss二分类示意图" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k1m4b63ij20fv0ib40x.jpg" width="50%" height="50%"></p><hr><h4 id="A-Softmax-loss"><a href="#A-Softmax-loss" class="headerlink" title="A-Softmax loss"></a>A-Softmax loss</h4><p>相关文献：</p><ul><li><strong>《Sphereface: Deep hypersphere embedding for face recognition》</strong></li></ul><script type="math/tex; mode=display">L _ { \mathrm { ang } } = \frac { 1 } { N } \sum _ { i } - \log \left( \frac { e ^ { \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { y _ { i } , i } \right) } } { e ^ { \left\| \boldsymbol { x } _ { i } \right\| \psi \left( \theta _ { y _ { i } } , i \right) } + \sum _ { j \neq y _ { i } } e ^ { \left\| \boldsymbol { x } _ { i } \right\| \cos \left( \theta _ { j , i } \right) } } \right)</script><script type="math/tex; mode=display">\psi \left( \theta _ { y _ { i } , i } \right) = ( - 1 ) ^ { k } \cos \left( m \theta _ { y _ { i } , i } \right) - 2 k</script><script type="math/tex; mode=display">\theta _ { y _ { i } , i } \in \left[ \frac { k \pi } { m } , \frac { ( k + 1 ) \pi } { m } \right] \text { and } k \in [ 0 , m - 1 ]</script><p><img alt="A-Softmax loss示意图" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k1mg2yrlj20hk0ca0v4.jpg" width="50%" height="50%"></p><hr><h4 id="AM-Softmax-loss"><a href="#AM-Softmax-loss" class="headerlink" title="AM-Softmax loss"></a>AM-Softmax loss</h4><p>相关文献：</p><ul><li><strong>《Additive margin softmax for face verification》</strong></li></ul><script type="math/tex; mode=display">\begin{aligned} \mathcal { L } _ { A M S } & = - \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \log \frac { e ^ { s \cdot \left( \cos \theta _ { y _ { i } } - m \right) } } { e ^ { s \cdot \left( \cos \theta _ { y _ { i } } - m \right) } + \sum _ { j = 1 , j \neq y _ { i } } ^ { c } e ^ { s \cdot c o s \theta _ { j } } } \\ & = - \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \log \frac { e ^ { s \cdot \left( W _ { y _ { i } } ^ { T } f _ { i } - m \right) } } { e ^ { s \cdot \left( W _ { y _ { i } } ^ { T } \boldsymbol { f } _ { i } - m \right) } + \sum _ { j = 1 , j \neq y _ { i } } ^ { c } e ^ { S W _ { j } ^ { T } \boldsymbol { f } _ { i } } } \end{aligned}</script><p><img alt="AM-Softmax loss示意图" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1mmsrz1j20n808dwfx.jpg" width="60%" height="60%"></p><hr><h4 id="CosFace"><a href="#CosFace" class="headerlink" title="CosFace"></a>CosFace</h4><p>相关文献：</p><ul><li><strong>《Cosface: Large margin cosine loss for deep face recognition》</strong></li></ul><script type="math/tex; mode=display">L _ { l m c } = \frac { 1 } { N } \sum _ { i } - \log \frac { e ^ { s \left( \cos \left( \theta _ { y _ { i } , i } \right) - m \right) } } { e ^ { s \left( \cos \left( \theta _ { y _ { i } } , i \right) - m \right) } + \sum _ { j \neq y _ { i } } e ^ { s \cos \left( \theta _ { j , i } \right) } }</script><script type="math/tex; mode=display">\begin{aligned} \text { subject to } \\  W & = \frac { W ^ { * } } { \left\| W ^ { * } \right\| } \\  x & = \frac { x ^ { * } } { \left\| x ^ { * } \right\| } \\ \cos \left( \theta _ { j } , i \right) & = W _ { j } ^ { T } x _ { i } \end{aligned}</script><p><img alt="CosFace示意图" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k1mtlmpmj20ck04sq3j.jpg" width="60%" height="60%"></p><hr><h4 id="ArcFace"><a href="#ArcFace" class="headerlink" title="ArcFace"></a>ArcFace</h4><p>相关文献：</p><ul><li><strong>《Arcface: Additive angular margin loss for deep face recognition》</strong></li></ul><script type="math/tex; mode=display">L  = - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \log \frac { e ^ { s \left( \cos \left( \theta _ { y _ { i } } + m \right) \right) } } { e ^ { s \left( \cos \left( \theta _ { y _ { i } } + m \right) \right) } + \sum _ { j = 1 , j \neq y _ { i } } ^ { n } e ^ { s \cos \theta _ { j } } }</script><p><img alt="ArcFace示意图" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k1mz40h8j21640ad0wf.jpg" width="100%" height="100%"></p><hr><h3 id="Softmax及其变种"><a href="#Softmax及其变种" class="headerlink" title="Softmax及其变种"></a>Softmax及其变种</h3><hr><h4 id="L2-Softmax"><a href="#L2-Softmax" class="headerlink" title="L2-Softmax"></a>L2-Softmax</h4><p>相关文献：</p><ul><li><strong>《L2-constrained softmax loss for discriminative face verification》</strong></li></ul><script type="math/tex; mode=display">\begin{array} { l l } { \text { minimize } } & { - \frac { 1 } { M } \sum _ { i = 1 } ^ { M } \log \frac { e ^ { W _ { y _ { i } } ^ { T } f \left( \mathbf { x } _ { i } \right) + b _ { y _ { i } } } } { \sum _ { j = 1 } ^ { C } e ^ { W _ { j } ^ { T } f \left( \mathbf { x } _ { i } \right) + b _ { j } } } } \\ { \text { subject to } } & { \left\| f \left( \mathbf { x } _ { i } \right) \right\| _ { 2 } = \alpha , \forall i = 1,2 , \ldots M } \end{array}</script><hr><h4 id="Normface"><a href="#Normface" class="headerlink" title="Normface"></a>Normface</h4><p>相关文献：</p><ul><li><strong>《NormFace: L2 Hypersphere Embedding for Face Verification》</strong></li></ul><script type="math/tex; mode=display">\mathcal { L } _ { S' }  = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \log \frac { e ^ { s \tilde { W } _ { y _ { i } } ^ { T } \tilde { \mathbf { f } } _ { i } } } { \sum _ { j = 1 } ^ { n } e ^ { s \tilde { W } _ { j } ^ { T } \mathbf { f } _ { i } } }</script><script type="math/tex; mode=display">\tilde { \mathbf { x } } = \frac { \mathbf { x } } { \| \mathbf { x } \| _ { 2 } } = \frac { \mathbf { x } } { \sqrt { \sum _ { i } \mathbf { x } _ { i } ^ { 2 } + \epsilon } }</script><hr><h4 id="CoCo-loss"><a href="#CoCo-loss" class="headerlink" title="CoCo loss"></a>CoCo loss</h4><p>相关文献：</p><ul><li><strong>《Rethinking feature discrimination and polymerization for large-scale recognition》</strong></li></ul><script type="math/tex; mode=display">\mathcal { L } ^ { C O C O } \left( \boldsymbol { f } ^ { ( i ) } , \boldsymbol { c } _ { k } \right) = - \sum _ { i \in \mathcal { B } , k } t _ { k } ^ { ( i ) } \log p _ { k } ^ { ( i ) } = - \sum _ { i \in \mathcal { B } } \log p _ { l _ { i } } ^ { ( i ) }</script><script type="math/tex; mode=display">\hat { \boldsymbol { c } } _ { k } = \frac { \boldsymbol { c } _ { k } } { \left\| \boldsymbol { c } _ { k } \right\| } , \hat { \boldsymbol { f } } ^ { ( i ) } = \frac { \alpha \boldsymbol { f } ^ { ( i ) } } { \left\| \boldsymbol { f } ^ { ( i ) } \right\| } , p _ { k } ^ { ( i ) } = \frac { \exp \left( \hat { \boldsymbol { c } } _ { k } ^ { T } \cdot \hat { \boldsymbol { f } } ^ { ( i ) } \right) } { \sum _ { m } \exp \left( \hat { \boldsymbol { c } } _ { m } ^ { T } \cdot \hat { \boldsymbol { f } } ^ { ( i ) } \right) }</script><hr><h4 id="Ring-loss"><a href="#Ring-loss" class="headerlink" title="Ring loss"></a>Ring loss</h4><p>相关文献：</p><ul><li><strong>《Ring loss: Convex feature normalization for face recognition》</strong></li></ul><script type="math/tex; mode=display">L _ { R } = \frac { \lambda } { 2 m } \sum _ { i = 1 } ^ { m } \left( \left\| \mathcal { F } \left( \mathbf { x } _ { i } \right) \right\| _ { 2 } - R \right) ^ { 2 }</script><p><img alt="Ring loss示意图" src="https://ws1.sinaimg.cn/large/8662e3cegy1g0k1n6hmq9j20lo09g0wz.jpg" width="60%" height="60%"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. <strong>Deepface: Closing the gap to human-level performance in face verification</strong>. In CVPR, pages 1701–1708, 2014.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. A. Krizhevsky, I. Sutskever, and G. E. Hinton. <strong>Imagenet classification with deep convolutional neural networks</strong>. In NIPS, pages 1097–1105, 2012.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. K. Simonyan and A. Zisserman. <strong>Very deep convolutional networks for large-scale image recognition</strong>. arXiv preprint arXiv:1409.1556, 2014.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, et al. <strong>Going deeper with convolutions</strong>. In CVPR, 2015.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. K. He, X. Zhang, S. Ren, and J. Sun. <strong>Deep residual learning for image recognition</strong>. In CVPR, pages 770–778, 2016.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. J. Hu, L. Shen, and G. Sun. <strong>Squeeze-and-excitation networks</strong>. arXiv preprint arXiv:1709.01507, 2017.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. X. Wu, R. He, Z. Sun, and T. Tan. <strong>A light cnn for deep face representation with noisy labels</strong>. arXiv preprint arXiv:1511.02683, 2015.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. A. R. Chowdhury, T.-Y. Lin, S. Maji, and E. Learned-Miller. <strong>One-to-many face recognition with bilinear cnns</strong>. In WACV, pages 1–9. IEEE, 2016.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. F. Schroff, D. Kalenichenko, and J. Philbin. <strong>Facenet: A unified embedding for face recognition and clustering</strong>. In CVPR, pages 815–823, 2015.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. O. M. Parkhi, A. Vedaldi, A. Zisserman, et al. <strong>Deep face recognition</strong>. In BMVC, volume 1, page 6, 2015.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. <strong>Sphereface: Deep hypersphere embedding for face recognition</strong>. In CVPR, volume 1, 2017.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. <strong>Vggface2: A dataset for recognising faces across pose and age</strong>. arXiv preprint arXiv:1710.08092, 2017.<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;&lt;strong&gt;论文题目&lt;/strong&gt;：《Deep Face Recognition: A Survey》&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文作者&lt;/strong&gt;：Mei Wang, Weihong Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文链接&lt;/strong&gt;：&lt;a href=&quot;http://cn.arxiv.org/pdf/1804.06655.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://cn.arxiv.org/pdf/1804.06655.pdf&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="https://orzyt.cn/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="https://orzyt.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="人脸识别" scheme="https://orzyt.cn/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>2018-2019学年研一上学期总结</title>
    <link href="https://orzyt.cn/posts/2018-2019-fall-semester-summary/"/>
    <id>https://orzyt.cn/posts/2018-2019-fall-semester-summary/</id>
    <published>2019-01-21T05:43:42.000Z</published>
    <updated>2019-01-22T06:49:10.651Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文暂未公开，请输入密码访问" />    <label for="pass">本文暂未公开，请输入密码访问</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">∑(っ°Д°;)っ 密码错误！</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;本文暂未公开，请输入密码访问
    
    </summary>
    
      <category term="随笔" scheme="https://orzyt.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="研究生学习" scheme="https://orzyt.cn/tags/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅谈支持向量机的推导</title>
    <link href="https://orzyt.cn/posts/svm/"/>
    <id>https://orzyt.cn/posts/svm/</id>
    <published>2018-12-19T16:00:00.000Z</published>
    <updated>2019-02-28T11:38:06.625Z</updated>
    
    <content type="html"><![CDATA[<hr><iframe src="/svm.html" scrolling="no" width="100%" height="11100px"></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;iframe src=&quot;/svm.html&quot; scrolling=&quot;no&quot; width=&quot;100%&quot; height=&quot;11100px&quot;&gt;&lt;/iframe&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://orzyt.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="支持向量机" scheme="https://orzyt.cn/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
      <category term="SVM" scheme="https://orzyt.cn/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode575 Distribute Candies</title>
    <link href="https://orzyt.cn/posts/leetcode575-distribute-candies/"/>
    <id>https://orzyt.cn/posts/leetcode575-distribute-candies/</id>
    <published>2018-01-14T05:21:28.000Z</published>
    <updated>2019-02-08T10:26:00.494Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given an integer array with <strong>even</strong> length, where different numbers in this array represent different <strong>kinds</strong> of candies. Each number means one candy of the corresponding kind. You need to distribute these candies <strong>equally</strong> in number to brother and sister. Return the maximum number of <strong>kinds</strong> of candies the sister could gain.</p><p><strong>Note:</strong></p><ol><li>The length of the given array is in range [2, 10,000], and will be even.</li><li>The number in given array is in range [-100,000, 100,000].</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input: candies = [1,1,2,2,3,3]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation:</span><br><span class="line">There are three different kinds of candies (1, 2 and 3), and two candies for each kind.</span><br><span class="line">Optimal distribution: The sister has candies [1,2,3] and the brother has candies [1,2,3], too. </span><br><span class="line">The sister has three different kinds of candies.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: candies = [1,1,2,3]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: For example, the sister has candies [2,3] and the brother has candies [1,1]. </span><br><span class="line">The sister has two different kinds of candies, the brother has only one kind of candies.</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>有偶数个不同种类的糖果，将其平均分给两个人，问某人能够得到最多的种类数是多少</p><p>首先，用哈希表记录种类数，这是答案的上限，而一个人只能获得一半的糖果，所以这又是一个上限。</p><p>最终的答案为二者取最小值。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">distributeCandies</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candies)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; count;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> x: candies) count[x]++;</span><br><span class="line">        <span class="keyword">return</span> min(count.size(), candies.size() / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given an integer array with &lt;strong&gt;even&lt;/strong&gt; length, where differ
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="哈希表" scheme="https://orzyt.cn/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode566 Reshape the Matrix</title>
    <link href="https://orzyt.cn/posts/leetcode566-reshape-the-matrix/"/>
    <id>https://orzyt.cn/posts/leetcode566-reshape-the-matrix/</id>
    <published>2018-01-14T05:03:19.000Z</published>
    <updated>2019-02-08T10:26:00.386Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>In MATLAB, there is a very useful function called ‘reshape’, which can reshape a matrix into a new one with different size but keep its original data.</p><p>You’re given a matrix represented by a two-dimensional array, and two <strong>positive</strong> integers <strong>r</strong> and <strong>c</strong> representing the <strong>row</strong> number and <strong>column</strong> number of the wanted reshaped matrix, respectively.</p><p>The reshaped matrix need to be filled with all the elements of the original matrix in the same <strong>row-traversing</strong> order as they were.</p><p>If the ‘reshape’ operation with given parameters is possible and legal, output the new reshaped matrix; Otherwise, output the original matrix.</p><p><strong>Note:</strong></p><ol><li>The height and width of the given matrix is in range [1, 100].</li><li>The given r and c are all positive.</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">nums = </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">r = 1, c = 4</span><br><span class="line">Output: </span><br><span class="line">[[1,2,3,4]]</span><br><span class="line">Explanation:</span><br><span class="line">The row-traversing of nums is [1,2,3,4]. The new reshaped matrix is a 1 * 4 matrix, fill it row by row by using the previous list.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">nums = </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">r = 2, c = 4</span><br><span class="line">Output: </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">Explanation:</span><br><span class="line">There is no way to reshape a 2 * 2 matrix to a 2 * 4 matrix. So output the original matrix.</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>给定一个二维数组，模拟 MATLAB 中 <code>reshape</code> 函数的操作，若无法完成，则输出原数组</p><p>题中要求元素以<code>row-traversing</code>顺序访问，则$r$行$n$列的二维数组第$i$个访问到的元素所在的位置为($i / c$, $i \% c$)</p><p>利用这一关系，可以得到从原数组$nums$（$n$行$m$列）<code>reshape</code>成 新数组$vec$（$r$行$c$列）后的位置关系，$vec[i / c][i \% c] = nums[i / m][i \% m]$</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; matrixReshape(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; nums, <span class="keyword">int</span> r, <span class="keyword">int</span> c) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size(), m = nums[<span class="number">0</span>].size();</span><br><span class="line">        <span class="comment">// 元素数量不匹配，reshape失败</span></span><br><span class="line">        <span class="keyword">if</span> (n * m != r * c) <span class="keyword">return</span> nums;</span><br><span class="line">        <span class="comment">// 初始化二维vector</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; vec(r, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(c));</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; r * c; ++i) vec[i / c][i % c] = nums[i / m][i % m];</span><br><span class="line">        <span class="keyword">return</span> vec;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;In MATLAB, there is a very useful function called ‘reshape’, which can
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="数组" scheme="https://orzyt.cn/tags/%E6%95%B0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode563 Binary Tree Tilt</title>
    <link href="https://orzyt.cn/posts/leetcode563-binary-tree-tilt/"/>
    <id>https://orzyt.cn/posts/leetcode563-binary-tree-tilt/</id>
    <published>2018-01-14T04:51:07.000Z</published>
    <updated>2019-02-08T10:26:00.486Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given a binary tree, return the tilt of the <strong>whole tree</strong>.</p><p>The tilt of a <strong>tree node</strong> is defined as the <strong>absolute difference</strong> between the sum of all left subtree node values and the sum of all right subtree node values. Null node has tilt 0.</p><p>The tilt of the <strong>whole tree</strong> is defined as the sum of all nodes’ tilt.</p><a id="more"></a><p><strong>Note:</strong></p><ol><li>The sum of node values in any subtree won’t exceed the range of 32-bit integer.</li><li>All the tilt values won’t exceed the range of 32-bit integer.</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">         1</span><br><span class="line">       /   \</span><br><span class="line">      2     3</span><br><span class="line">Output: 1</span><br><span class="line">Explanation: </span><br><span class="line">Tilt of node 2 : 0</span><br><span class="line">Tilt of node 3 : 0</span><br><span class="line">Tilt of node 1 : |2-3| = 1</span><br><span class="line">Tilt of binary tree : 0 + 0 + 1 = 1</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>求二叉树的倾斜度。</p><p>一个节点的倾斜度是指：该节点 <strong>左子树所有节点值之和</strong> 与 <strong>右子树所有节点值之和</strong> 的 <code>绝对差值</code></p><p>一棵树的倾斜度是指：该棵树所有节点的倾斜度之和</p><p>对二叉树dfs一遍即可求出答案</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 以root为根的子树所有节点值之和</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> lsum = dfs(root-&gt;left), rsum = dfs(root-&gt;right);</span><br><span class="line">        <span class="comment">// 添加节点root的倾斜度</span></span><br><span class="line">        ans += <span class="built_in">abs</span>(lsum - rsum);</span><br><span class="line">        <span class="keyword">return</span> lsum + rsum + root-&gt;val;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findTilt</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        dfs(root);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given a binary tree, return the tilt of the &lt;strong&gt;whole tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The tilt of a &lt;strong&gt;tree node&lt;/strong&gt; is defined as the &lt;strong&gt;absolute difference&lt;/strong&gt; between the sum of all left subtree node values and the sum of all right subtree node values. Null node has tilt 0.&lt;/p&gt;
&lt;p&gt;The tilt of the &lt;strong&gt;whole tree&lt;/strong&gt; is defined as the sum of all nodes’ tilt.&lt;/p&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="DFS" scheme="https://orzyt.cn/tags/DFS/"/>
    
      <category term="二叉树" scheme="https://orzyt.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode561 Array Partition I</title>
    <link href="https://orzyt.cn/posts/leetcode561-array-partition-i/"/>
    <id>https://orzyt.cn/posts/leetcode561-array-partition-i/</id>
    <published>2018-01-14T04:31:32.000Z</published>
    <updated>2019-02-08T10:26:00.398Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given an array of <strong>2n</strong> integers, your task is to group these integers into <strong>n</strong> pairs of integer, say (a1, b1), (a2, b2), …, (an, bn) which makes sum of min(ai, bi) for all i from 1 to n as large as possible.</p><p><strong>Note:</strong></p><ol><li><strong>n</strong> is a positive integer, which is in the range of [1, 10000].</li><li>All the integers in the array will be in the range of [-10000, 10000].</li></ol><a id="more"></a><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [1,4,3,2]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: n is 2, and the maximum sum of pairs is 4 = min(1, 2) + min(3, 4).</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>将$2n$个元素两两分组($a_1$, $b_1$), ($a_2$, $b_2$), …, ($a_n$, $b_n$) ，使得这$n$个分组中最小值之和最大。</p><p>贪心题，将数组元素从小到大排序，然后相邻的两个元素分为一组。</p><p>可以这样考虑，假设元素$a_1$是数组中最小的元素，那么和$a_1$同一组的元素对答案是没有贡献的，因此，应该找到剩下的元素中值最小的和$a_1$匹配。以此类推，可以得出贪心的策略。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">arrayPairSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        sort(nums.begin(), nums.end());</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i += <span class="number">2</span>) ans += nums[i];</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given an array of &lt;strong&gt;2n&lt;/strong&gt; integers, your task is to group these integers into &lt;strong&gt;n&lt;/strong&gt; pairs of integer, say (a1, b1), (a2, b2), …, (an, bn) which makes sum of min(ai, bi) for all i from 1 to n as large as possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;n&lt;/strong&gt; is a positive integer, which is in the range of [1, 10000].&lt;/li&gt;
&lt;li&gt;All the integers in the array will be in the range of [-10000, 10000].&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="贪心" scheme="https://orzyt.cn/tags/%E8%B4%AA%E5%BF%83/"/>
    
      <category term="greedy" scheme="https://orzyt.cn/tags/greedy/"/>
    
  </entry>
  
</feed>
