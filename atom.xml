<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>扬涛的博客</title>
  
  <subtitle>上善若水·大道至简</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://orzyt.cn/"/>
  <updated>2019-01-21T05:52:30.513Z</updated>
  <id>https://orzyt.cn/</id>
  
  <author>
    <name>orzyt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2018-2019研一上学期总结</title>
    <link href="https://orzyt.cn/posts/2018-2019-fall-semester-summary/"/>
    <id>https://orzyt.cn/posts/2018-2019-fall-semester-summary/</id>
    <published>2019-01-21T05:43:42.000Z</published>
    <updated>2019-01-21T05:52:30.513Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文暂未公开，请输入密码访问" />    <label for="pass">本文暂未公开，请输入密码访问</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">∑(っ°Д°;)っ 密码错误！</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;本文暂未公开，请输入密码访问
    
    </summary>
    
      <category term="随笔" scheme="https://orzyt.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="研究生学习" scheme="https://orzyt.cn/tags/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode575 Distribute Candies</title>
    <link href="https://orzyt.cn/posts/LeetCode575-Distribute-Candies/"/>
    <id>https://orzyt.cn/posts/LeetCode575-Distribute-Candies/</id>
    <published>2018-01-14T05:21:28.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given an integer array with <strong>even</strong> length, where different numbers in this array represent different <strong>kinds</strong> of candies. Each number means one candy of the corresponding kind. You need to distribute these candies <strong>equally</strong> in number to brother and sister. Return the maximum number of <strong>kinds</strong> of candies the sister could gain.</p><p><strong>Note:</strong></p><ol><li>The length of the given array is in range [2, 10,000], and will be even.</li><li>The number in given array is in range [-100,000, 100,000].</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input: candies = [1,1,2,2,3,3]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation:</span><br><span class="line">There are three different kinds of candies (1, 2 and 3), and two candies for each kind.</span><br><span class="line">Optimal distribution: The sister has candies [1,2,3] and the brother has candies [1,2,3], too. </span><br><span class="line">The sister has three different kinds of candies.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: candies = [1,1,2,3]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: For example, the sister has candies [2,3] and the brother has candies [1,1]. </span><br><span class="line">The sister has two different kinds of candies, the brother has only one kind of candies.</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>有偶数个不同种类的糖果，将其平均分给两个人，问某人能够得到最多的种类数是多少</p><p>首先，用哈希表记录种类数，这是答案的上限，而一个人只能获得一半的糖果，所以这又是一个上限。</p><p>最终的答案为二者取最小值。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">distributeCandies</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; candies)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; count;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> x: candies) count[x]++;</span><br><span class="line">        <span class="keyword">return</span> min(count.size(), candies.size() / <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given an integer array with &lt;strong&gt;even&lt;/strong&gt; length, where differ
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="哈希表" scheme="https://orzyt.cn/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode566 Reshape the Matrix</title>
    <link href="https://orzyt.cn/posts/LeetCode566-Reshape-the-Matrix/"/>
    <id>https://orzyt.cn/posts/LeetCode566-Reshape-the-Matrix/</id>
    <published>2018-01-14T05:03:19.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>In MATLAB, there is a very useful function called ‘reshape’, which can reshape a matrix into a new one with different size but keep its original data.</p><p>You’re given a matrix represented by a two-dimensional array, and two <strong>positive</strong> integers <strong>r</strong> and <strong>c</strong> representing the <strong>row</strong> number and <strong>column</strong> number of the wanted reshaped matrix, respectively.</p><p>The reshaped matrix need to be filled with all the elements of the original matrix in the same <strong>row-traversing</strong> order as they were.</p><p>If the ‘reshape’ operation with given parameters is possible and legal, output the new reshaped matrix; Otherwise, output the original matrix.</p><p><strong>Note:</strong></p><ol><li>The height and width of the given matrix is in range [1, 100].</li><li>The given r and c are all positive.</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">nums = </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">r = 1, c = 4</span><br><span class="line">Output: </span><br><span class="line">[[1,2,3,4]]</span><br><span class="line">Explanation:</span><br><span class="line">The row-traversing of nums is [1,2,3,4]. The new reshaped matrix is a 1 * 4 matrix, fill it row by row by using the previous list.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">nums = </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">r = 2, c = 4</span><br><span class="line">Output: </span><br><span class="line">[[1,2],</span><br><span class="line"> [3,4]]</span><br><span class="line">Explanation:</span><br><span class="line">There is no way to reshape a 2 * 2 matrix to a 2 * 4 matrix. So output the original matrix.</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>给定一个二维数组，模拟 MATLAB 中 <code>reshape</code> 函数的操作，若无法完成，则输出原数组</p><p>题中要求元素以<code>row-traversing</code>顺序访问，则$r$行$n$列的二维数组第$i$个访问到的元素所在的位置为($i / c$, $i \% c$)</p><p>利用这一关系，可以得到从原数组$nums$（$n$行$m$列）<code>reshape</code>成 新数组$vec$（$r$行$c$列）后的位置关系，$vec[i / c][i \% c] = nums[i / m][i \% m]$</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; matrixReshape(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; nums, <span class="keyword">int</span> r, <span class="keyword">int</span> c) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size(), m = nums[<span class="number">0</span>].size();</span><br><span class="line">        <span class="comment">// 元素数量不匹配，reshape失败</span></span><br><span class="line">        <span class="keyword">if</span> (n * m != r * c) <span class="keyword">return</span> nums;</span><br><span class="line">        <span class="comment">// 初始化二维vector</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; vec(r, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(c));</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; r * c; ++i) vec[i / c][i % c] = nums[i / m][i % m];</span><br><span class="line">        <span class="keyword">return</span> vec;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;In MATLAB, there is a very useful function called ‘reshape’, which can
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="数组" scheme="https://orzyt.cn/tags/%E6%95%B0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode563 Binary Tree Tilt</title>
    <link href="https://orzyt.cn/posts/LeetCode563-Binary-Tree-Tilt/"/>
    <id>https://orzyt.cn/posts/LeetCode563-Binary-Tree-Tilt/</id>
    <published>2018-01-14T04:51:07.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given a binary tree, return the tilt of the <strong>whole tree</strong>.</p><p>The tilt of a <strong>tree node</strong> is defined as the <strong>absolute difference</strong> between the sum of all left subtree node values and the sum of all right subtree node values. Null node has tilt 0.</p><p>The tilt of the <strong>whole tree</strong> is defined as the sum of all nodes’ tilt.</p><a id="more"></a><p><strong>Note:</strong></p><ol><li>The sum of node values in any subtree won’t exceed the range of 32-bit integer.</li><li>All the tilt values won’t exceed the range of 32-bit integer.</li></ol><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">         1</span><br><span class="line">       /   \</span><br><span class="line">      2     3</span><br><span class="line">Output: 1</span><br><span class="line">Explanation: </span><br><span class="line">Tilt of node 2 : 0</span><br><span class="line">Tilt of node 3 : 0</span><br><span class="line">Tilt of node 1 : |2-3| = 1</span><br><span class="line">Tilt of binary tree : 0 + 0 + 1 = 1</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>求二叉树的倾斜度。</p><p>一个节点的倾斜度是指：该节点 <strong>左子树所有节点值之和</strong> 与 <strong>右子树所有节点值之和</strong> 的 <code>绝对差值</code></p><p>一棵树的倾斜度是指：该棵树所有节点的倾斜度之和</p><p>对二叉树dfs一遍即可求出答案</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 以root为根的子树所有节点值之和</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> lsum = dfs(root-&gt;left), rsum = dfs(root-&gt;right);</span><br><span class="line">        <span class="comment">// 添加节点root的倾斜度</span></span><br><span class="line">        ans += <span class="built_in">abs</span>(lsum - rsum);</span><br><span class="line">        <span class="keyword">return</span> lsum + rsum + root-&gt;val;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findTilt</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        dfs(root);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given a binary tree, return the tilt of the &lt;strong&gt;whole tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The tilt of a &lt;strong&gt;tree node&lt;/strong&gt; is defined as the &lt;strong&gt;absolute difference&lt;/strong&gt; between the sum of all left subtree node values and the sum of all right subtree node values. Null node has tilt 0.&lt;/p&gt;
&lt;p&gt;The tilt of the &lt;strong&gt;whole tree&lt;/strong&gt; is defined as the sum of all nodes’ tilt.&lt;/p&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="DFS" scheme="https://orzyt.cn/tags/DFS/"/>
    
      <category term="二叉树" scheme="https://orzyt.cn/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode561 Array Partition I</title>
    <link href="https://orzyt.cn/posts/LeetCode561-Array-Partition-I/"/>
    <id>https://orzyt.cn/posts/LeetCode561-Array-Partition-I/</id>
    <published>2018-01-14T04:31:32.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given an array of <strong>2n</strong> integers, your task is to group these integers into <strong>n</strong> pairs of integer, say (a1, b1), (a2, b2), …, (an, bn) which makes sum of min(ai, bi) for all i from 1 to n as large as possible.</p><p><strong>Note:</strong></p><ol><li><strong>n</strong> is a positive integer, which is in the range of [1, 10000].</li><li>All the integers in the array will be in the range of [-10000, 10000].</li></ol><a id="more"></a><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [1,4,3,2]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: n is 2, and the maximum sum of pairs is 4 = min(1, 2) + min(3, 4).</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>将$2n$个元素两两分组($a_1$, $b_1$), ($a_2$, $b_2$), …, ($a_n$, $b_n$) ，使得这$n$个分组中最小值之和最大。</p><p>贪心题，将数组元素从小到大排序，然后相邻的两个元素分为一组。</p><p>可以这样考虑，假设元素$a_1$是数组中最小的元素，那么和$a_1$同一组的元素对答案是没有贡献的，因此，应该找到剩下的元素中值最小的和$a_1$匹配。以此类推，可以得出贪心的策略。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">arrayPairSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        sort(nums.begin(), nums.end());</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i += <span class="number">2</span>) ans += nums[i];</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given an array of &lt;strong&gt;2n&lt;/strong&gt; integers, your task is to group these integers into &lt;strong&gt;n&lt;/strong&gt; pairs of integer, say (a1, b1), (a2, b2), …, (an, bn) which makes sum of min(ai, bi) for all i from 1 to n as large as possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;n&lt;/strong&gt; is a positive integer, which is in the range of [1, 10000].&lt;/li&gt;
&lt;li&gt;All the integers in the array will be in the range of [-10000, 10000].&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="贪心" scheme="https://orzyt.cn/tags/%E8%B4%AA%E5%BF%83/"/>
    
      <category term="greedy" scheme="https://orzyt.cn/tags/greedy/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode557 Reverse Words in a String III</title>
    <link href="https://orzyt.cn/posts/LeetCode557-Reverse-Words-in-a-String-III/"/>
    <id>https://orzyt.cn/posts/LeetCode557-Reverse-Words-in-a-String-III/</id>
    <published>2018-01-14T04:17:22.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given a string, you need to reverse the order of characters in each word within a sentence while still preserving whitespace and initial word order.</p><p><strong>Note:</strong> In the string, each word is separated by single space and there will not be any extra space in the string.</p><a id="more"></a><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: </span><br><span class="line">&quot;Let&apos;s take LeetCode contest&quot;</span><br><span class="line">Output: </span><br><span class="line">&quot;s&apos;teL ekat edoCteeL tsetnoc&quot;</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>将句子中的单词翻转（单词之间以空格隔开）</p><p>模拟一下，记录每个单词的起始位置和长度，然后翻转即可</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 字符串翻转函数</span></span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">rev</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> l = <span class="number">0</span>, r = s.size() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) swap(s[l++], s[r--]);</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">reverseWords</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">string</span> str;</span><br><span class="line">        <span class="comment">// 变量p记录单词起始位置</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, p = <span class="number">0</span>; i &lt; s.size(); ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s[i] == <span class="string">' '</span>) &#123;</span><br><span class="line">                str += rev(s.substr(p, i - p)) + <span class="string">' '</span>;</span><br><span class="line">                p = i + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 特判字符串末尾</span></span><br><span class="line">            <span class="keyword">if</span> (i == s.size() - <span class="number">1</span>) str += rev(s.substr(p));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> str;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given a string, you need to reverse the order of characters in each word within a sentence while still preserving whitespace and initial word order.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In the string, each word is separated by single space and there will not be any extra space in the string.&lt;/p&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="字符串" scheme="https://orzyt.cn/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    
  </entry>
  
  <entry>
    <title>ssh本地端口转发的应用</title>
    <link href="https://orzyt.cn/posts/ssh-port-forwarding/"/>
    <id>https://orzyt.cn/posts/ssh-port-forwarding/</id>
    <published>2018-01-13T07:34:17.000Z</published>
    <updated>2018-11-10T11:47:16.931Z</updated>
    
    <content type="html"><![CDATA[<hr><p>SSH有三种端口转发模式，<strong>本地端口转发(Local Port Forwarding)</strong>，<strong>远程端口转发(Remote Port Forwarding)</strong>以及<strong>动态端口转发(Dynamic Port Forwarding)</strong>。本文只简单介绍<strong>本地端口转发</strong>，用于实现本机访问远程服务器上的<code>jupyter notebook</code>、<code>TensorBoard</code>等服务。</p><h2 id="什么是本地端口转发？"><a href="#什么是本地端口转发？" class="headerlink" title="什么是本地端口转发？"></a>什么是本地端口转发？</h2><p>所谓本地端口转发，就是<strong>将发送到本地端口的请求，转发到目标端口</strong>。这样，就可以通过访问本地端口，来访问目标端口的服务。</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -h</span><br><span class="line">unknown option -- h</span><br><span class="line">usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]</span><br><span class="line">           [-D [bind_address:]port] [-E log_file] [-e escape_char]</span><br><span class="line">           [-F configfile] [-I pkcs11] [-i identity_file] [-L address]</span><br><span class="line">           [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port]</span><br><span class="line">           [-Q query_option] [-R address] [-S ctl_path] [-W host:port]</span><br><span class="line">           [-w local_tun[:remote_tun]] [user@]hostname [<span class="built_in">command</span>]</span><br></pre></td></tr></table></figure><p>需要用到的命令是<code>ssh -L address user@hostname</code> </p><p>其中，address的具体语法为 <code>[bind_address:]port:host:hostport</code> ，即 <strong>[本地主机地址:]本地端口:目标地址:目标端口</strong></p><h2 id="应用场景-—-以-jupyter-notebook-为例"><a href="#应用场景-—-以-jupyter-notebook-为例" class="headerlink" title="应用场景 — 以 jupyter notebook 为例"></a>应用场景 — 以 jupyter notebook 为例</h2><p>由于服务器上一般是没有安装桌面的，所以像<code>jupyter notebook</code>、<code>TensorBoard</code>等服务是无法直接通过服务器上的浏览器来访问。因此，我们需要采取ssh的本地端口转发方式，从而通过访问本地端口，来访问服务器上目标端口的服务。</p><hr><p>首先，在服务器上运行<code>jupyter notebook</code></p><p>会发现有一个黄色的 <strong>warning</strong>: <em>No web browser found: could not locate runnable browser.</em> (说明服务器上是无法打开的)</p><p>然后，我们记下<strong>端口号</strong>（8008）以及 <strong>token</strong>（链接中?token=后面一长串的字符，用于登录认证）</p><p>接着，执行以下命令进行本地端口转发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f: 后台执行命令</span></span><br><span class="line"><span class="comment"># N: 不进行实际连接，而仅做端口转发</span></span><br><span class="line"><span class="comment"># 本地主机地址可省略，本地端口号随意填，远程端口号为上述的8008</span></span><br><span class="line">ssh -fNL 本地端口号:localhost:远程端口号 username@serverAddress</span><br></pre></td></tr></table></figure><p>最后，在本机打开地址<code>localhost:本地端口号</code> 输入token后即可访问服务器上的 <code>jupyter notebook</code> </p><p><img src="https://tuchuang001.com/images/2018/01/13/jupyter.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;SSH有三种端口转发模式，&lt;strong&gt;本地端口转发(Local Port Forwarding)&lt;/strong&gt;，&lt;strong&gt;远程端口转发(Remote Port Forwarding)&lt;/strong&gt;以及&lt;strong&gt;动态端口转发(Dynamic Port Forwarding)&lt;/strong&gt;。本文只简单介绍&lt;strong&gt;本地端口转发&lt;/strong&gt;，用于实现本机访问远程服务器上的&lt;code&gt;jupyter notebook&lt;/code&gt;、&lt;code&gt;TensorBoard&lt;/code&gt;等服务。&lt;/p&gt;
&lt;h2 id=&quot;什么是本地端口转发？&quot;&gt;&lt;a href=&quot;#什么是本地端口转发？&quot; class=&quot;headerlink&quot; title=&quot;什么是本地端口转发？&quot;&gt;&lt;/a&gt;什么是本地端口转发？&lt;/h2&gt;&lt;p&gt;所谓本地端口转发，就是&lt;strong&gt;将发送到本地端口的请求，转发到目标端口&lt;/strong&gt;。这样，就可以通过访问本地端口，来访问目标端口的服务。&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="https://orzyt.cn/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="ssh" scheme="https://orzyt.cn/tags/ssh/"/>
    
      <category term="端口转发" scheme="https://orzyt.cn/tags/%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/"/>
    
      <category term="jupyter notebook" scheme="https://orzyt.cn/tags/jupyter-notebook/"/>
    
      <category term="tensorboard" scheme="https://orzyt.cn/tags/tensorboard/"/>
    
  </entry>
  
  <entry>
    <title>简化ssh连接服务器流程</title>
    <link href="https://orzyt.cn/posts/ssh-login/"/>
    <id>https://orzyt.cn/posts/ssh-login/</id>
    <published>2018-01-13T03:02:26.000Z</published>
    <updated>2018-11-10T11:47:16.931Z</updated>
    
    <content type="html"><![CDATA[<hr><p>登录远程服务器一般采用<code>ssh</code>（Secure Shell）的方式，为了避免每次登录时手动输入用户名、密码、服务器地址等信息，故进行以下配置来达到简化登录流程。</p><h2 id="生成-ssh-key"><a href="#生成-ssh-key" class="headerlink" title="生成 ssh key"></a>生成 ssh key</h2><p>在本机的终端中执行命令 <code>ssh-keygen</code> ，然后根据提示操作即可在目录（默认为<code>~/.ssh</code>）中生成<code>id_rsa</code>（私钥） 和 <code>id_rsa.pub</code>（公钥）文件。</p><a id="more"></a><h2 id="添加-ssh-config-文件"><a href="#添加-ssh-config-文件" class="headerlink" title="添加 ssh config 文件"></a>添加 ssh config 文件</h2><p>根据下列内容，修改（或新建）本机中的<code>~/.ssh/config</code> 文件 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Host fastlogin                 # 随便取个名字，用于登录</span><br><span class="line">    HostName xxx.xxx.xxx.xxx   # 主机地址 </span><br><span class="line">    User username              # 用户名</span><br><span class="line">    Port 1234                  # 端口号</span><br></pre></td></tr></table></figure><h2 id="向服务器中添加公钥"><a href="#向服务器中添加公钥" class="headerlink" title="向服务器中添加公钥"></a>向服务器中添加公钥</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [此命令在本机中执行]：将公钥文件上传到服务器的用户目录</span></span><br><span class="line">scp ~/.ssh/id_rsa.pub username@xxx.xxx.xxx.xxx:~/</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [此命令在服务器中执行]：将公钥文件添加进服务器的ssh信任列表</span></span><br><span class="line">cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h2 id="快速登录"><a href="#快速登录" class="headerlink" title="快速登录"></a>快速登录</h2><p>配置好之后，只需执行下列命令即可快速登录服务器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fastlogin 为之前在ssh config中定义的名字</span></span><br><span class="line">ssh fastlogin</span><br></pre></td></tr></table></figure><h2 id="scp-远程拷贝文件"><a href="#scp-远程拷贝文件" class="headerlink" title="scp 远程拷贝文件"></a>scp 远程拷贝文件</h2><p>现在，如果想要复制本机的文件到服务器上，可以更加简便</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp 本机源地址 fastlogin:服务器目标地址</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;登录远程服务器一般采用&lt;code&gt;ssh&lt;/code&gt;（Secure Shell）的方式，为了避免每次登录时手动输入用户名、密码、服务器地址等信息，故进行以下配置来达到简化登录流程。&lt;/p&gt;
&lt;h2 id=&quot;生成-ssh-key&quot;&gt;&lt;a href=&quot;#生成-ssh-key&quot; class=&quot;headerlink&quot; title=&quot;生成 ssh key&quot;&gt;&lt;/a&gt;生成 ssh key&lt;/h2&gt;&lt;p&gt;在本机的终端中执行命令 &lt;code&gt;ssh-keygen&lt;/code&gt; ，然后根据提示操作即可在目录（默认为&lt;code&gt;~/.ssh&lt;/code&gt;）中生成&lt;code&gt;id_rsa&lt;/code&gt;（私钥） 和 &lt;code&gt;id_rsa.pub&lt;/code&gt;（公钥）文件。&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="https://orzyt.cn/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="ssh" scheme="https://orzyt.cn/tags/ssh/"/>
    
      <category term="scp" scheme="https://orzyt.cn/tags/scp/"/>
    
  </entry>
  
  <entry>
    <title>常用软件更换国内镜像源</title>
    <link href="https://orzyt.cn/posts/change-mirrors/"/>
    <id>https://orzyt.cn/posts/change-mirrors/</id>
    <published>2018-01-13T02:45:51.000Z</published>
    <updated>2018-11-10T11:47:16.927Z</updated>
    
    <content type="html"><![CDATA[<hr><p>由于某些原因，国内访问一些国外的软件仓库时比较慢。为了提高下载速度，通常可以更换相应的国内镜像源。</p><p>以下基于<code>Ubuntu</code>系统介绍相应的换源方式。</p><h2 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h2><p>修改（或新建）文件：<code>~/.pip/pip.conf</code>，添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="npm"><a href="#npm" class="headerlink" title="npm"></a>npm</h2><p>在终端中运行下列命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure><h2 id="anaconda"><a href="#anaconda" class="headerlink" title="anaconda"></a>anaconda</h2><p>在终端中运行下列命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;由于某些原因，国内访问一些国外的软件仓库时比较慢。为了提高下载速度，通常可以更换相应的国内镜像源。&lt;/p&gt;
&lt;p&gt;以下基于&lt;code&gt;Ubuntu&lt;/code&gt;系统介绍相应的换源方式。&lt;/p&gt;
&lt;h2 id=&quot;pip&quot;&gt;&lt;a href=&quot;#pip&quot; class=&quot;headerlink&quot; title=&quot;pip&quot;&gt;&lt;/a&gt;pip&lt;/h2&gt;&lt;p&gt;修改（或新建）文件：&lt;code&gt;~/.pip/pip.conf&lt;/code&gt;，添加以下内容：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[global]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;index-url = https://pypi.tuna.tsinghua.edu.cn/simple&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="教程" scheme="https://orzyt.cn/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="pip" scheme="https://orzyt.cn/tags/pip/"/>
    
      <category term="npm" scheme="https://orzyt.cn/tags/npm/"/>
    
      <category term="anaconda" scheme="https://orzyt.cn/tags/anaconda/"/>
    
      <category term="镜像源" scheme="https://orzyt.cn/tags/%E9%95%9C%E5%83%8F%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode543 Diameter of Binary Tree</title>
    <link href="https://orzyt.cn/posts/LeetCode543-Diameter-of-Binary-Tree/"/>
    <id>https://orzyt.cn/posts/LeetCode543-Diameter-of-Binary-Tree/</id>
    <published>2018-01-09T05:14:50.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given a binary tree, you need to compute the length of the diameter of the tree. The diameter of a binary tree is the length of the <strong>longest</strong> path between any two nodes in a tree. This path may or may not pass through the root.</p><p><strong>Note:</strong> The length of path between two nodes is represented by the number of edges between them.</p><a id="more"></a><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">Given a binary tree </span><br><span class="line">          1</span><br><span class="line">         / \</span><br><span class="line">        2   3</span><br><span class="line">       / \     </span><br><span class="line">      4   5    </span><br><span class="line">      </span><br><span class="line">Output:</span><br><span class="line">Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3].</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>求二叉树的直径</p><p>普通树的直径需要<a href="https://www.cnblogs.com/wuyiqi/archive/2012/04/08/2437424.html" target="_blank" rel="noopener">两遍DFS</a>，而二叉树因为结构特殊只需一次DFS即可</p><p>假设二叉树中共有$n$个节点，第$i$个节点的左子树树高为$l_i$，右子树树高为$r_i$，则以第$i$个节点为子树的直径$d_i = l_i + r_i  $，最终整棵二叉树的直径$D = max\{d_i | i=1..n\}$</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> ans;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">dfs</span><span class="params">(TreeNode* u, <span class="keyword">int</span> dep)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (u == <span class="literal">NULL</span>) <span class="keyword">return</span> dep - <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 递归求解左子树深度及右子树深度</span></span><br><span class="line">        <span class="keyword">int</span> l = dfs(u-&gt;left, dep + <span class="number">1</span>), r = dfs(u-&gt;right, dep + <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 因为当前深度是相对于root来说的，如果相对于u来说，其左右子树树高需要减去u的深度</span></span><br><span class="line">        <span class="comment">// 即 d = l - dep + r - dep = l + r - 2 * dep</span></span><br><span class="line">        ans = max(ans, l + r - <span class="number">2</span> * dep);</span><br><span class="line">        <span class="keyword">return</span> max(l, r);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">diameterOfBinaryTree</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        ans = <span class="number">0</span>;</span><br><span class="line">        dfs(root, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given a binary tree, you need to compute the length of the diameter of the tree. The diameter of a binary tree is the length of the &lt;strong&gt;longest&lt;/strong&gt; path between any two nodes in a tree. This path may or may not pass through the root.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The length of path between two nodes is represented by the number of edges between them.&lt;/p&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode541 Reverse String II</title>
    <link href="https://orzyt.cn/posts/LeetCode541-Reverse-String-II/"/>
    <id>https://orzyt.cn/posts/LeetCode541-Reverse-String-II/</id>
    <published>2018-01-09T05:04:49.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given a string and an integer k, you need to reverse the first k characters for every 2k characters counting from the start of the string. If there are less than k characters left, reverse all of them. If there are less than 2k but greater than or equal to k characters, then reverse the first k characters and left the other as original.</p><p><strong>Restrictions:</strong></p><ol><li>The string consists of lower English letters only.</li><li>Length of the given string and k will in the range [1, 10000]</li></ol><a id="more"></a><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: s = &quot;abcdefg&quot;, k = 2</span><br><span class="line">Output: &quot;bacdfeg&quot;</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>给一个字符串和一个整数$k$，需要每$2k$个字符就把前$k$个字符翻转，如果少于$k$个字符就都翻转，如果多等于$k$个而于少于$2k$个字符，就翻转前$k$个而剩下的不变。</p><p>按题意模拟一下即可…</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">rev</span><span class="params">(<span class="built_in">string</span>&amp; s, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) swap(s[l++], s[r--]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">reverseStr</span><span class="params">(<span class="built_in">string</span> s, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> siz = s.size();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; siz; i += <span class="number">2</span>*k) rev(s, i, min(i + k - <span class="number">1</span>, siz - <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given a string and an integer k, you need to reverse the first k characters for every 2k characters counting from the start of the string. If there are less than k characters left, reverse all of them. If there are less than 2k but greater than or equal to k characters, then reverse the first k characters and left the other as original.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Restrictions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The string consists of lower English letters only.&lt;/li&gt;
&lt;li&gt;Length of the given string and k will in the range [1, 10000]&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="字符串" scheme="https://orzyt.cn/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode530 Minimum Absolute Difference in BST</title>
    <link href="https://orzyt.cn/posts/LeetCode530-Minimum-Absolute-Difference-in-BST/"/>
    <id>https://orzyt.cn/posts/LeetCode530-Minimum-Absolute-Difference-in-BST/</id>
    <published>2018-01-09T04:50:30.000Z</published>
    <updated>2018-11-10T11:47:16.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Given a binary search tree with non-negative values, find the minimum <a href="https://en.wikipedia.org/wiki/Absolute_difference" target="_blank" rel="noopener">absolute difference</a> between values of any two nodes.</p><a id="more"></a><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line"></span><br><span class="line">   1</span><br><span class="line">    \</span><br><span class="line">     3</span><br><span class="line">    /</span><br><span class="line">   2</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">Explanation:</span><br><span class="line">The minimum absolute difference is 1, which is the difference between 2 and 1 (or between 2 and 3).</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>求节点为非负整数的二叉搜索树中，任意两个节点绝对差值的最小值。</p><p>根据二叉搜索树的性质，对其进行中序遍历即可得到一个有序（从小到大）的结果。由于结果是有序的，因此只需要考虑相邻两个元素之间的差值来获得最小值。</p><p>比如对如下的二叉搜索树进行中序遍历，得到 <code>[3, 5, 7, 8, 10]</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   5</span><br><span class="line"> /   \</span><br><span class="line">3     8</span><br><span class="line">     / \</span><br><span class="line">    7  10</span><br></pre></td></tr></table></figure><p>然后答案即为相邻元素的最小差值，ans = min {<code>5-3</code>, <code>7-5</code>, <code>8-7</code>, <code>10-8</code>} = 1</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> ans, pre;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">inOrder</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) <span class="keyword">return</span> ;</span><br><span class="line">        inOrder(root-&gt;left);</span><br><span class="line">        ans = min(ans, root-&gt;val - pre);</span><br><span class="line">        pre = root-&gt;val;</span><br><span class="line">        inOrder(root-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getMinimumDifference</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        ans = INT_MAX; </span><br><span class="line">        pre = <span class="number">-0x3f3f3f3f</span>; <span class="comment">// 记录中序遍历时当前节点的前驱</span></span><br><span class="line">        inOrder(root);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Given a binary search tree with non-negative values, find the minimum &lt;a href=&quot;https://en.wikipedia.org/wiki/Absolute_difference&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;absolute difference&lt;/a&gt; between values of any two nodes.&lt;/p&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="BST" scheme="https://orzyt.cn/tags/BST/"/>
    
      <category term="二叉搜索树" scheme="https://orzyt.cn/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode155 Min Stack</title>
    <link href="https://orzyt.cn/posts/LeetCode155-Min-Stack/"/>
    <id>https://orzyt.cn/posts/LeetCode155-Min-Stack/</id>
    <published>2018-01-02T04:47:09.000Z</published>
    <updated>2018-11-10T11:47:16.907Z</updated>
    
    <content type="html"><![CDATA[<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>Design a stack that supports push, pop, top, and retrieving the minimum element in constant time.</p><ul><li>push(x) — Push element x onto stack.</li><li>pop() — Removes the element on top of the stack.</li><li>top() — Get the top element.</li><li>getMin() — Retrieve the minimum element in the stack.</li></ul><a id="more"></a><h2 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MinStack minStack = new MinStack();</span><br><span class="line">minStack.push(-2);</span><br><span class="line">minStack.push(0);</span><br><span class="line">minStack.push(-3);</span><br><span class="line">minStack.getMin();   --&gt; Returns -3.</span><br><span class="line">minStack.pop();</span><br><span class="line">minStack.top();      --&gt; Returns 0.</span><br><span class="line">minStack.getMin();   --&gt; Returns -2.</span><br></pre></td></tr></table></figure><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>模拟一个栈，在<strong>常数时间</strong>内支持<code>进栈push</code>、<code>出栈pop</code>、<code>输出栈顶元素top</code>、<code>输出栈内最小元素getMin</code>四种操作</p><p>我们知道，普通的栈对于前三种操作已经是常数时间了，但是如何实现常数时间内找到栈内最小元素呢？</p><p>可能一开始会想，不是可以$\mathcal{O}(1)$维护一个栈内最小元素的下标吗？<code>getMin</code>的时候直接根据下标输出不就好了</p><p>emm…是这样的没错。但是，一旦最小元素出栈后，我们就得找出栈内第二小元素来更新下标 </p><p>这个该怎么处理呢，肯定不能把栈遍历一遍啊（时间复杂度不满足要求…</p><hr><p>其实，只要维护一个<strong>单调栈</strong>即可（非严格单调递减），单调栈的栈顶就是<strong>原栈中最小的元素</strong></p><p>其中stack A为原栈，stack B为辅助栈，# 表示栈底方向</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">stack A: # | 10 |               // 元素10进栈</span><br><span class="line">stack B: # | 10 |               // 当前最小元素为10</span><br><span class="line"></span><br><span class="line">stack A: # | 10 | 12 |          // 元素12进栈</span><br><span class="line">stack B: # | 10 |               // 由于维护的是单调递减栈，当前最小元素仍然为10</span><br><span class="line"></span><br><span class="line">stack A: # | 10 | 12 | 9 |      // 元素9进栈</span><br><span class="line">stack B: # | 10 | 9 |           // 满足单调栈，当前最小元素更新为9</span><br><span class="line"></span><br><span class="line">stack A: # | 10 | 12 | 9 | 14 | // 元素14进栈</span><br><span class="line">stack B: # | 10 | 9 |           // 不满足单调栈，当前最小元素仍为9</span><br><span class="line"></span><br><span class="line">stack A: # | 10 | 12 | 9 |      // 元素14出栈</span><br><span class="line">stack B: # | 10 | 9 |           // 不影响辅助栈，当前最小元素仍为9</span><br><span class="line"></span><br><span class="line">stack A: # | 10 | 12 |          // 元素9出栈</span><br><span class="line">stack B: # | 10 |               // 辅助栈栈顶的9同时出栈，当前最小元素更新为10</span><br><span class="line"></span><br><span class="line">stack A: # | 10 |               // 元素12出栈</span><br><span class="line">stack B: # | 10 |               // 不影响辅助栈，当前最小元素仍为10</span><br></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinStack</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; a, b;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** initialize your data structure here. */</span></span><br><span class="line">    MinStack() &#123;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123; </span><br><span class="line">        <span class="comment">// 原栈进栈的同时，看满不满足辅助栈的单调性质</span></span><br><span class="line">        <span class="keyword">if</span> (b.empty() || x &lt;= getMin()) b.push(x);</span><br><span class="line">        a.push(x);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 原栈出栈的同时，看影不影响辅助栈的栈顶</span></span><br><span class="line">        <span class="keyword">if</span> (a.top() == getMin()) b.pop();</span><br><span class="line">        a.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 输出原栈的栈顶元素</span></span><br><span class="line">        <span class="keyword">return</span> a.top();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getMin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 输出辅助栈栈顶元素，即为原栈中的最小元素</span></span><br><span class="line">        <span class="keyword">return</span> b.top();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Your MinStack object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"> * MinStack obj = new MinStack();</span></span><br><span class="line"><span class="comment"> * obj.push(x);</span></span><br><span class="line"><span class="comment"> * obj.pop();</span></span><br><span class="line"><span class="comment"> * int param_3 = obj.top();</span></span><br><span class="line"><span class="comment"> * int param_4 = obj.getMin();</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h2&gt;&lt;p&gt;Design a stack that supports push, pop, top, and retrieving the minimum element in constant time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;push(x) — Push element x onto stack.&lt;/li&gt;
&lt;li&gt;pop() — Removes the element on top of the stack.&lt;/li&gt;
&lt;li&gt;top() — Get the top element.&lt;/li&gt;
&lt;li&gt;getMin() — Retrieve the minimum element in the stack.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="栈" scheme="https://orzyt.cn/tags/%E6%A0%88/"/>
    
      <category term="单调栈" scheme="https://orzyt.cn/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode Algorithms&#39; Solutions</title>
    <link href="https://orzyt.cn/posts/leetcode-algorithms-solutions/"/>
    <id>https://orzyt.cn/posts/leetcode-algorithms-solutions/</id>
    <published>2017-12-31T16:00:00.000Z</published>
    <updated>2018-11-10T11:47:16.931Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>“ <strong>Talk is cheap, show me the code!</strong> ”<br>— <em>Linus Torvalds</em></p></blockquote><ul><li>如果题解中有谬误或疑问的地方，欢迎大家留言交流！<a id="more"></a> </li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">#</th><th>Title</th><th style="text-align:center">Acceptance</th><th style="text-align:center">Difficulty</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td><a href="/posts/LeetCode1-Two-Sum/">Two Sum</a></td><td style="text-align:center">36.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">2</td><td><a href="/posts/LeetCode2-Add-Two-Numbers/">Add Two Numbers</a></td><td style="text-align:center">28.2%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">7</td><td><a href="/posts/LeetCode7-Reverse-Integer/">Reverse Integer</a></td><td style="text-align:center">24.4%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">8</td><td><a href="/posts/LeetCode8-String-to-Integer-(atoi)/">String to Integer (atoi)</a></td><td style="text-align:center">13.9%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">9</td><td><a href="/posts/LeetCode9-Palindrome-Number/">Palindrome Number</a></td><td style="text-align:center">35.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">13</td><td><a href="/posts/LeetCode13-Roman-to-Integer/">Roman to Integer</a></td><td style="text-align:center">47.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">20</td><td><a href="/posts/LeetCode20-Valid-Parentheses/">Valid Parentheses</a></td><td style="text-align:center">33.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">21</td><td><a href="/posts/LeetCode21-Merge-Two-Sorted-Lists/">Merge Two Sorted Lists</a></td><td style="text-align:center">39.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">27</td><td><a href="/posts/LeetCode27-Remove-Element/">Remove Element</a></td><td style="text-align:center">40.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">28</td><td><a href="/posts/LeetCode28-Implement-strStr/">Implement strStr()</a></td><td style="text-align:center">28.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">35</td><td><a href="/posts/LeetCode35-Search-Insert-Position/">Search Insert Position</a></td><td style="text-align:center">39.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">50</td><td><a href="/posts/LeetCode50-Pow(x,n)/">Pow(x, n)</a></td><td style="text-align:center">26.0%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">53</td><td><a href="/posts/LeetCode53-Maximum-Subarray-Element/">Maximum Subarray</a></td><td style="text-align:center">40.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">69</td><td><a href="/posts/LeetCode69-Sqrt(x)/">Sqrt(x)</a></td><td style="text-align:center">28.4%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">70</td><td><a href="/posts/LeetCode70-Climbing-Stairs/">Climbing Stairs</a></td><td style="text-align:center">40.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">83</td><td><a href="/posts/LeetCode83-Remove-Duplicates-from-Sorted-List/">Remove Duplicates from Sorted List</a></td><td style="text-align:center">40.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">84</td><td><a href="/posts/LeetCode84-Largest-Rectangle-in-Histogram/">Largest Rectangle in Histogram</a></td><td style="text-align:center">27.3%</td><td style="text-align:center"><span class="label label-danger round"><p style="display: none;">3</p>Hard</span></td></tr><tr><td style="text-align:center">100</td><td><a href="/posts/LeetCode100-Same-Tree/">Same Tree</a></td><td style="text-align:center">47.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">101</td><td><a href="/posts/LeetCode101-Symmetric-Tree/">Symmetric Tree</a></td><td style="text-align:center">39.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">102</td><td><a href="/posts/LeetCode102-Binary-Tree-Level-Order-Traversal/">Binary Tree Level Order Traversal</a></td><td style="text-align:center">41.3%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">104</td><td><a href="/posts/LeetCode104-Maximum-Depth-of-Binary-Tree/">Maximum Depth of Binary Tree</a></td><td style="text-align:center">53.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">107</td><td><a href="/posts/LeetCode107-Binary-Tree-Level-Order-Traversal-II/">Binary Tree Level Order Traversal II</a></td><td style="text-align:center">41.3%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">108</td><td><a href="/posts/LeetCode108-Convert-Sorted-Array-to-Binary-Search-Tree/">Convert Sorted Array to Binary Search Tree</a></td><td style="text-align:center">43.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">112</td><td><a href="/posts/LeetCode112-Path-Sum/">Path Sum</a></td><td style="text-align:center">34.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">118</td><td><a href="/posts/LeetCode118-Pascals-Triangle/">Pascal’s Triangle</a></td><td style="text-align:center">39.4%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">119</td><td><a href="/posts/LeetCode119-Pascals-Triangle-II/">Pascal’s Triangle II</a></td><td style="text-align:center">37.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">121</td><td><a href="/posts/LeetCode121-Best-Time-to-Buy-and-Sell-Stock/">Best Time to Buy and Sell Stock</a></td><td style="text-align:center">42.3%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">122</td><td><a href="/posts/LeetCode122-Best-Time-to-Buy-and-Sell-Stock-II/">Best Time to Buy and Sell Stock II</a></td><td style="text-align:center">47.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">125</td><td><a href="/posts/LeetCode125-Valid-Palindrome/">Valid Palindrome</a></td><td style="text-align:center">26.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">136</td><td><a href="/posts/LeetCode136-Single-Number/">Single Number</a></td><td style="text-align:center">55.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">153</td><td><a href="/posts/LeetCode153-Find-Minimum-in-Rotated-Sorted-Array/">Find Minimum in Rotated Sorted Array</a></td><td style="text-align:center">40.5%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">154</td><td><a href="/posts/LeetCode154-Find-Minimum-in-Rotated-Sorted-Array-II/">Find Minimum in Rotated Sorted Array II</a></td><td style="text-align:center">37.6%</td><td style="text-align:center"><span class="label label-danger round"><p style="display: none;">3</p>Hard</span></td></tr><tr><td style="text-align:center">155</td><td><a href="/posts/LeetCode155-Min-Stack/">Min Stack</a></td><td style="text-align:center">30.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">167</td><td><a href="/posts/LeetCode167-Two-Sum-II-Input-array-is-sorted/">Two Sum II - Input array is sorted</a></td><td style="text-align:center">47.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">168</td><td><a href="/posts/LeetCode168-Excel-Sheet-Column-Title/">Excel Sheet Column Title</a></td><td style="text-align:center">26.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">169</td><td><a href="/posts/LeetCode169-Majority-Element/">Majority Element</a></td><td style="text-align:center">47.4%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">171</td><td><a href="/posts/LeetCode171-Excel-Sheet-Column-Number/">Excel Sheet Column Number</a></td><td style="text-align:center">47.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">172</td><td><a href="/posts/LeetCode172-Factorial-Trailing-Zeroes/">Factorial Trailing Zeroes</a></td><td style="text-align:center">36.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">189</td><td><a href="/posts/LeetCode189-Rotate-Array/">Rotate Array</a></td><td style="text-align:center">25.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">190</td><td><a href="/posts/LeetCode190-Reverse-Bits/">Reverse Bits</a></td><td style="text-align:center">29.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">191</td><td><a href="/posts/LeetCode191-Number-of-1-Bits/">Number of 1 Bits</a></td><td style="text-align:center">40.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">202</td><td><a href="/posts/LeetCode202-Happy-Number/">Happy Number</a></td><td style="text-align:center">41.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">204</td><td><a href="/posts/LeetCode204-Count-Primes/">Count Primes</a></td><td style="text-align:center">26.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">206</td><td><a href="/posts/LeetCode206-Reverse-Linked-List/">Reverse Linked List</a></td><td style="text-align:center">46.3%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">217</td><td><a href="/posts/LeetCode217-Contains-Duplicate/">Contains Duplicate</a></td><td style="text-align:center">46.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">226</td><td><a href="/posts/LeetCode226-Invert-Binary-Tree/">Invert Binary Tree</a></td><td style="text-align:center">52.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">231</td><td><a href="/posts/LeetCode231-Power-of-Two/">Power of Two</a></td><td style="text-align:center">40.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">237</td><td><a href="/posts/LeetCode237-Delete-Node-in-a-Linked-List/">Delete Node in a Linked List</a></td><td style="text-align:center">47.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">242</td><td><a href="/posts/LeetCode242-Valid-Anagram/">Valid Anagram</a></td><td style="text-align:center">47.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">258</td><td><a href="/posts/LeetCode258-Add-Digits/">Add Digits</a></td><td style="text-align:center">51.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">263</td><td><a href="/posts/LeetCode263-Ugly-Number/">Ugly Number</a></td><td style="text-align:center">39.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">264</td><td><a href="/posts/LeetCode264-Ugly-Number-II/">Ugly Number II</a></td><td style="text-align:center">33.1%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">268</td><td><a href="/posts/LeetCode268-Missing-Number/">Missing Number</a></td><td style="text-align:center">44.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">278</td><td><a href="/posts/LeetCode278-First-Bad-Version/">First Bad Version</a></td><td style="text-align:center">25.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">283</td><td><a href="/posts/LeetCode283-Move-Zeroes/">Move Zeroes</a></td><td style="text-align:center">50.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">292</td><td><a href="/posts/LeetCode292-Nim-Game/">Nim Game</a></td><td style="text-align:center">55.3%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">303</td><td><a href="/posts/LeetCode303-Range-Sum-Query-Immutable/">Range Sum Query - Immutable</a></td><td style="text-align:center">31.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">307</td><td><a href="/posts/LeetCode307-Range-Sum-Query-Mutable/">Range Sum Query - Mutable</a></td><td style="text-align:center">21.7%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">326</td><td><a href="/posts/LeetCode326-Power-of-Three/">Power of Three</a></td><td style="text-align:center">40.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">338</td><td><a href="/posts/LeetCode338-Counting-Bits/">Counting Bits</a></td><td style="text-align:center">61.9%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">342</td><td><a href="/posts/LeetCode342-Power-of-Four/">Power of Four</a></td><td style="text-align:center">38.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">344</td><td><a href="/posts/LeetCode344-Reverse-String/">Reverse String</a></td><td style="text-align:center">59.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">345</td><td><a href="/posts/LeetCode345-Reverse-Vowels-of-a-String/">Reverse Vowels of a String</a></td><td style="text-align:center">38.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">349</td><td><a href="/posts/LeetCode349-Intersection-of-Two-Arrays/">Intersection of Two Arrays</a></td><td style="text-align:center">47.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">350</td><td><a href="/posts/LeetCode350-Intersection-of-Two-Arrays-II/">Intersection of Two Arrays II</a></td><td style="text-align:center">44.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">367</td><td><a href="/posts/LeetCode367-Valid-Perfect-Square/">Valid Perfect Square</a></td><td style="text-align:center">38.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">371</td><td><a href="/posts/LeetCode371-Sum-of-Two-Integers/">Sum of Two Integers</a></td><td style="text-align:center">51.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">383</td><td><a href="/posts/LeetCode383-Ransom-Note/">Ransom Note</a></td><td style="text-align:center">47.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">387</td><td><a href="/posts/LeetCode387-First-Unique-Character-in-a-String/">First Unique Character in a String</a></td><td style="text-align:center">47.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">389</td><td><a href="/posts/LeetCode389-Find-the-Difference/">Find the Difference</a></td><td style="text-align:center">51.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">401</td><td><a href="/posts/LeetCode401-Binary-Watch/">Binary Watch</a></td><td style="text-align:center">44.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">404</td><td><a href="/posts/LeetCode404-Sum-of-Left-Leaves/">Sum of Left Leaves</a></td><td style="text-align:center">47.4%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">405</td><td><a href="/posts/LeetCode405-Convert-a-Number-to-Hexadecimal/">Convert a Number to Hexadecimal</a></td><td style="text-align:center">41.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">409</td><td><a href="/posts/LeetCode409-Longest-Palindrome/">Longest Palindrome</a></td><td style="text-align:center">45.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">412</td><td><a href="/posts/LeetCode412-Fizz-Buzz/">Fizz Buzz</a></td><td style="text-align:center">58.3%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">413</td><td><a href="/posts/LeetCode413-Arithmetic-Slices/">Arithmetic Slices</a></td><td style="text-align:center">54.6%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">414</td><td><a href="/posts/LeetCode414-Third-Maximum-Number/">Third Maximum Number</a></td><td style="text-align:center">28.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">415</td><td><a href="/posts/LeetCode415-Add-Strings/">Add Strings</a></td><td style="text-align:center">41.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">419</td><td><a href="/posts/LeetCode419-Battleships-in-a-Board/">Battleships in a Board</a></td><td style="text-align:center">62.5%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">437</td><td><a href="/posts/LeetCode437-Path-Sum-III/">Path Sum III</a></td><td style="text-align:center">40.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">442</td><td><a href="/posts/LeetCode442-Find-All-Duplicates-in-an-Array/">Find All Duplicates in an Array</a></td><td style="text-align:center">56.6%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">447</td><td><a href="/posts/LeetCode447-Number-of-Boomerangs/">Number of Boomerangs</a></td><td style="text-align:center">45.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">448</td><td><a href="/posts/LeetCode448-Find-All-Numbers-Disappeared-in-an-Array/">Find All Numbers Disappeared in an Array</a></td><td style="text-align:center">51.3%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">451</td><td><a href="/posts/LeetCode451-Sort-Characters-By-Frequency/">Sort Characters By Frequency</a></td><td style="text-align:center">51.4%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">453</td><td><a href="/posts/LeetCode453-Minimum-Moves-to-Equal-Array-Elements/">Minimum Moves to Equal Array Elements</a></td><td style="text-align:center">47.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">455</td><td><a href="/posts/LeetCode455-Assign-Cookies/">Assign Cookies</a></td><td style="text-align:center">47.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">459</td><td><a href="/posts/LeetCode459-Repeated-Substring-Pattern/">Repeated Substring Pattern</a></td><td style="text-align:center">38.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">461</td><td><a href="/posts/LeetCode461-Hamming-Distance/">Hamming Distance</a></td><td style="text-align:center">69.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">463</td><td><a href="/posts/LeetCode463-Island-Perimeter/">Island Perimeter</a></td><td style="text-align:center">57.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">476</td><td><a href="/posts/LeetCode476-Number-Complement/">Number Complement</a></td><td style="text-align:center">61.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">479</td><td><a href="/posts/LeetCode479-Largest-Palindrome-Product/">Largest Palindrome Product</a></td><td style="text-align:center">24.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">485</td><td><a href="/posts/LeetCode485-Max-Consecutive-Ones/">Max Consecutive Ones</a></td><td style="text-align:center">54.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">492</td><td><a href="/posts/LeetCode492-Construct-the-Rectangle/">Construct the Rectangle</a></td><td style="text-align:center">48.3%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">496</td><td><a href="/posts/LeetCode496-Next-Greater-Element-I/">Next Greater Element I</a></td><td style="text-align:center">56.6%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">500</td><td><a href="/posts/LeetCode500-Keyboard-Row/">Keyboard Row</a></td><td style="text-align:center">59.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">501</td><td><a href="/posts/LeetCode501-Find-Mode-in-Binary-Search-Tree/">Find Mode in Binary Search Tree</a></td><td style="text-align:center">37.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">504</td><td><a href="/posts/LeetCode504-Base-7/">Base 7</a></td><td style="text-align:center">44.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">506</td><td><a href="/posts/LeetCode506-Relative-Ranks/">Relative Ranks</a></td><td style="text-align:center">46.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">513</td><td><a href="/posts/LeetCode513-Find-Bottom-Left-Tree-Value/">Find Bottom Left Tree Value</a></td><td style="text-align:center">56.1%</td><td style="text-align:center"><span class="label label-warning round"><p style="display: none;">2</p>Medium</span></td></tr><tr><td style="text-align:center">520</td><td><a href="/posts/LeetCode520-Detect-Capital/">Detect Capital</a></td><td style="text-align:center">52.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">530</td><td><a href="/posts/LeetCode530-Minimum-Absolute-Difference-in-BST/">Minimum Absolute Difference in BST</a></td><td style="text-align:center">47.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">541</td><td><a href="/posts/LeetCode541-Reverse-String-II/">Reverse String II</a></td><td style="text-align:center">43.7%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">543</td><td><a href="/posts/LeetCode543-Diameter-of-Binary-Tree/">Diameter of Binary Tree</a></td><td style="text-align:center">44.9%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">557</td><td><a href="/posts/LeetCode557-Reverse-Words-in-a-String-III/">Reverse Words in a String III</a></td><td style="text-align:center">59.8%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">561</td><td><a href="/posts/LeetCode561-Array-Partition-I/">Array Partition I</a></td><td style="text-align:center">66.5%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">563</td><td><a href="/posts/LeetCode563-Binary-Tree-Tilt/">Binary Tree Tilt</a></td><td style="text-align:center">47.1%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">566</td><td><a href="/posts/LeetCode566-Reshape-the-Matrix/">Reshape the Matrix</a></td><td style="text-align:center">58.2%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">575</td><td><a href="/posts/LeetCode575-Distribute-Candies/">Distribute Candies</a></td><td style="text-align:center">58.4%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">669</td><td><a href="/posts/LeetCode669-Trim-a-Binary-Search-Tree/">Trim a Binary Search Tree</a></td><td style="text-align:center">58.4%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr><tr><td style="text-align:center">671</td><td><a href="/posts/LeetCode671-Second-Minimum-Node-In-a-Binary-Tree/">Second Minimum Node In a Binary Tree</a></td><td style="text-align:center">42.0%</td><td style="text-align:center"><span class="label label-success round"><p style="display: none;">1</p>Easy</span></td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;“ &lt;strong&gt;Talk is cheap, show me the code!&lt;/strong&gt; ”&lt;br&gt;— &lt;em&gt;Linus Torvalds&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;如果题解中有谬误或疑问的地方，欢迎大家留言交流！
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
      <category term="算法" scheme="https://orzyt.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>条件生成对抗网络(Conditional GANs)</title>
    <link href="https://orzyt.cn/posts/conditional-gan/"/>
    <id>https://orzyt.cn/posts/conditional-gan/</id>
    <published>2017-12-26T04:39:58.000Z</published>
    <updated>2018-11-10T11:47:16.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在原始的生成对抗网络论文中，作者指出了一个可能的拓展:</p><blockquote><p>A conditional generative model $p(x|c)$ can be obtained by adding $c$ as input to both $G$ and $D$</p></blockquote><p>于是同年11月，Mirza等人便提出了<a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="noopener">Conditional Generative Adversarial Networks</a>，这是一种带条件约束的生成模型。<br><a id="more"></a><br>它在生成器 $G$ 和 判别器 $D$ 中均引入了条件变量 $y$，这里 $y$ 可以是任何的辅助信息（比如说，类别标签、其它模态的数据等等）。使用这个额外的条件变量，对生成器数据的生成具有指导作用。因此，该项工作可以看成是把无监督的GAN变成有监督模型的一种改进。</p><h2 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h2><p><img src="https://tuchuang001.com/images/2017/12/27/cgan.png" alt="Conditional GANs的基本框架" width="60%" height="60%"></p><p><code>Conditional GANs</code>的基本框架非常地简单，只需在原始GAN的生成器和判别器的输入中，加入额外的条件信息即可。</p><p>显然地，目标函数改为：</p><script type="math/tex; mode=display">\mathop{\min}_{G}\mathop{\max}_{D}V(D,G)=\mathbb{E}_{\boldsymbol{x}\sim p_{\text{data}}}\left[\log D(\boldsymbol{x}|\boldsymbol{y})\right]+\mathbb{E}_{\boldsymbol{z}\sim p_z(\boldsymbol{z})}\left[\log(1-D(G(\boldsymbol{z}|\boldsymbol{y})|\boldsymbol{y}))\right]</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在论文中，作者做了两个实验，一个是单模态的<code>MNIST</code>手写数字生成，另一个是基于多模态的图像自动标注。</p><p><img src="https://tuchuang001.com/images/2017/12/27/mnist.png" alt="mnist手写数字生成(论文原图)" width="70%" height="70%"></p><p>在<code>MNIST</code>数据集的实验中，生成器 $G$ 的输入分为两部分：</p><ul><li>噪声 $z$：<code>100维</code> 服从均匀分布的向量</li><li>条件 $y$：类别标签的<code>one hot编码</code></li></ul><p>然后将噪声 $z$ 和 标签 $y$ 分别映射到隐层(<strong>200</strong>和<strong>1000units</strong>)，在映射到第二层前，连接所有<strong>1200units</strong>。最终，用一个<code>sigmoid</code>层输出<strong>784</strong>维(<strong>28*28</strong>)的单通道图像。</p><p>判别器 $D$ 把输入图像 $x$ 映射到一个有<strong>240units</strong>和<strong>5pieces</strong>的<code>maxout layer</code>，把标签 $y$ 映射到有<strong>50units</strong>和<strong>5pieces</strong>的<code>maxout layer</code>。同时，把所有隐层连接成为一个有<strong>240units</strong>和<strong>4pieces</strong>的<code>maxout layer</code>。最后送入<code>sigmoid</code>层，该层的输出即为在条件 $y$ 下，输入图像 $x$ 为真实样本的概率。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>Github地址：<a href="https://github.com/orzyt/Generative-Adversarial-Nets/tree/master/conditional-gan" target="_blank" rel="noopener">(orzyt/Generative-Adversarial-Nets/conditional-gan)</a></p><p>具体实现细节与论文有所不同，网络架构使用的是<a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">DCGAN</a>，效果会比论文中的好（毕竟DCGAN是在CGAN之后才提出的…</p><p>让我们看下代码跑出来的效果</p><p><img src="https://tuchuang001.com/images/2017/12/27/mnist.gif" alt="mnist手写数字生成(实测效果)" width="60%" height="60%"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在原始的生成对抗网络论文中，作者指出了一个可能的拓展:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A conditional generative model $p(x|c)$ can be obtained by adding $c$ as input to both $G$ and $D$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;于是同年11月，Mirza等人便提出了&lt;a href=&quot;https://arxiv.org/abs/1411.1784&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Conditional Generative Adversarial Networks&lt;/a&gt;，这是一种带条件约束的生成模型。&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://orzyt.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="cgan" scheme="https://orzyt.cn/tags/cgan/"/>
    
      <category term="conditional gan" scheme="https://orzyt.cn/tags/conditional-gan/"/>
    
      <category term="条件生成对抗网络" scheme="https://orzyt.cn/tags/%E6%9D%A1%E4%BB%B6%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>生成对抗网络(Generative Adversarial Nets)</title>
    <link href="https://orzyt.cn/posts/generative-adversarial-nets/"/>
    <id>https://orzyt.cn/posts/generative-adversarial-nets/</id>
    <published>2017-12-18T23:16:14.000Z</published>
    <updated>2018-11-10T11:47:16.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><strong>生成对抗网络</strong>（Generative Adversarial Nets, GANs）是由 Ian Goodfellow 等人于2014年6月提出的一种生成模型，至今仍是非常热门的研究方向。深度学习领域泰斗级人物 Yann LeCun 曾如此评价道：<em>“GANs and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.”</em>，从中可见GAN的强大之处！</p><a id="more"></a><hr><h2 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h2><p>生成对抗网络的主要框架基于博弈论中的<a href="https://en.wikipedia.org/wiki/Zero-sum_game" target="_blank" rel="noopener">二人零和博弈游戏</a>（zero-sum game）</p><p>在该游戏中，两位博弈方的利益之和为零或一个常数，即一方有所得，另一方必有所失。而在生成对抗网络中，这两个博弈的角色分别为：<strong>生成器</strong>（generator）和<strong>判别器</strong>（discriminator），作用如下：</p><p>生成器 $G$：接受随机噪声（noise）输入，生成尽可能真实的样本<br>判别器 $D$：接受任意样本 $x$，输出 $D(x)$ 代表 $x$ 为真实样本的概率</p><p>在训练过程中，生成器 $G$ 的目标就是<strong>尽量生成真实的样本去欺骗判别器$D$</strong>，而判别器 $D$ 的目标就是<strong>尽量把生成器$G$生成的假样本和真实的样本区分开来</strong>。如此一来，生成器 $G$ 和判别器 $D$ 就构成了一个动态的“博弈过程”</p><p>在理想的情况下，博弈的结果是：<strong>生成器 $G$ 所拟合的分布 $p_\it{g}$ 可以无限接近于真实样本中的分布 $p_\it{data}$</strong> ，足以生成以假乱真的样本。而判别器 $D$ 无法再区分出样本的真假，因此对任意样本 $x$ ，都有 $D(x)=\frac{1}{2}$</p><p>最终，我们得到了一个生成模型 $G$！</p><p><img src="https://tuchuang001.com/images/2017/12/19/2.png" alt="GAN的基本框架"></p><h2 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h2><p>GAN的优化目标函数如下：</p><script type="math/tex; mode=display">\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z)))] \tag{1}</script><p>其中，</p><ul><li>$x$：真实样本</li><li>$p_{data}(x)$：真实样本的分布</li><li>$z$：随机噪声</li><li>$p_{z}(z)$：随机噪声的分布，一般采用<em>高斯分布</em></li><li>$G(z)$：生成器根据随机噪声 $z$ 生成的假样本</li><li>$D(x)$：样本 $x$ 为真实样本的概率</li></ul><p>上述目标函数是一个<strong>极小化极大</strong>的过程。对于判别器 $D$ 来说，它的目标是极大化 $V(D, G)$，那么必然导致 $D(x) \to 1$ 且 $D(G(z)) \to 0$，也就是前面提到的尽可能把生成的假样本和真样本正确地区分开来。而对于生成器 $G$ 来说，它的目标是极小化最优的判别器，同样地，有 $D(G(z)) \to 1$，即生成器尽量生成真实的样本去欺骗判别器</p><hr><p>那么，对于固定的生成器 $G$，最优的判别器 $D^*$ 是多少呢？</p><script type="math/tex; mode=display">\begin{align}D^* &= \max_D V(D, G) \nonumber \\&= \max_D \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z))] \nonumber \\&= \max_D \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{x \sim p_{G}(x)}[\log (1 - D(x))] \nonumber \\ &= \max_D \int_x P_{data}(x) \log D(x)dx + \int_x P_{G}(x) \log (1-D(x))dx \nonumber \\ &= \max_D \int_x \left[ P_{data}(x) \log D(x)+ P_{G}(x) \log (1-D(x)) \right]dx \nonumber \\\end{align}</script><p>想要极大化 $V(D, G)$，那么对每一个样本 $x$，都令 $P_{data}(x) \log D(x)+ P_{G}(x) \log (1-D(x))$ 取到极大值即可。</p><p>由于给定了 $x$, 那么 $P_{data}(x)$ 和 $P_{G}(x)$ 都为定值，分别记作 $a$ 和 $b$<br>对于函数 $f(D)=a\log D + b\log (1-D)$，令 $\frac{df(D)}{dD} = a \cdot \frac{1}{D}-b \cdot \frac{1}{1-D} = 0 $，解得 $D^* = \frac{a}{a + b} = \frac{P_{data}(x)}{P_{data}(x) + P_{G}(x)}$</p><p>即对于固定的生成器 $G$，最优的判别器为</p><script type="math/tex; mode=display">D^*  = \frac{P_{data}(x)}{P_{data}(x) + P_{G}(x)} \tag{2}</script><hr><p>现在，考虑生成器 $G$ 的目标，极小化最优的判别器，得到最优生成器 $G^*$</p><script type="math/tex; mode=display">\begin{align}G^* &= \min_G V(D^*, G) \nonumber \\&= \min_G \mathbb{E}_{x \sim p_{data}(x)}[\log D^*(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D^*(G(z))] \nonumber \\&= \min_G \mathbb{E}_{x \sim p_{data}(x)}[\log D^*(x)] + \mathbb{E}_{x \sim p_{G}(x)}[\log (1 - D^*(x))] \nonumber \\ &= \min_G \mathbb{E}_{x \sim p_{data}(x)}[\log \frac{P_{data}(x)}{P_{data}(x) + P_{G}(x)}] + \mathbb{E}_{x \sim p_{G}(x)}[\log (1 - \frac{P_{data}(x)}{P_{data}(x) + P_{G}(x)})] \nonumber \\ &= \min_G \mathbb{E}_{x \sim p_{data}(x)}[\log \frac{P_{data}(x)}{P_{data}(x) + P_{G}(x)}] + \mathbb{E}_{x \sim p_{G}(x)}[\log ( \frac{P_{G}(x)}{P_{data}(x) + P_{G}(x)})] \nonumber \\ &= \min_G \mathbb{E}_{x \sim p_{data}(x)}[\log \frac{P_{data}(x)}{\frac{P_{data}(x) + P_{G}(x)}{2}}] + \mathbb{E}_{x \sim p_{G}(x)}[\log ( \frac{P_{G}(x)}{\frac{P_{data}(x) + P_{G}(x)}{2}})] -2\log2 \nonumber \\ &= \min_G KL(P_{data}||\frac{P_{data}(x) + P_{G}(x)}{2}) + KL(P_{G}||\frac{P_{data}(x) + P_{G}(x)}{2}) - 2\log2 \nonumber \\&= \min_G 2JS(P_{data}||P_{G}) - 2\log 2 \tag{3} \end{align}</script><p>其中，$KL$表示<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">Kullback–Leibler divergence</a>，$JS$表示<a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank" rel="noopener">Jensen–Shannon divergence</a>，都是用来度量两个分布的相似性<br>对于$JS$散度来说，其取值范围为$[0, \log2]$，当且仅当两个分布相等时取到最小值0</p><p>因此，最优生成器 $G^* = \min_G 2JS(P_{data}||P_{G}) - 2\log 2$ ，当且仅当 $P_{data} = P_{G}$ 时，取到最小值 $-2\log2$</p><hr><p>由上述分析可知，该极小化极大的博弈对抗过程确实可以让生成器学习到真实的数据分布！</p><hr><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>根据前面 $V(D,G)$ 的定义，我们需要求两个数学期望，即 $\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]$ 和 $\mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z)))]$。但在实践中，我们是没办法利用积分来求这两个数学期望的，所以一般只能从数据集中做采样以逼近真实的数学期望</p><script type="math/tex; mode=display">\tilde{V}(D, G) = \frac{1}{m} \sum_{i=1}^{m} \log D(\boldsymbol{x}^i) + \frac{1}{m} \sum_{i=1}^{m} \log (1 - D(G(\boldsymbol{z}^i))</script><p><img src="https://tuchuang001.com/images/2017/12/19/3.png" alt="GAN的算法流程"></p><hr><p><strong>训练判别器</strong> $D$ （训练 $k$ 次）：<br>每次采样一组噪声变量 $\{z^1,z^2, \dots,z^m\}$ 和 一组真实数据 $\{x^1,x^2, \dots,x^m\}$，计算 $\theta_d$ 的梯度：</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{\theta}_d} \frac{1}{m} \sum_{i=1}^{m} \left[ \log(D(\boldsymbol{x}^i)) + \log(1 - D(G(\boldsymbol{z}^i))) \right]</script><p>然后使用<strong>梯度上升</strong>算法更新$\theta_d$</p><p><strong>训练生成器</strong> $G$ （训练 $1$ 次）：<br>每次采样一组噪声变量 $\{z^1,z^2, \dots,z^m\}$，计算 $\theta_g$ 的梯度：</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{\theta}_g} \frac{1}{m} \sum_{i=1}^{m} \log(1 - D(G(\boldsymbol{z}^i)))</script><p>然后使用<strong>梯度下降</strong>算法更新$\theta_g$</p><hr><blockquote><p>Q：<em>为什么判别器要训练k次，而生成器才训练1次？</em><br>A：因为优化生成器D的前提是判别器G要达到最优的状态。如果判别器比较弱的话，那么它将给生成器错误的引导，使得生成器的优化方向不对</p><p>Q：<em>为什么生成器的梯度只有后一项？</em><br>A：因为 $\tilde{V}$ 的前一项与参数 $\theta_g$ 无关</p></blockquote><hr><p>此外在论文中，作者针对生成器的训练提出了一个 “<strong>$-\log(D)$ trick</strong>“：<br><em>将最小化 $\log (1-D(G(z)))$ 改为 最小化 $-\log D(G(z))$</em></p><p>我们先来观察一下二者的函数图像</p><p><img src="https://tuchuang001.com/images/2017/12/19/5.jpg" alt="log(1-D(x))和-log(x)函数图像"></p><p>作者指出，在训练的早期，判别器 $G$ 可以轻易地区分出假样本，使得$D(G(z)) \to 0$。观察图像发现，$\log (1-D(x))$这个函数在 $x \to 0$ 时比较平滑，梯度也比较小，这就会导致生成器 $G$ 的训练变得十分地缓慢。而改为优化 $-\log(D(x))$ 后，在训练的早期能提供比较高的梯度，从而提高了训练速度。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;生成对抗网络&lt;/strong&gt;（Generative Adversarial Nets, GANs）是由 Ian Goodfellow 等人于2014年6月提出的一种生成模型，至今仍是非常热门的研究方向。深度学习领域泰斗级人物 Yann LeCun 曾如此评价道：&lt;em&gt;“GANs and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.”&lt;/em&gt;，从中可见GAN的强大之处！&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="https://orzyt.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="GAN" scheme="https://orzyt.cn/tags/GAN/"/>
    
      <category term="生成对抗网络" scheme="https://orzyt.cn/tags/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>使用TensorFlow实现逻辑回归</title>
    <link href="https://orzyt.cn/posts/use-tensorflow-to-implement-logistic-regression/"/>
    <id>https://orzyt.cn/posts/use-tensorflow-to-implement-logistic-regression/</id>
    <published>2017-12-02T03:31:49.000Z</published>
    <updated>2018-11-10T11:47:16.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h2><blockquote><p>使用TensorFlow实现小批量梯度下降的逻辑回归。<br>数据集：<a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html" target="_blank" rel="noopener"><code>moons dataset</code></a><br><em>— From《Hands-on Machine Learning with Scikit-Learn and TensorFlow》Chapter9 Exercise12</em></p></blockquote><a id="more"></a><hr><p><strong>*附加功能：</strong></p><blockquote><p>• 在<code>logistic_regression()</code>函数中定义计算图，以便复用<br>• 在训练的时候定期保存检查点，并在训练结束的时候保存最终的模型<br>• 若训练中断，则从检查点中恢复<br>• 使用命名域来定义图<br>• 增加summaries日志记录，在TensorBoard中可视化学习曲线<br>•  调参（如，学习率、批数据大小等）并观察学习曲线</p></blockquote><hr><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>首先载入数据集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line">m = <span class="number">1000</span>  <span class="comment"># 样本数量</span></span><br><span class="line"><span class="comment"># 载入数据集</span></span><br><span class="line">X_moons, y_moons = make_moons(m, noise=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure></p><p>接着将数据集可视化，以便有一个直观的感受<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_moons == 1提取正样本的索引</span></span><br><span class="line">plt.plot(X_moons[y_moons == <span class="number">1</span>, <span class="number">0</span>], X_moons[y_moons == <span class="number">1</span>, <span class="number">1</span>], <span class="string">'go'</span>, label=<span class="string">'Positive'</span>)</span><br><span class="line">plt.plot(X_moons[y_moons == <span class="number">0</span>, <span class="number">0</span>], X_moons[y_moons == <span class="number">0</span>, <span class="number">1</span>], <span class="string">'r^'</span>, label=<span class="string">'Negative'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="https://tuchuang001.com/images/2017/12/02/moons_dataset.png" alt="moons dataset"></p><p>为每个样本在第0维上添加<code>bias</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加bias</span></span><br><span class="line">X_moons_with_bias = np.c_[np.ones((m, <span class="number">1</span>)), X_moons]</span><br></pre></td></tr></table></figure></p><p>标签形状需要从<code>(m, )</code>reshape为<code>(m, 1)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据标签从 1-D reshape成 2-D</span></span><br><span class="line">y_moons_column_vector = y_moons.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>从整个数据集中以<code>8:2</code>的比例，划分出训练集和测试集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集占整个数据集的比例</span></span><br><span class="line">test_ratio = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试样本数量</span></span><br><span class="line">test_size = int(m * test_ratio)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集</span></span><br><span class="line">X_train = X_moons_with_bias[:-test_size]</span><br><span class="line">y_train = y_moons_column_vector[:-test_size]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分测试集</span></span><br><span class="line">X_test = X_moons_with_bias[-test_size:]</span><br><span class="line">y_test = y_moons_column_vector[-test_size:]</span><br></pre></td></tr></table></figure></p><p>定义一个随机划分批数据函数，方便后续训练<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_batch</span><span class="params">(X_train, y_train, batch_size)</span>:</span></span><br><span class="line">    <span class="string">''' # 随机划分批数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param X_train: 整个训练集样本</span></span><br><span class="line"><span class="string">    :param y_train: 整个训练集标签</span></span><br><span class="line"><span class="string">    :param batch_size: 每个batch的大小</span></span><br><span class="line"><span class="string">    :return: 样本和标签的批数据</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    rnd_indices = np.random.randint(<span class="number">0</span>, len(X_train), size=batch_size)</span><br><span class="line">    X_batch = X_train[rnd_indices]</span><br><span class="line">    y_batch = y_train[rnd_indices]</span><br><span class="line">    <span class="keyword">return</span> X_batch, y_batch</span><br></pre></td></tr></table></figure></p><hr><h2 id="构造计算图阶段"><a href="#构造计算图阶段" class="headerlink" title="构造计算图阶段"></a>构造计算图阶段</h2><p>moons dataset的特征只有2个<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征数量</span></span><br><span class="line">n_inputs = <span class="number">2</span></span><br></pre></td></tr></table></figure></p><p>构造计算图<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入数据</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n_inputs + <span class="number">1</span>), name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权值初始化</span></span><br><span class="line">theta = tf.Variable(tf.random_uniform([n_inputs + <span class="number">1</span>, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>, seed=<span class="number">42</span>), name=<span class="string">'theta'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算logits</span></span><br><span class="line">logits = tf.matmul(X, theta, name=<span class="string">'logits'</span>)</span><br></pre></td></tr></table></figure></p><p>sigmod函数的计算方式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一（根据定义）</span></span><br><span class="line">y_proba = <span class="number">1</span> / (<span class="number">1</span> + tf.exp(-logits))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式二（内建函数）</span></span><br><span class="line">y_proba = tf.sigmoid(logits)</span><br></pre></td></tr></table></figure></p><p>计算逻辑回归的损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一（根据定义）</span></span><br><span class="line">epsilon = <span class="number">1e-7</span> <span class="comment"># 避免运算溢出</span></span><br><span class="line">loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (<span class="number">1</span> - y) * tf.log(<span class="number">1</span> - y_proba + epsilon), name = <span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式二（内建函数）</span></span><br><span class="line">loss = tf.losses.log_loss(y, y_proba, epsilon=epsilon)</span><br></pre></td></tr></table></figure></p><p>定义学习率、梯度下降优化器及变量初始化节点<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line"><span class="comment"># 训练节点</span></span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量初始化节点</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure></p><p>训练相关的参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练的epoch次数（即遍历epoch次整个数据集）</span></span><br><span class="line">n_epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次批训练的样本数量</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成一次epoch所需要的批训练次数</span></span><br><span class="line">n_batches = int(np.ceil(m / batch_size))</span><br></pre></td></tr></table></figure></p><hr><h2 id="运行计算图阶段"><a href="#运行计算图阶段" class="headerlink" title="运行计算图阶段"></a>运行计算图阶段</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_index <span class="keyword">in</span> range(n_batches):</span><br><span class="line">            <span class="comment"># 获取批数据</span></span><br><span class="line">            X_batch, y_batch = random_batch(X_train, y_train, batch_size)</span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        loss_val = loss.eval(feed_dict=&#123;X: X_test, y: y_test&#125;)</span><br><span class="line">        <span class="comment"># 每训练100个epoch打印当前的loss值</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'\tLoss:'</span>, loss_val)</span><br><span class="line">    <span class="comment"># 在测试集上预测</span></span><br><span class="line">    y_proba_val = y_proba.eval(feed_dict=&#123;X: X_test, y: y_test&#125;)</span><br></pre></td></tr></table></figure><p>将概率大等于0.5的样本预测为正类<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = (y_proba_val &gt;= <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure></p><hr><h2 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h2><p>使用准确率（precision）和召回率（recall）来评价模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准确率</span></span><br><span class="line">p_score = precision_score(y_test, y_pred)</span><br><span class="line"><span class="comment"># 召回率</span></span><br><span class="line">r_score = recall_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Precision score:'</span>, p_score)</span><br><span class="line">print(<span class="string">'Recall score:'</span>, r_score)</span><br></pre></td></tr></table></figure></p><hr><h2 id="可视化预测结果"><a href="#可视化预测结果" class="headerlink" title="可视化预测结果"></a>可视化预测结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred_idx = y_pred.reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X_test[y_pred_idx, <span class="number">1</span>], X_test[y_pred_idx, <span class="number">2</span>], <span class="string">'go'</span>, label=<span class="string">'Positive'</span>)</span><br><span class="line">plt.plot(X_test[~y_pred_idx, <span class="number">1</span>], X_test[~y_pred_idx, <span class="number">2</span>], <span class="string">'r^'</span>, label=<span class="string">'Negative'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang001.com/images/2017/12/02/result1.png" alt="预测结果可视化"></p><hr><h2 id="实现附加功能"><a href="#实现附加功能" class="headerlink" title="实现附加功能"></a>实现附加功能</h2><p>由于逻辑回归是一个线性分类器（从上面可视化的预测结果也可看出），效果不是特别好。<br>因此，我们使用多项式回归，即额外增加4个特征($x_{1}^2$、$x_{2}^2$、$x_{1}^3$、$x_{2}^3$)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加4个特征</span></span><br><span class="line">X_train_enhanced = np.c_[X_train, X_train[:, <span class="number">1</span>] ** <span class="number">2</span>,</span><br><span class="line">                         X_train[:, <span class="number">2</span>] ** <span class="number">2</span>,</span><br><span class="line">                         X_train[:, <span class="number">1</span>] ** <span class="number">3</span>,</span><br><span class="line">                         X_train[:, <span class="number">2</span>] ** <span class="number">3</span>,]</span><br><span class="line">X_test_enhanced = np.c_[X_test,</span><br><span class="line">                        X_test[:, <span class="number">1</span>] ** <span class="number">2</span>,</span><br><span class="line">                        X_test[:, <span class="number">2</span>] ** <span class="number">2</span>,</span><br><span class="line">                        X_test[:, <span class="number">1</span>] ** <span class="number">3</span>,</span><br><span class="line">                        X_test[:, <span class="number">2</span>] ** <span class="number">3</span>,]</span><br></pre></td></tr></table></figure><p>将逻辑回归封装成一个函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">(X, y, initializer=None, seed=<span class="number">42</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">''' 逻辑回归</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param X: 样本</span></span><br><span class="line"><span class="string">    :param y: 标签</span></span><br><span class="line"><span class="string">    :param initializer: 权值初始化器</span></span><br><span class="line"><span class="string">    :param seed: 随机数种子</span></span><br><span class="line"><span class="string">    :param learning_rate: 学习率</span></span><br><span class="line"><span class="string">    :return: sigmod概率, 损失函数, 训练节点, loss日志记录, 保存器</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    n_inputs_with_bias = int(X.get_shape()[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'logistic_regression'</span>): <span class="comment"># 使用命名域</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'model'</span>):</span><br><span class="line">            <span class="keyword">if</span> initializer <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                initializer = tf.random_uniform([n_inputs_with_bias, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>, seed=seed)</span><br><span class="line">            theta = tf.Variable(initializer, name=<span class="string">'theta'</span>)</span><br><span class="line">            logits = tf.matmul(X, theta)</span><br><span class="line">            y_proba = tf.sigmoid(logits)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">            loss = tf.losses.log_loss(y, y_proba, scope=<span class="string">'loss'</span>)</span><br><span class="line">            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">            training_op = optimizer.minimize(loss)</span><br><span class="line">            loss_summary = tf.summary.scalar(<span class="string">'log_loss'</span>, loss)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'init'</span>):</span><br><span class="line">            init = tf.global_variables_initializer()</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'save'</span>):</span><br><span class="line">            saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">return</span> y_proba, loss, training_op, loss_summary, init, saver</span><br></pre></td></tr></table></figure></p><p>构造日志文件目录<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_dir</span><span class="params">(prefix=<span class="string">''</span>)</span>:</span></span><br><span class="line">    now = datetime.utcnow().strftime(<span class="string">'%Y%m%d%H%M%S'</span>)</span><br><span class="line">    root_logdir = <span class="string">'tf_logs'</span></span><br><span class="line">    <span class="keyword">if</span> prefix:</span><br><span class="line">        prefix += <span class="string">'-'</span></span><br><span class="line">    name = prefix + <span class="string">'run-'</span> + now</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'&#123;&#125;/&#123;&#125;/'</span>.format(root_logdir, name)</span><br></pre></td></tr></table></figure></p><p>构造计算图<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征数量，注意额外增加了4个特征</span></span><br><span class="line">n_inputs = <span class="number">2</span> + <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志文件目录</span></span><br><span class="line">logdir = log_dir(<span class="string">'logreg'</span>)</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n_inputs + <span class="number">1</span>), name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 封装好logistic_regression，直接调用</span></span><br><span class="line">y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存计算图结构</span></span><br><span class="line">file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())</span><br></pre></td></tr></table></figure></p><p>运行计算图<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">10001</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">n_batches = int(np.ceil(m / batch_size))</span><br><span class="line"></span><br><span class="line">checkpoint_path = <span class="string">'./tmp/my_logreg_model.ckpt'</span></span><br><span class="line">checkpoint_epoch_path = checkpoint_path + <span class="string">'.epoch'</span></span><br><span class="line">final_model_path = <span class="string">'./my_logreg_model'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 判断checkpoint_epoch_path文件是否存在</span></span><br><span class="line">    <span class="keyword">if</span> os.path.isfile(checkpoint_epoch_path):</span><br><span class="line">        <span class="keyword">with</span> open(checkpoint_epoch_path, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># 文件记录了最后一次保存的epoch编号</span></span><br><span class="line">            start_epoch = int(f.read())</span><br><span class="line">        print(<span class="string">'Training was interrupted. Continuing at epoch'</span>, start_epoch)</span><br><span class="line">        <span class="comment"># 恢复会话</span></span><br><span class="line">        saver.restore(sess, checkpoint_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 重新开始</span></span><br><span class="line">        start_epoch = <span class="number">0</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, n_epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_index <span class="keyword">in</span> range(n_batches):</span><br><span class="line">            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)</span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 计算每个epoch的loss值及其日志记录</span></span><br><span class="line">        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict=&#123;X: X_test_enhanced, y: y_test&#125;)</span><br><span class="line">        <span class="comment"># 追加loss日志记录,注意当前epoch的编号也要记录</span></span><br><span class="line">        file_writer.add_summary(summary_str, epoch)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每500个epoch保存checkpoint</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch:'</span>, epoch, <span class="string">'\tLoss:'</span>, loss_val)</span><br><span class="line">            saver.save(sess, checkpoint_path)</span><br><span class="line">            <span class="comment"># 每次覆盖写入新的epoch编号</span></span><br><span class="line">            <span class="keyword">with</span> open(checkpoint_epoch_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(<span class="string">b'%d'</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 保存最终模型            </span></span><br><span class="line">    saver.save(sess, final_model_path)</span><br><span class="line">    <span class="comment"># 在测试集上进行预测</span></span><br><span class="line">    y_proba_val = y_proba.eval(feed_dict=&#123;X: X_test_enhanced, y: y_test&#125;)</span><br><span class="line">    <span class="comment"># 若训练未中断,则删除checkpoint_epoch_path文件</span></span><br><span class="line">    os.remove(checkpoint_epoch_path)</span><br></pre></td></tr></table></figure></p><p>预测结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = (y_proba_val &gt;= <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure></p><p>输出准确率和召回率<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Precision score:'</span>, precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">'Recall score:'</span>, recall_score(y_test, y_pred))</span><br></pre></td></tr></table></figure></p><p>可视化预测结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred_idx = y_pred.reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(X_test[y_pred_idx, <span class="number">1</span>], X_test[y_pred_idx, <span class="number">2</span>], <span class="string">'go'</span>, label=<span class="string">'Positive'</span>)</span><br><span class="line">plt.plot(X_test[~y_pred_idx, <span class="number">1</span>], X_test[~y_pred_idx, <span class="number">2</span>], <span class="string">'r^'</span>, label=<span class="string">'Negative'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="https://tuchuang001.com/images/2017/12/02/result2.png" alt="可视化预测结果"></p><p>可以看出，增加额外4个特征，能够显著提高预测结果</p><hr><p>开始对<code>learning rate</code>和<code>batch size</code>进行玄学调参…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> reciprocal</span><br><span class="line"></span><br><span class="line">n_search_iterations = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> search_iteration <span class="keyword">in</span> range(n_search_iterations):</span><br><span class="line">    batch_size = np.random.randint(<span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">    <span class="comment"># reciprocal为倒数分布</span></span><br><span class="line">    <span class="comment"># 详见https://en.wikipedia.org/wiki/Reciprocal_distribution</span></span><br><span class="line">    <span class="comment"># 一般来说，如果对超参数的最优量级没把握的话，可以使用该分布进行调参</span></span><br><span class="line">    learning_rate = reciprocal.rvs(<span class="number">0.0001</span>, <span class="number">0.1</span>, random_state=search_iteration)</span><br><span class="line">    </span><br><span class="line">    n_inputs = <span class="number">2</span> + <span class="number">4</span></span><br><span class="line">    logdir = log_dir(<span class="string">'logdir'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'Iteration'</span>, search_iteration)</span><br><span class="line">    print(<span class="string">'  logdir:'</span>, logdir)</span><br><span class="line">    print(<span class="string">'  batch size:'</span>, batch_size)</span><br><span class="line">    print(<span class="string">'  learning rate:'</span>, learning_rate)</span><br><span class="line">    print(<span class="string">'  training: '</span>, end=<span class="string">''</span>)</span><br><span class="line">    </span><br><span class="line">    X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n_inputs + <span class="number">1</span>), name=<span class="string">'X'</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'y'</span>)</span><br><span class="line">    </span><br><span class="line">    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y, learning_rate=learning_rate)</span><br><span class="line">    </span><br><span class="line">    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())</span><br><span class="line">    </span><br><span class="line">    n_epochs = <span class="number">10001</span></span><br><span class="line">    n_batches = int(np.ceil(m / batch_size))</span><br><span class="line">    </span><br><span class="line">    final_model_path = <span class="string">'./model/my_logreg_model_%d'</span> % search_iteration</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">            <span class="keyword">for</span> batch_index <span class="keyword">in</span> range(n_batches):</span><br><span class="line">                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)</span><br><span class="line">                sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict=&#123;X: X_test_enhanced, y: y_test&#125;)    </span><br><span class="line">            file_writer.add_summary(summary_str, epoch)</span><br><span class="line">            <span class="keyword">if</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'.'</span>, end=<span class="string">''</span>)</span><br><span class="line">        print()</span><br><span class="line">        </span><br><span class="line">        saver.save(sess, final_model_path)</span><br><span class="line">        </span><br><span class="line">        y_proba_val = y_proba.eval(feed_dict=&#123;X: X_test_enhanced, y: y_test&#125;)</span><br><span class="line">        y_pred = (y_proba_val &gt;= <span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'  Precision:'</span>, precision_score(y_test, y_pred))</span><br><span class="line">        print(<span class="string">'  Recall:'</span>, recall_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><p>打印训练信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">Iteration 0</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202101244/</span><br><span class="line">  batch size: 54</span><br><span class="line">  learning rate: 0.00443037524522</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.979797979798</span><br><span class="line">  Recall: 0.979797979798</span><br><span class="line">Iteration 1</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202101623/</span><br><span class="line">  batch size: 22</span><br><span class="line">  learning rate: 0.00178264971514</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.979797979798</span><br><span class="line">  Recall: 0.979797979798</span><br><span class="line">Iteration 2</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202102501/</span><br><span class="line">  batch size: 74</span><br><span class="line">  learning rate: 0.00203228544324</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.969696969697</span><br><span class="line">  Recall: 0.969696969697</span><br><span class="line">Iteration 3</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202102742/</span><br><span class="line">  batch size: 58</span><br><span class="line">  learning rate: 0.00449152382514</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.979797979798</span><br><span class="line">  Recall: 0.979797979798</span><br><span class="line">Iteration 4</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202103106/</span><br><span class="line">  batch size: 61</span><br><span class="line">  learning rate: 0.0796323472178</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.980198019802</span><br><span class="line">  Recall: 1.0</span><br><span class="line">Iteration 5</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202103417/</span><br><span class="line">  batch size: 92</span><br><span class="line">  learning rate: 0.000463425058329</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.912621359223</span><br><span class="line">  Recall: 0.949494949495</span><br><span class="line">Iteration 6</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202103630/</span><br><span class="line">  batch size: 74</span><br><span class="line">  learning rate: 0.0477068184194</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.98</span><br><span class="line">  Recall: 0.989898989899</span><br><span class="line">Iteration 7</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202103916/</span><br><span class="line">  batch size: 58</span><br><span class="line">  learning rate: 0.000169404470952</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.9</span><br><span class="line">  Recall: 0.909090909091</span><br><span class="line">Iteration 8</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202104242/</span><br><span class="line">  batch size: 61</span><br><span class="line">  learning rate: 0.0417146119941</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.980198019802</span><br><span class="line">  Recall: 1.0</span><br><span class="line">Iteration 9</span><br><span class="line">  logdir: tf_logs/logdir-run-20171202104602/</span><br><span class="line">  batch size: 92</span><br><span class="line">  learning rate: 0.000107429229684</span><br><span class="line">  training: .....................</span><br><span class="line">  Precision: 0.882352941176</span><br><span class="line">  Recall: 0.757575757576</span><br></pre></td></tr></table></figure></p><p>  让我们打开<code>TensorBoard</code>观察10次训练的学习曲线</p><p>  <img src="https://tuchuang001.com/images/2017/12/02/result3.png" alt="10次训练的学习曲线"></p><p>  可以看出，第4次（从0开始）的<code>loss</code>值最小<br>  最终，找到的超参数为</p><div class="table-container"><table><thead><tr><th style="text-align:center">超参数</th><th style="text-align:center">取值</th></tr></thead><tbody><tr><td style="text-align:center">learning rate</td><td style="text-align:center">0.0796323472178</td></tr><tr><td style="text-align:center">batch size</td><td style="text-align:center">61</td></tr></tbody></table></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目大意&quot;&gt;&lt;a href=&quot;#题目大意&quot; class=&quot;headerlink&quot; title=&quot;题目大意&quot;&gt;&lt;/a&gt;题目大意&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;使用TensorFlow实现小批量梯度下降的逻辑回归。&lt;br&gt;数据集：&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;moons dataset&lt;/code&gt;&lt;/a&gt;&lt;br&gt;&lt;em&gt;— From《Hands-on Machine Learning with Scikit-Learn and TensorFlow》Chapter9 Exercise12&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="https://orzyt.cn/categories/TensorFlow/"/>
    
    
      <category term="机器学习" scheme="https://orzyt.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="hands-on ML" scheme="https://orzyt.cn/tags/hands-on-ML/"/>
    
      <category term="逻辑回归" scheme="https://orzyt.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="TensorFlow" scheme="https://orzyt.cn/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Hands-on Machine learning 之 TensorFLow入门</title>
    <link href="https://orzyt.cn/posts/hands-on-ml-up-and-running-tensorflow/"/>
    <id>https://orzyt.cn/posts/hands-on-ml-up-and-running-tensorflow/</id>
    <published>2017-12-01T16:00:00.000Z</published>
    <updated>2018-11-10T11:47:16.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><img src="https://blog.rescale.com/wp-content/uploads/2017/02/markblogtensorflow.png" alt=""></p><p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener"><strong>TensorFlow</strong></a>是Google在2015年11月份开源的人工智能系统，是之前所开发的深度学习基础架构<code>DistBelief</code>的改进版本，该系统可以被用于语音识别、图像识别等多个领域。本文将介绍<code>TensorFlow</code>的基本概念和常见用法。</p><a id="more"></a><hr><h2 id="创建图并在会话中运行"><a href="#创建图并在会话中运行" class="headerlink" title="创建图并在会话中运行"></a>创建图并在会话中运行</h2><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_036.png" alt="一个简单的数据流图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入tensorflow</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量x，y及计算节点f</span></span><br><span class="line">x = tf.Variable(<span class="number">3</span>, name=<span class="string">'x'</span>)</span><br><span class="line">y = tf.Variable(<span class="number">4</span>, name=<span class="string">'y'</span>)</span><br><span class="line">f = x * x * y + y + <span class="number">2</span></span><br></pre></td></tr></table></figure><p>值得注意的是，上述代码并没有进行任何的运算，仅仅是创建了一个<code>计算图</code>（computation graph）而已。实际上，连变量都还没有被初始化。</p><p>为了对该计算图进行运算，我们必须创建一个<code>会话</code>（session）。然后在会话中初始化变量，以及计算<code>f</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session() <span class="comment"># 创建会话</span></span><br><span class="line">sess.run(x.initializer) <span class="comment"># 初始化变量x</span></span><br><span class="line">sess.run(y.initializer) <span class="comment"># 初始化变量y</span></span><br><span class="line">result = sess.run(f) <span class="comment"># 计算f</span></span><br><span class="line">print(result) <span class="comment"># 打印结果</span></span><br><span class="line">&gt;&gt; <span class="number">42</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.close() <span class="comment"># 关闭会话</span></span><br></pre></td></tr></table></figure><p>重复使用<code>sess.run(...)</code>可能有点繁琐，有一个更好的方式是使用python的<a href="https://docs.python.org/3/reference/compound_stmts.html#the-with-statement" target="_blank" rel="noopener"><code>with</code></a>语句。</p><p>在<code>with</code>语句块开始时，创建的会话会成为计算图的默认会话。在语句块结束时，创建的会话也会自动结束。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个会话并命名为sess</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x.initializer.run() <span class="comment"># 等同于 sess.run(x.initializer)</span></span><br><span class="line">    y.initializer.run() <span class="comment"># 等同于 sess.run(y.initializer)</span></span><br><span class="line">    result = f.eval() <span class="comment"># result = sess.run(f)</span></span><br></pre></td></tr></table></figure><p>除了手动初始化每个变量，也可以使用<code>global_variables_initializer()</code>函数初始化所有变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意，并没有立即将变量初始化，而是创建一个初始化节点</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run() <span class="comment"># 在这里才真正初始化</span></span><br><span class="line">    result = f.eval()</span><br></pre></td></tr></table></figure><hr><h2 id="管理计算图"><a href="#管理计算图" class="headerlink" title="管理计算图"></a>管理计算图</h2><p>所有创建的<code>节点</code>（node）都会自动添加进默认图中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 判断变量x1所在的图是否是默认图</span></span><br><span class="line">x1.graph <span class="keyword">is</span> tf.get_default_graph()</span><br><span class="line">&gt;&gt; <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>但是，有的时候需要管理多个独立的图。我们便可以创建一个临时的图，并在<code>with</code>语句块内将其设置为默认图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph = tf.Graph() <span class="comment"># 创建图</span></span><br><span class="line"><span class="comment"># 在with内将graph设置为默认图</span></span><br><span class="line"><span class="keyword">with</span> graph.as_default(): </span><br><span class="line">    <span class="comment"># 此时创建的变量应该在图graph里</span></span><br><span class="line">    x2 = tf.Variable(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断x2所在的图，是不是真的在graph里</span></span><br><span class="line">x2.graph <span class="keyword">is</span> graph</span><br><span class="line">&gt;&gt; <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同样，判断x2是不是在全局的默认图里(显然不是的)</span></span><br><span class="line">x2.graph <span class="keyword">is</span> tf.get_default_graph()</span><br><span class="line">&gt;&gt; <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>若想要将默认图重置（删除图中所有的节点），可以使用<code>tf.reset_default_graph()</code>函数。</p><hr><h2 id="节点的生命周期"><a href="#节点的生命周期" class="headerlink" title="节点的生命周期"></a>节点的生命周期</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个简单的图</span></span><br><span class="line">w = tf.constant(<span class="number">3</span>)</span><br><span class="line">x = w + <span class="number">2</span></span><br><span class="line">y = x + <span class="number">5</span></span><br><span class="line">z = x * <span class="number">3</span></span><br><span class="line"><span class="comment"># 在会话中计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(y.eval()) <span class="comment"># 10</span></span><br><span class="line">    print(z.eval()) <span class="comment"># 15</span></span><br></pre></td></tr></table></figure><p>在会话中，为了计算<code>y</code>，会自动检测出<code>y</code>依赖于<code>x</code>，而<code>x</code>又依赖于<code>w</code>。那么，将会依次计算<code>w</code>和<code>x</code>，最后再计算<code>y</code>。而为了计算<code>z</code>，也同样会依次计算<code>w</code>和<code>x</code>。</p><p>注意在此过程中，<code>w</code>和<code>x</code>的值<strong>并不会被重复使用</strong>！也就是说，上述代码总共对<code>w</code>和<code>x</code>计算了两次(即使两次的结果都是一样的)。</p><p>因此，对于同一张图的多次运算，除了<code>tf.Variable()</code>变量外，其他节点的值在一次运行结束后都会被丢弃，不会被重复使用。</p><div class="table-container"><table><thead><tr><th style="text-align:center">节点类型</th><th style="text-align:center">生命周期</th></tr></thead><tbody><tr><td style="text-align:center"><code>tf.Variable()</code></td><td style="text-align:center">整个会话</td></tr><tr><td style="text-align:center"><code>others</code></td><td style="text-align:center">会话的某次运行</td></tr></tbody></table></div><p>为了高效地求得<code>y</code>和<code>z</code>的值，可以在会话的一次运行内同时计算它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 同时计算y和z，此时w和x只计算一次</span></span><br><span class="line">    y_val, z_val = sess.run([y, z])</span><br><span class="line">    print(y_val) <span class="comment"># 10</span></span><br><span class="line">    print(z_val) <span class="comment"># 15</span></span><br></pre></td></tr></table></figure><hr><h2 id="使用TensorFlow进行线性回归"><a href="#使用TensorFlow进行线性回归" class="headerlink" title="使用TensorFlow进行线性回归"></a>使用TensorFlow进行线性回归</h2><p>TensorFlow的操作（operations，记作ops）可以接收任意多个输入，可以产生任意多个输出。</p><p>比如<code>addition</code>和<code>multiplication</code>可以接受2个输入，产生1个输出。<br>而被成为<strong>源操作</strong>（source ops）的<code>Constants</code>和<code>variables</code>，则没有输入。</p><p>其中，输入和输出都是多维数组，称之为<strong>张量</strong>（tensor）。</p><hr><p>接下来，将使用<a href="http://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">sklearn</a>的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html" target="_blank" rel="noopener">加利福尼亚房屋数据</a>来进行线性回归。</p><p>对于线性回归参数<code>theta</code>的拟合，将使用正规方程（Normal Equation）计算：$\theta = (X^T X)^{-1}X^T y$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入sklearn自带的加利福尼亚房屋数据</span></span><br><span class="line">housing = fetch_california_housing()</span><br><span class="line"><span class="comment"># 样本数及特征数</span></span><br><span class="line">m, n = housing.data.shape</span><br><span class="line"><span class="comment"># 添加bias</span></span><br><span class="line">housing_data_plus_bias = np.c_[np.ones((m, <span class="number">1</span>)), housing.data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的常量节点X和y，分别用来存放样本和标签</span></span><br><span class="line">X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.constant(housing.target.reshape(<span class="number">-1</span>, <span class="number">1</span>), dtype=tf.float32, name=<span class="string">'y'</span>)</span><br><span class="line"><span class="comment"># 计算X的转置</span></span><br><span class="line">XT = tf.transpose(X)</span><br><span class="line"><span class="comment"># 使用正规方程计算theta</span></span><br><span class="line">theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次注意，上述代码并没有进行实际的运算，只是在构建计算图而已</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 在会话中计算theta的值</span></span><br><span class="line">    theta_value = theta.eval()</span><br></pre></td></tr></table></figure><hr><h2 id="实现梯度下降"><a href="#实现梯度下降" class="headerlink" title="实现梯度下降"></a>实现梯度下降</h2><p>接下来将使用<strong>批梯度下降</strong>（Batch Gradient Descent）方法来进行线性回归参数的拟合。</p><p>使用梯度下降方法一般要先对特征进行<strong>标准化</strong>（normalize，即减均值，除方差）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaled_housing_data = scaler.fit_transform(housing.data)</span><br><span class="line">scaled_housing_data_plus_bias = np.c_[np.ones((m, <span class="number">1</span>)), scaled_housing_data]</span><br></pre></td></tr></table></figure><hr><h3 id="手动计算梯度"><a href="#手动计算梯度" class="headerlink" title="手动计算梯度"></a>手动计算梯度</h3><p>$\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})$<br>其中，$\alpha$是学习率，$m$是批样本数量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">1000</span> <span class="comment"># 遍历1000次数据集</span></span><br><span class="line">learning_rate = <span class="number">0.01</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的常量节点X和y，分别用来存放样本和标签</span></span><br><span class="line"><span class="comment"># 特征已经过normalize，并加上了bias</span></span><br><span class="line">X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.constant(housing.target.reshape(<span class="number">-1</span>, <span class="number">1</span>), dtype=tf.float32, name=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的变量节点theta，用来存放待求解的参数（使用均匀分布初始化节点）</span></span><br><span class="line">theta = tf.Variable(tf.random_uniform([n + <span class="number">1</span>, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'theta'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建预测节点</span></span><br><span class="line">y_pred = tf.matmul(X, theta, name=<span class="string">'predictions'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建误差节点</span></span><br><span class="line">error = y_pred - y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数节点，使用均方根误差（mean square error）</span></span><br><span class="line">mse = tf.reduce_mean(tf.square(error), name=<span class="string">'mse'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建梯度计算节点</span></span><br><span class="line">gradients = <span class="number">1</span> / m * tf.matmul(tf.transpose(X), error)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建赋值节点，即 theta = theta - learning_rate * gradients</span></span><br><span class="line">training_op = tf.assign(theta, theta - learning_rate * gradients)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量初始化节点</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="comment"># 每100个epoch打印当前的loss值</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch'</span>, epoch, <span class="string">'MSE='</span>, mse.eval())</span><br><span class="line">        <span class="comment"># 进行梯度下降更新参数theta</span></span><br><span class="line">        sess.run(training_op)</span><br><span class="line">    <span class="comment"># 训练完成后，计算最终的theta参数值</span></span><br><span class="line">    best_theta = theta.eval()</span><br></pre></td></tr></table></figure><hr><h3 id="使用autodiff"><a href="#使用autodiff" class="headerlink" title="使用autodiff"></a>使用autodiff</h3><p>在前面的代码中，需要我们事先手动计算好loss function（MSE）的梯度才能进行训练。<br>虽然在线性回归里面求解梯度还不算复杂，但是对于深度神经网络来说，梯度的求解将会让人十分头疼。</p><p>在TensorFlow中，提供了<code>autodiff</code>能够帮助我们自动计算梯度。</p><p>只需将之前的梯度计算替换为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradients = tf.gradients(mse, [theta])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><p><code>gradients()</code>函数接受一个操作节点（如，<code>mse</code>损失函数计算节点），以及一系列需要求解梯度的变量（如，<code>theta</code>），最终返回对应的梯度列表。</p><hr><h3 id="使用优化器"><a href="#使用优化器" class="headerlink" title="使用优化器"></a>使用优化器</h3><p>TensorFlow能够自动计算梯度已经很方便了，但是可以将事情变得更加简单——使用优化器。</p><p>比如，使用<strong>梯度下降优化器</strong>（Gardient Descent optimizer）。</p><p>便可以简单地将之前<code>gradients = ...</code>和<code>training_op = ...</code>直接替换成下列方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(mse)</span><br></pre></td></tr></table></figure><hr><h2 id="为训练算法提供数据"><a href="#为训练算法提供数据" class="headerlink" title="为训练算法提供数据"></a>为训练算法提供数据</h2><p>首先，我们将之前的批梯度下降算法改为<strong>小批量梯度下降算法</strong>（Mini-batch Gradient Descent）。<br>那么对于每个batch，我们都需要不断地替换<code>X</code>和<code>y</code>常量节点，来向训练算法提供数据。</p><p>在TensorFlow中，一个典型的做法是使用<code>placeholder</code><strong>占位符节点</strong>。</p><p>占位符节点在创建的时候，只需指定其存放的数据类型（如，floa32等），以及存放的数据维度大小即可。<br>然后，等到训练运行时才真正往占位符里放数据（使用<code>feed_dict</code>参数放数据）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># None 意味着在该维度不限制大小</span></span><br><span class="line">A = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">3</span>))</span><br><span class="line">B = A + <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 由于B依赖于A，因此在对B求值的时候，必须先用feed_dict向占位符A提供数据</span></span><br><span class="line">    B_val_1 = B.eval(feed_dict=&#123;A: [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]&#125;)</span><br><span class="line">    <span class="comment"># 占位符A的第0维度可以是任意大小</span></span><br><span class="line">    B_val_2 = B.eval(feed_dict=&#123;A: [[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(B_val_1)</span><br><span class="line">&gt;&gt; [[ <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>]]</span><br><span class="line"></span><br><span class="line">print(B_val_2)</span><br><span class="line">&gt;&gt; [[  <span class="number">9.</span>  <span class="number">10.</span>  <span class="number">11.</span>]</span><br><span class="line">    [ <span class="number">12.</span>  <span class="number">13.</span>  <span class="number">14.</span>]]</span><br></pre></td></tr></table></figure><hr><p>使用<code>placeholder</code>实现小批量梯度下降算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">10</span> <span class="comment"># 遍历10次数据集</span></span><br><span class="line">learning_rate = <span class="number">0.01</span> <span class="comment"># 学习率</span></span><br><span class="line">batch_size = <span class="number">100</span> <span class="comment"># 批数据大小</span></span><br><span class="line">n_batches = int(np.ceil(m / batch_size)) <span class="comment"># 完成一次epoch所需要的批次数</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_batch</span><span class="params">(epoch, batch_index, batch_size)</span>:</span></span><br><span class="line">    <span class="string">''' 获取批数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param epoch: 当前epoch的次数</span></span><br><span class="line"><span class="string">    :param batch_index: 当前batch的序号</span></span><br><span class="line"><span class="string">    :param batch_size: 每个batch的大小</span></span><br><span class="line"><span class="string">    :return: 样本和标签的批数据</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># seed的种子设为：当前batch的总序号</span></span><br><span class="line">    <span class="comment"># 比如，完成一个epoch需要5个batch</span></span><br><span class="line">    <span class="comment"># 当前是第3个epoch的第2个batch</span></span><br><span class="line">    <span class="comment"># 那么这个batch总序号就是3 × 5 + 2 = 17</span></span><br><span class="line">    np.random.seed(epoch * n_batches + batch_index)</span><br><span class="line">    <span class="comment"># 在[0, m)范围内，产生batch_size个索引</span></span><br><span class="line">    indices = np.random.randint(m, size=batch_size)</span><br><span class="line">    <span class="comment"># 根据索引，获取样本的批数据</span></span><br><span class="line">    X_batch = scaled_housing_data_plus_bias[indices]</span><br><span class="line">    <span class="comment"># 根据索引，获取标签的批数据</span></span><br><span class="line">    y_batch = housing.target.reshape(<span class="number">-1</span>, <span class="number">1</span>)[indices]</span><br><span class="line">    <span class="keyword">return</span> X_batch, y_batch</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建TensorFlow的占位符节点X和y，分别用来存放样本和标签</span></span><br><span class="line"><span class="comment"># 第0维度为None则不指定大小，因为样本数需要在运行时确定</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n + <span class="number">1</span>), name=<span class="string">'X'</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorFlow的变量节点theta，用来存放待求解的参数（使用均匀分布初始化节点）</span></span><br><span class="line">theta = tf.Variable(tf.random_uniform([n + <span class="number">1</span>, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>, seed=<span class="number">42</span>), name=<span class="string">'thete'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建预测节点</span></span><br><span class="line">y_pred = tf.matmul(X, theta, name=<span class="string">'predictions'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建误差节点</span></span><br><span class="line">error = y_pred - y</span><br><span class="line"></span><br><span class="line">＃ 创建损失函数节点，使用均方根误差（mean square error）</span><br><span class="line">mse = tf.reduce_mean(tf.square(error), name=<span class="string">'mse'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建梯度下降优化器节点 及 对应的训练节点</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量初始化节点</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init) </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_index <span class="keyword">in</span> range(n_batches):</span><br><span class="line">            <span class="comment"># 获取批数据</span></span><br><span class="line">            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)</span><br><span class="line">            <span class="comment"># 由于训练节点依赖于X和y，因此使用feed_dict参数传送一个字典，分别为X和y提供数据</span></span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">    best_theta = theta.eval()</span><br></pre></td></tr></table></figure><hr><h2 id="模型的存储及读取"><a href="#模型的存储及读取" class="headerlink" title="模型的存储及读取"></a>模型的存储及读取</h2><p>模型一旦训练好，那么应该将相关的信息（如，参数、计算图等）存储在硬盘里，方便以后的使用。<br>同样，在模型的训练过程中，应该定时地保存检查点（checkpoint）。这使得模型训练中断之后，可以直接从最后一次检查点开始训练而不是重头开始。</p><hr><h3 id="存储模型"><a href="#存储模型" class="headerlink" title="存储模型"></a>存储模型</h3><p>只需在计算图<strong>构造阶段</strong>（construction phase）的最后新建一个<code>Saver</code>保存节点即可。<br>然后在<strong>执行阶段</strong>（execution phase），调用其<code>save(sess, path)</code>方法即可将模型保存到<code>path</code>路径下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[...]</span><br><span class="line">theta = tf.Variable(tf.random_uniform([n + <span class="number">1</span>, <span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">"theta"</span>)</span><br><span class="line">[...]</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 在construction phase之后创建Saver node即可</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 每100个epoch保存checkpoint</span></span><br><span class="line">            save_path = saver.save(sess, <span class="string">'tmp/my_model.ckpt'</span>)</span><br><span class="line">        sess.run(training_op)</span><br><span class="line">    best_theta = theta.eval()</span><br><span class="line">    <span class="comment"># 训练完毕，保存最终模型</span></span><br><span class="line">    save_path = saver.save(sess, <span class="string">'tmp/my_model_final.ckpt'</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h3><p>首先，还是在构造阶段的最后新建一个<code>Saver</code>保存节点。<br>然后，在执行阶段的开始，调用其<code>restore(sess, path)</code>方法即可将path路径下的模型读取到当前的会话中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'tmp/my_model_final.ckpt'</span>)</span><br><span class="line">    [...]</span><br></pre></td></tr></table></figure><hr><p><code>Saver</code>默认情况下，会存储和读取模型的所有参数。<br>不过，我们也可以指定只保存哪些变量，以及使用什么名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只保存theta变量，命名为weights</span></span><br><span class="line">saver = tf.train.Saver(&#123;<span class="string">"weights"</span>: theta&#125;)</span><br></pre></td></tr></table></figure><p><code>Saver</code>默认情况下，也会存储计算图的结构，保存在路径的<code>*.meta</code>文件中。<br>如果需要读取模型的计算图，可以调用<code>tf.train.import_meta_graph()</code>函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取模型的计算图</span></span><br><span class="line"><span class="comment"># 这样可以完整地恢复模型，不仅包括模型的参数，还包括模型的计算图结构</span></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"/tmp/my_model_final.ckpt.meta"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"/tmp/my_model_final.ckpt"</span>)</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><hr><h2 id="使用TensorBoard进行可视化"><a href="#使用TensorBoard进行可视化" class="headerlink" title="使用TensorBoard进行可视化"></a>使用TensorBoard进行可视化</h2><p>在此之前，我们都是用<code>print</code>函数打印出训练过程。然而，有一个更好的选择是：使用<code>TensorBoard</code>！</p><p>接下来，我们将对线性回归的loss值进行可视化。</p><hr><p>首先，新建一个存放数据的日志目录（使用时间戳作为目录名）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以指定格式获取当前时间</span></span><br><span class="line">now = datetime.utcnow().strftime(<span class="string">'%Y%m%d%H%M%S'</span>)</span><br><span class="line"><span class="comment"># 根目录</span></span><br><span class="line">root_logdir = <span class="string">'tf_logs'</span></span><br><span class="line"><span class="comment"># 日志文件目录</span></span><br><span class="line">logdir = <span class="string">'&#123;&#125;/run-&#123;&#125;/'</span>.format(root_logdir, now)</span><br></pre></td></tr></table></figure><p>其次，在构造阶段的末尾添加以下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># summary是TensorBoard的一种二进制日志字符串</span></span><br><span class="line"><span class="comment"># 我们使用它的scalar标量类型（还有其他类型，如tf.summary.image可以可视化图像）</span></span><br><span class="line"><span class="comment"># 参数'MSE'：可视化时变量的名称</span></span><br><span class="line"><span class="comment"># 参数mse：loss function节点</span></span><br><span class="line">mse_summary = tf.summary.scalar(<span class="string">'MSE'</span>, mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FileWriter可以将summaries写进指定的日志文件中</span></span><br><span class="line"><span class="comment"># 参数logdir：指定的日志文件路径</span></span><br><span class="line"><span class="comment"># 参数tf.get_default_graph()：需要可视化的计算图结构</span></span><br><span class="line">file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())</span><br></pre></td></tr></table></figure><p>最后，在执行阶段添加以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[...]</span><br><span class="line"><span class="keyword">for</span> batch_index <span class="keyword">in</span> range(n_batches):</span><br><span class="line">    X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)</span><br><span class="line">    <span class="keyword">if</span> batch_index % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 计算mse_summary的值</span></span><br><span class="line">        summary_str = mse_summary.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        step = epoch * n_batches + batch_index</span><br><span class="line">        <span class="comment"># 将summary添加到日志文件中，同时需要指定当前的step（也就是可视化时的横轴坐标）</span></span><br><span class="line">        file_writer.add_summary(summary_str, step)</span><br><span class="line">    sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">[...]</span><br><span class="line"><span class="comment"># 关闭FileWriter</span></span><br><span class="line">file_writer.close()</span><br></pre></td></tr></table></figure><p>现在，让我们启动<code>TensorBoard</code>！</p><p>首先，在终端中输入:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ tensorboard --logdir tf_logs/</span><br><span class="line">&gt;&gt; Starting TensorBoard on port 6006</span><br><span class="line">   (You can navigate to http://0.0.0.0:6006)</span><br></pre></td></tr></table></figure></p><p>然后，在浏览器中打开<code>http://0.0.0.0:6006/</code> (或<code>http://localhost:6006/</code>)，即可访问TensorBoard！</p><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_037.png" alt="可视化loss值"></p><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_038.png" alt="可视化计算图"></p><hr><h2 id="命名域"><a href="#命名域" class="headerlink" title="命名域"></a>命名域</h2><p>当处理复杂模型的时候，图中可能有成千上万个节点。因此，非常有必要将相关的节点组织起来放到一起。<br>这就需要TensorFlow中的<strong>命名域</strong>（Name Scopes）来管理节点！</p><p>比如，我们可以将之前代码里的<code>error</code>和<code>mse</code>操作节点放在一个名叫<code>loss</code>的命名域里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    error = y_pred - y</span><br><span class="line">    mse = tf.reduce_mean(tf.square(error), name=<span class="string">'mse'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(error.op.name)</span><br><span class="line">&gt;&gt; loss/sub</span><br><span class="line">print(mse.op.name)</span><br><span class="line">&gt;&gt; loss/mse</span><br></pre></td></tr></table></figure><p>在TensorBoard中，error和mse将出现在loss命名域内。</p><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_039.png" alt="TensorBoard中的命名域"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a1 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line">a2 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"param"</span>): </span><br><span class="line">    a3 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"param"</span>):     </span><br><span class="line">    a4 = tf.Variable(<span class="number">0</span>, name=<span class="string">"a"</span>)</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> (a1, a2, a3, a4):</span><br><span class="line">    print(node.op.name)</span><br><span class="line">    </span><br><span class="line">&gt;&gt; a</span><br><span class="line">   a_1</span><br><span class="line">   param/a</span><br><span class="line">   param_1/a</span><br></pre></td></tr></table></figure><hr><h2 id="模块化"><a href="#模块化" class="headerlink" title="模块化"></a>模块化</h2><p>假设现在要对多个<code>ReLU</code>（rectified linear units，修正线性单元）输出进行累加。</p><script type="math/tex; mode=display">ReLU_{x,b}(X) = max(X \cdot w + b, 0 )</script><p>由于要计算多次ReLU，所以基于模块化的思想，我们可以将实现ReLU功能的语句单独封装成一个函数以供调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">''' 实现ReLU</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param X: 输入样本</span></span><br><span class="line"><span class="string">    :return: 经过ReLU修正后的输出</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w_shape = (int(X.get_shape()[<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">    w = tf.Variable(tf.random_normal(w_shape), name=<span class="string">'weights'</span>)</span><br><span class="line">    b = tf.Variable(<span class="number">0.0</span>, name=<span class="string">'bias'</span>)</span><br><span class="line">    z = tf.add(tf.matmul(X, w), b, name=<span class="string">'z'</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.maximum(z, <span class="number">0.0</span>, name=<span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_features = <span class="number">3</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n_features), name=<span class="string">'X'</span>)</span><br><span class="line"><span class="comment"># 调用5次relu函数</span></span><br><span class="line">relus = [relu(X) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line"><span class="comment"># 将5次的结果累加</span></span><br><span class="line">output = tf.add_n(relus, name=<span class="string">'output'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_040.png" alt="ReLU的计算图(good)"></p><hr><p>还可以做得更好…</p><p>我们将之前讲过的命名域加进来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">''' 实现ReLU</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :param X: 输入样本</span></span><br><span class="line"><span class="string">    :return: 经过ReLU修正后的输出</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'relu'</span>):</span><br><span class="line">        [...]</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_041.png" alt="ReLU的计算图(better)"></p><hr><h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>如果你想让计算图中不同的部分共享一个变量（比如CNN中卷积核的权值），一个可能的操作是将变量作为参数传递过去。但是，当计算图中需要共享的变量非常多时，将变得十分麻烦。</p><p>TensorFlow有一个更好的解决方案是，使用<code>get_variable()</code>函数来创建（或复用）共享变量。<br>而选择创建还是选择复用则是由当前的变量域<code>variable_scope()</code>决定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在relu变量域内创建threshold变量</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>):</span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>, shape=(), initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复用，设置reuse=True</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 另一种复用方式</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    scope.reuse_variables() <span class="comment"># 调用scope的reuse_variables方法</span></span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="复用ReLU的阈值"><a href="#复用ReLU的阈值" class="headerlink" title="复用ReLU的阈值"></a>复用ReLU的阈值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="comment"># 存在则复用，不存在则创建</span></span><br><span class="line">    threshold = tf.get_variable(<span class="string">'threshold'</span>, shape=(), initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    [...]                   </span><br><span class="line">    <span class="keyword">return</span> tf.maximum(z, threshold, name=<span class="string">"max"</span>)</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, n_features), name=<span class="string">'X'</span>)</span><br><span class="line">relus = []</span><br><span class="line"><span class="keyword">for</span> relu_index <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="comment"># 第一次调用relu时reuse为0，则不复用选择创建变量</span></span><br><span class="line">    <span class="comment"># 后续调用relu则会选择复用变量</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'relu'</span>, reuse=(relu_index &gt;= <span class="number">1</span>)) <span class="keyword">as</span> scope:</span><br><span class="line">        relus.append(relu(X))</span><br><span class="line">output = tf.add_n(relus, name=<span class="string">'output'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://tuchuang001.com/images/2017/12/02/Selection_042.png" alt="5个ReLUs共享threshold变量"></p><hr><h3 id="复用CNN卷积层参数"><a href="#复用CNN卷积层参数" class="headerlink" title="复用CNN卷积层参数"></a>复用CNN卷积层参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_images = tf.placeholder(tf.float32, shape = (<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义了一层卷积神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></span><br><span class="line">    <span class="comment"># 创建名为weights的变量</span></span><br><span class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape, initializer=tf.random_normal_initializer())</span><br><span class="line">    <span class="comment"># 创建名为biases的变量</span></span><br><span class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape, initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    conv = tf.nn.conv2d(input, weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</span><br><span class="line">        <span class="comment"># 在名为conv1的variable scope下调用一层神经网络，对应的参数名为</span></span><br><span class="line">        <span class="comment"># "conv1/weights", "conv1/biases"</span></span><br><span class="line">        relu1 = conv_relu(input_images, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</span><br><span class="line">        <span class="comment"># 在名为conv2的variable scope下调用一层神经网络，对应的参数名为</span></span><br><span class="line">        <span class="comment"># "conv2/weights", "conv2/biases"</span></span><br><span class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"image_filter"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    result1 = my_image_filter(input_images)</span><br><span class="line">    scope.reuse_variables() <span class="comment"># 复用变量</span></span><br><span class="line">    result2 = my_image_filter(input_images)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer();</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    image = np.random.rand(<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">    result1 = sess.run(result1, feed_dict=&#123;input_images: image&#125;)</span><br><span class="line">    result2 = sess.run(result2, feed_dict=&#123;input_images: image&#125;)</span><br><span class="line">    print(result2.all() == result1.all())</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="keyword">True</span> </span><br><span class="line"><span class="comment"># 说明第二次的参数没有重新初始化，而是复用了第一次的参数</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://blog.rescale.com/wp-content/uploads/2017/02/markblogtensorflow.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/a&gt;是Google在2015年11月份开源的人工智能系统，是之前所开发的深度学习基础架构&lt;code&gt;DistBelief&lt;/code&gt;的改进版本，该系统可以被用于语音识别、图像识别等多个领域。本文将介绍&lt;code&gt;TensorFlow&lt;/code&gt;的基本概念和常见用法。&lt;/p&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="https://orzyt.cn/categories/TensorFlow/"/>
    
    
      <category term="机器学习" scheme="https://orzyt.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="hands-on ML" scheme="https://orzyt.cn/tags/hands-on-ML/"/>
    
      <category term="TensorFlow" scheme="https://orzyt.cn/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>使用WebIDE搭建Hexo云端写作环境</title>
    <link href="https://orzyt.cn/posts/use-hexo-on-coding-webIDE/"/>
    <id>https://orzyt.cn/posts/use-hexo-on-coding-webIDE/</id>
    <published>2017-11-22T23:20:58.000Z</published>
    <updated>2018-11-10T11:47:16.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于学习的需要，现在日常使用Ubuntu系统。但是，之前hexo的写作环境是搭建在Win10系统里的，如果想在Ubuntu系统里写博客的话，又得重新搭建环境，耗时耗力！</p><p>本着<code>Setup Once, Write Anywhere</code>的原则，便开始寻找一劳永逸的方法。</p><p>因为本博客是托管在<a href="https://pages.coding.net/" target="_blank" rel="noopener">Coding Pages</a>上的，所以首先看看Coding有没有解决方案。幸运的是，Coding有一款产品叫<a href="https://ide.coding.net/index" target="_blank" rel="noopener">Coding webIDE</a>满足我们的需求！</p><a id="more"></a><blockquote><p>Coding WebIDE 是 Coding 自主研发的在线集成开发环境 (IDE)。用户可以通过 WebIDE 创建项目的工作空间, 进行在线开发, 调试等操作。同时 WebIDE 集成了 Git 代码版本控制, 用户可以选择 Coding、GitHub、BitBucket、Git@OSC 等任意的代码仓库。 WebIDE 还提供了分享开发环境的功能, 用户可以保存当前的开发环境, 分享给团队的其他成员。</p></blockquote><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ul><li>最好有hexo、git、ubuntu等的使用经验</li><li>若没有coding.net的账号，可以<a href="https://coding.net" target="_blank" rel="noopener">点此注册</a></li><li>完善个人资料，即可免费成为<code>银牌会员</code><br>注：银牌会员可以使用一个webIDE工作空间，配置：CPU×1，内存256M，磁盘空间2G</li></ul><h2 id="新建工作空间"><a href="#新建工作空间" class="headerlink" title="新建工作空间"></a>新建工作空间</h2><h3 id="新建工作空间-1"><a href="#新建工作空间-1" class="headerlink" title="新建工作空间"></a>新建工作空间</h3><p><img src="https://tuchuang001.com/images/2017/11/23/Selection_024.png" alt="新建工作空间"></p><h3 id="选择空项目"><a href="#选择空项目" class="headerlink" title="选择空项目"></a>选择空项目</h3><p><img src="https://tuchuang001.com/images/2017/11/23/Selection_025.png" alt="选择空项目"></p><h3 id="填写项目信息并选择配置"><a href="#填写项目信息并选择配置" class="headerlink" title="填写项目信息并选择配置"></a>填写项目信息并选择配置</h3><p><img src="https://tuchuang001.com/images/2017/11/23/Selection_026.png" alt="填写项目信息并选择配置"></p><h3 id="运行工作空间"><a href="#运行工作空间" class="headerlink" title="运行工作空间"></a>运行工作空间</h3><p><img src="https://tuchuang001.com/images/2017/11/23/Selection_028.png" alt="运行工作空间"></p><h2 id="配置工作空间"><a href="#配置工作空间" class="headerlink" title="配置工作空间"></a>配置工作空间</h2><h3 id="配置hexo"><a href="#配置hexo" class="headerlink" title="配置hexo"></a>配置hexo</h3><p>点击右侧边栏的<code>Environments</code>选项卡，然后选择使用<code>ide-ttf-hexo</code>环境。</p><p><img src="https://tuchuang001.com/images/2017/11/23/Screenshotfrom2017-11-2317-14-27a3c0a.png" alt="配置hexo环境"></p><h3 id="配置git"><a href="#配置git" class="headerlink" title="配置git"></a>配置git</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.email &quot;Your Email&quot;</span><br><span class="line">git config --global user.name &quot;Your Name&quot;</span><br></pre></td></tr></table></figure><h3 id="配置npm"><a href="#配置npm" class="headerlink" title="配置npm"></a>配置npm</h3><p>hexo的插件需要通过npm安装，换国内的源比较快<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure></p><h2 id="初始化hexo"><a href="#初始化hexo" class="headerlink" title="初始化hexo"></a>初始化hexo</h2><p>在<code>workspace</code>（即<code>hexo-cloud</code>）目录下，执行下列命令初始化hexo<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init blog</span><br><span class="line">cd blog</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure></p><p>至此，hexo环境已经搭好，并成功初始化！</p><h2 id="预览博客"><a href="#预览博客" class="headerlink" title="预览博客"></a>预览博客</h2><p>写好文章之后，通常需要使用命令<code>hexo s</code>预览博客效果。<br>在本地的话，直接在地址栏输入<code>http://localhost:4000</code>即可预览。<br>但是在云端要怎么访问呢？</p><p>方法是，点击右侧边栏的<code>Access URL</code>选项卡，然后在<code>Port</code>选择使用<code>4000</code>端口。<br>然后点击添加<code>+</code>，会生成一个临时链接（有效期为1个小时），访问该链接即可预览博客。<br><img src="https://tuchuang001.com/images/2017/11/23/Selection_030.png" alt="预览博客"></p><h2 id="（可选）导入原有博客"><a href="#（可选）导入原有博客" class="headerlink" title="（可选）导入原有博客"></a>（可选）导入原有博客</h2><p>上述操作是在webIDE里新建一个hexo博客，但是如何导入已有的博客呢？</p><h3 id="方式一"><a href="#方式一" class="headerlink" title="方式一"></a>方式一</h3><p>右键点击文件树空白处，可以选择上传文件。（但是好像不能上传整个目录）</p><h3 id="方式二"><a href="#方式二" class="headerlink" title="方式二"></a>方式二</h3><p>由于每次更新博客之后，我都会把博客文件备份在coding的仓库里。所以，现在只需把该仓库clone到webIDE中即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 复制原有的博客文件到文件夹 hexo_cloud/blog_backup 下</span><br><span class="line">git clone https://git.coding.net/orzyt/blog_backup.git</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 删除之前初始化的博客文件</span><br><span class="line">sudo rm -rf blog/*</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 将blog_backup中的所有文件移到blog中</span><br><span class="line">sudo mv blog_backup/* blog_backup/.[^.]* blog/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 安装原有博客所依赖的插件</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于学习的需要，现在日常使用Ubuntu系统。但是，之前hexo的写作环境是搭建在Win10系统里的，如果想在Ubuntu系统里写博客的话，又得重新搭建环境，耗时耗力！&lt;/p&gt;
&lt;p&gt;本着&lt;code&gt;Setup Once, Write Anywhere&lt;/code&gt;的原则，便开始寻找一劳永逸的方法。&lt;/p&gt;
&lt;p&gt;因为本博客是托管在&lt;a href=&quot;https://pages.coding.net/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Coding Pages&lt;/a&gt;上的，所以首先看看Coding有没有解决方案。幸运的是，Coding有一款产品叫&lt;a href=&quot;https://ide.coding.net/index&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Coding webIDE&lt;/a&gt;满足我们的需求！&lt;/p&gt;
    
    </summary>
    
      <category term="教程" scheme="https://orzyt.cn/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="webIDE" scheme="https://orzyt.cn/tags/webIDE/"/>
    
      <category term="hexo" scheme="https://orzyt.cn/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hands-on Machine learning 之 机器学习概览</title>
    <link href="https://orzyt.cn/posts/hands-on-ml-the-machine-learning-landscape/"/>
    <id>https://orzyt.cn/posts/hands-on-ml-the-machine-learning-landscape/</id>
    <published>2017-11-04T16:00:00.000Z</published>
    <updated>2018-11-10T11:47:16.927Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h2><p>在1959年， <a href="http://en.wikipedia.org/wiki/Arthur_Samuel" target="_blank" rel="noopener">Arthur Samuel</a> 给机器学习定义如下：</p><blockquote><p> [Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.—Arthur Samuel, 1959</p></blockquote><p>Arthur Samuel是美国计算机游戏和人工智能领域的先驱之一。在IBM的时候，他开发出的西洋跳棋程序被认为是世界上第一个自学习程序。Samuel让该程序自己和自己下棋，很快它就习得了在哪些情况下赢的机会比较大，从而可以不断提高自己的棋艺。最终，该程序的棋艺已经远远超过Samuel。</p><p>Samuel这段话的意思是说，“<strong>机器学习是这样的一个研究领域，在没有显式编码的情况下赋予计算机学习的能力。</strong>”虽然这个定义看上去是挺不错的，但是却有点含糊不清。</p><a id="more"></a><p>后来在1998年， <a href="https://en.wikipedia.org/wiki/Tom_M._Mitchell" target="_blank" rel="noopener">Tom Mitchell</a>在他的著作<a href="https://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077" target="_blank" rel="noopener">《Machine Learning 》</a>中给出了一个被广泛引用且更加正式的定义：</p><blockquote><p> A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.—Tom Mitchell, 1997</p></blockquote><p>这段话是说，“<strong>如果一个计算机程序在任务T中以P为度量的性能从经验E中得到了提升，那么可以说该程序从与某类任务T和性能度量P有关的经验E中进行了学习。</strong>” </p><p>根据该定义，一个机器学习问题必须具备三个要素：</p><ul><li><strong>任务（Tasks）</strong></li><li><strong>性能度量（Performance Measure）</strong></li><li><strong>训练经验（Training Experience）</strong></li></ul><p>以垃圾邮件过滤器（spam filter）为例，该程序的任务是标记垃圾邮件，性能度量可以是正确分类的比例，训练经验通常是包含正常邮件和垃圾邮件的训练数据集。</p><h2 id="为什么要使用机器学习？"><a href="#为什么要使用机器学习？" class="headerlink" title="为什么要使用机器学习？"></a>为什么要使用机器学习？</h2><p>考虑如何构建一个垃圾邮件过滤器。</p><h3 id="使用传统方式"><a href="#使用传统方式" class="headerlink" title="使用传统方式"></a>使用传统方式</h3><p><img src="https://tuchuang001.com/images/2017/11/05/d84888bdf33ee66725e1bda704a73eed.jpg" alt=""></p><p>首先人工提取垃圾邮件中的特征（比如一些经常出现的单词或短语），然后总结成“规则”写进代码中。若一封邮件满足其中某些规则，则将其标记为垃圾邮件。</p><p>弊端在于：随着时间的推移，程序中复杂、冗长的“规则”将变得难以维护。</p><h3 id="使用机器学习"><a href="#使用机器学习" class="headerlink" title="使用机器学习"></a>使用机器学习</h3><p><img src="https://tuchuang001.com/images/2017/11/05/53455b51d6df2b84f0970d44ca346f43.jpg" alt=""></p><p>基于机器学习的方法可以自动地从训练数据中学习垃圾邮件的特征，该过程无需过多的人工操作。</p><p>优点在于：程序通常更短、更容易维护，且准确率更高。</p><p>一般来说，机器学习擅长以下几个方面：</p><ul><li><strong>已存在的解法需要过多的人工操作，或者手动维护一长串的“规则”。</strong>例如，垃圾邮件的过滤；</li><li><strong>对于非常复杂的问题，传统方式通常不能够有效地解决。</strong>例如，语音识别问题；</li><li><strong>能够自动适应复杂多变的外部环境。</strong>例如，机器学习系统可以自动适应新数据；</li><li><strong>对复杂问题和大规模数据有很好的洞察能力。</strong>例如，可以发现数据中潜在的模式。</li></ul><h2 id="机器学习系统的类型"><a href="#机器学习系统的类型" class="headerlink" title="机器学习系统的类型"></a>机器学习系统的类型</h2><h3 id="有-无监督学习"><a href="#有-无监督学习" class="headerlink" title="有/无监督学习"></a>有/无监督学习</h3><p>根据在训练过程中得到的监督类型和数量，通常可以分为：</p><h4 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h4><p>在<strong>有监督学习（supervised learning）</strong>中，每一个训练数据都带有<strong>标签（labels）</strong>。比如，在垃圾邮件问题中，训练数据中的每封邮件都会带上是否是垃圾邮件（spam or ham）的标签。</p><p>有监督学习一般分为以下两类：</p><ul><li><strong>分类（classification）</strong></li></ul><p>比如，将邮件按是否是垃圾邮件进行分类。通常，该问题的标签是离散型数据。</p><p><img src="https://tuchuang001.com/images/2017/11/05/b1f3d44377279694e881806810f83e44.jpg" alt=""></p><ul><li><strong>回归（regression）</strong></li></ul><p>比如，根据车的一系列特征预测车的价格是多少。通常，该问题的标签是连续型数据。</p><p><img src="https://tuchuang001.com/images/2017/11/05/5e635c85e4335d577dce358f003f0230.jpg" alt=""></p><p>一些比较重要的有监督学习算法：</p><ul><li>K近邻（k-Nearest Neighbors）</li><li>线性回归（Linear Regression）</li><li>对数几率回归（Logistic Regression）</li><li>支持向量机（SVMs，Support Vector Machines）</li><li>决策树和随机森林（Decision Trees and Random Forests）</li><li>神经网络（Neural networks）</li></ul><h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h4><p>在<strong>无监督学习（unsupervised learning）</strong>中，训练数据都是无标签的。</p><p>一般有以下几类任务：</p><ul><li><strong>聚类（Clustering）</strong></li></ul><p>比如，通过计算样本之间的相似性，将其聚拢成一个个簇。使得簇内样本的相似度高，簇外样本的相似度低。</p><p><img src="https://tuchuang001.com/images/2017/11/05/289e3e988536a156b3a04c331b773ec7.jpg" alt=""></p><ul><li><strong>可视化与降维（Visualization and dimensionality reduction）</strong></li></ul><p>比如，将高维的数据降维到低维空间（二维或三维），使得可以进行数据可视化。</p><p><img src="https://tuchuang001.com/images/2017/11/05/e85fc01f835edefaa407455b7a619f11.jpg" alt=""></p><ul><li><strong>异常检测（anomaly detection）</strong></li></ul><p>比如，检测信用卡中的非法交易、剔除数据中的异常点。</p><p><img src="https://tuchuang001.com/images/2017/11/05/9e028aacfbe2c58091bfedba9fdd19d6.jpg" alt=""></p><ul><li><strong>关联规则学习（association rule learning）</strong></li></ul><p>比如，在大量购物记录中挖掘出各个商品之间的购买关系。</p><p><img src="https://tuchuang001.com/images/2017/11/05/dc59dc05ed31036b7ebd444017adbd8f.jpg" alt=""></p><p>一些比较重要的无监督算法：</p><ul><li>聚类（Clustering）<ul><li>k均值（k-Means）</li><li>层序聚类分析（HCA，Hierarchical Cluster Analysis ）</li><li>期望最大化（EM，Expectation Maximization）</li></ul></li><li>可视化与降维（Visualization and dimensionality reduction）<ul><li>主成分分析（PCA，Principal Component Analysis ）</li><li>核PCA（Kernel PCA）</li><li>局部线性嵌入（LLE，Locally-Linear Embedding）</li><li>t分布随机领域嵌入（t-SNE，t-distributed Stochastic Neighbor Embedding）</li></ul></li><li>关联规则学习（Association rule learning）<ul><li>Apriori</li><li>Eclat</li></ul></li></ul><h4 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h4><p>在<strong>半监督学习（semi-supervised learning）</strong>中，训练数据通常由<strong>大量无标签数据</strong>和<strong>少量带标签数据</strong>构成。</p><p>比如，当你把一张家庭照上传到Google Photos，便可以自动识别出在哪些照片中出现了人物A，在哪些照片中出现了人物B，这是无监督聚类的过程。但是，如果想要知道人物A、人物B叫什么的话，那么只需要给每个人物打上一个”姓名“标签即可，这是有监督的过程。</p><p>因此，只需标记少量样本，便可以对每张照片的每个人物打上标签。</p><p><img src="https://tuchuang001.com/images/2017/11/05/6abc78a655c67db16fd2c7a392719c5f.jpg" alt=""></p><h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><p><strong>强化学习（Reinforcement Learning）</strong>是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。</p><p>在该问题中，学习系统被称为<strong>agent</strong>，它可以观察<strong>环境（environment）</strong>，执行<strong>动作（actions）</strong>，然后可以得到<strong>奖励（rewards）</strong>，最终学习到最优的<strong>策略（policy）</strong>。</p><p><img src="https://tuchuang001.com/images/2017/11/05/d4222a249b2d675a413dc1f0e4845d51.jpg" alt=""></p><h3 id="批量-在线学习"><a href="#批量-在线学习" class="headerlink" title="批量/在线学习"></a>批量/在线学习</h3><p>该分类标准是根据系统能否从连续的数据流中进行学习。</p><ul><li><strong>批量学习（Batching learning）</strong></li></ul><p>批量学习的每次训练都需要使用整个数据集，该过程通常是<strong>离线（offline）</strong>进行的。</p><p>模型离线训练完毕后，再部署到生产环境中去，在运行过程中模型也不会再进行训练。如果批量学习系统需要适应新的数据，那么必须重新进行训练，训练的数据由旧数据和新数据组成，训练完毕后再部署上去。</p><p>批量学习的过程非常消耗计算资源，通常是每隔24h或更久训练一次。</p><ul><li><strong>在线学习（Online learning）</strong></li></ul><p>在在线学习的过程中，可以依次递送数据（单个或者小批量）来逐步地训练系统。</p><p>该方法特别适用于需要不断接收连续的数据流，或者需要不断适应变化数据的系统。同时，在线学习也适用于计算资源缺乏的环境，因为通常系统学习完新数据后，便不再需要保留该数据，从而可以节约巨大的存储空间。</p><p><img src="https://tuchuang001.com/images/2017/11/05/2c77418e6ee25717f0e3103209e3dcba.jpg" alt=""></p><p>在线学习也常被用来进行<code>out-of-core learning</code>，使得系统可以在巨大的数据集（不能一次性放入机器的内存中）上进行训练。每次训练只载入数据集的部分数据，不断重复，直到所有数据都被处理过。</p><p>对于在线学习来说，一个很重要的参数是<strong>学习率（learning rate）</strong>。若学习率过高，那么系统将会更加适应新的数据，而更快遗忘旧的数据。若学习率过低，那么系统将会学习得很慢，但同时对新数据中的噪声或数据中没有代表性的样本不会太敏感。</p><p>在线学习面临的一个挑战是，一旦”坏数据“被输入到系统中，那么系统的性能将会逐渐降低。为了降低该风险，可以不断地监测系统，在性能下降时恢复到之前正常的状态；也可以使用异常检测算法对数据进行检测，没有问题后再输入到系统中去。</p><h3 id="基于实例-基于模型的学习"><a href="#基于实例-基于模型的学习" class="headerlink" title="基于实例/基于模型的学习"></a>基于实例/基于模型的学习</h3><p>该分类标准是系统是如何对未知数据进行<strong>泛化（generalize）</strong>。</p><ul><li><strong>基于实例的学习（Instance-based learning）</strong></li></ul><p>基于实例的学习只是简单地把训练数据存储起来，当遇到新的查询实例时，根据相似性计算规则从已知数据中找到相似的实例，然后对其进行泛化预测。例如，k近邻算法。</p><p><img src="https://tuchuang001.com/images/2017/11/05/aab4a3831ca8e8a0be55969771415327.jpg" alt=""></p><ul><li><strong>基于模型的学习（Mode-based learning）</strong></li></ul><p>基于模型的学习是从训练数据中建立一个<strong>模型（model）</strong>，然后使用该模型对未知数据进行泛化预测。例如，支持向量机。</p><p><img src="https://tuchuang001.com/images/2017/11/05/1f521df80ace687224d1262b900f952f.jpg" alt=""></p><h2 id="机器学习的主要挑战"><a href="#机器学习的主要挑战" class="headerlink" title="机器学习的主要挑战"></a>机器学习的主要挑战</h2><h3 id="训练数据方面"><a href="#训练数据方面" class="headerlink" title="训练数据方面"></a>训练数据方面</h3><ul><li><strong>训练数据的数量不够</strong></li></ul><p>训练数据的数量在一定程度上也影响最终模型的效果，特别是采用深度学习等方法，其所需数据量通常比较大。对于图片数据来说，可以通过平移、翻转、裁剪、加噪声等方式进行<strong>数据增强（data augmentation）</strong>。</p><ul><li><strong>训练数据的代表性不足</strong></li></ul><p>如果训练数据的代表性不足，通常训练出来的模型的泛化能力也比较弱。在数据采样的时候，如果样本数量较少，那么可能出现<strong>采样噪声（sampling noise）</strong>的问题；若采样方式错误，即使样本数量很多也可能不具有代表性，依然会出现<strong>采样偏差（sampling bias）</strong>的问题。</p><ul><li><strong>训练数据的质量不好</strong></li></ul><p>如果训练数据中充满了错误、异常点、噪声等等，那么训练出的模型也很难检测出原本真实数据中潜在的模式。所以，<strong>数据清洗（data cleaning）</strong>是整个机器学习过程中不可缺少的一个环节，其结果质量直接关系到模型效果和最终结论。</p><ul><li><strong>训练数据的特征与问题不相关</strong></li></ul><p><strong>特征工程（feature engineering）</strong>其本质上是一项工程活动，它目的是最大限度地从原始数据中提取特征以供算法和模型使用。一般认为，数据和特征决定了机器学习的上限，而模型和算法只能逼近这个上限而已。特征工程一般包括<strong>特征选择（feature selection）</strong>、<strong>特征提取（feature extraction）</strong>等部分。</p><h3 id="算法模型方面"><a href="#算法模型方面" class="headerlink" title="算法模型方面"></a>算法模型方面</h3><ul><li><strong>过拟合（overfitting）问题</strong></li></ul><p>模型复杂度过高，训练数据过少，训练误差虽小，但测试误差大。</p><p>一般通过<strong>正则化（generalization）</strong>方式解决，本质上是降低模型的复杂度。</p><ul><li><strong>欠拟合（underfitting）问题</strong></li></ul><p>模型复杂度过低，不能很好地拟合所有的数据，训练误差大。</p><p><img src="https://tuchuang001.com/images/2017/11/05/750fc8894f1a5daa7d30071878f2d708.jpg" alt=""></p><h2 id="测试和验证"><a href="#测试和验证" class="headerlink" title="测试和验证"></a>测试和验证</h2><p>评估一个模型的好坏，可以测试该模型对新样本的泛化能力。</p><p>通常将数据集分为两部分：<strong>训练集（training set）</strong>和<strong>测试集（test set）</strong>。其中，训练集用于模型参数的拟合，测试集用于对已经训练好的模型进行性能评估。</p><p>如果算法中存在<strong>超参数（hyperparameter）</strong>，那么可以从训练集中再划分出<strong>验证集（validation set）</strong>来进行调参。</p><p>一种常用的验证方式是：<strong>k折交叉验证（k-fold cross validation）</strong>。k折交叉验证将所有训练样本分成K份，一般每份样本的数量相等或相差不多。取一份作为测试样本，剩余K-1份作为训练样本。这个过程重复K次，取平均值作为最终的验证结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是机器学习？&quot;&gt;&lt;a href=&quot;#什么是机器学习？&quot; class=&quot;headerlink&quot; title=&quot;什么是机器学习？&quot;&gt;&lt;/a&gt;什么是机器学习？&lt;/h2&gt;&lt;p&gt;在1959年， &lt;a href=&quot;http://en.wikipedia.org/wiki/Arthur_Samuel&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Arthur Samuel&lt;/a&gt; 给机器学习定义如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; [Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.—Arthur Samuel, 1959&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Arthur Samuel是美国计算机游戏和人工智能领域的先驱之一。在IBM的时候，他开发出的西洋跳棋程序被认为是世界上第一个自学习程序。Samuel让该程序自己和自己下棋，很快它就习得了在哪些情况下赢的机会比较大，从而可以不断提高自己的棋艺。最终，该程序的棋艺已经远远超过Samuel。&lt;/p&gt;
&lt;p&gt;Samuel这段话的意思是说，“&lt;strong&gt;机器学习是这样的一个研究领域，在没有显式编码的情况下赋予计算机学习的能力。&lt;/strong&gt;”虽然这个定义看上去是挺不错的，但是却有点含糊不清。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://orzyt.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://orzyt.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="hands-on ML" scheme="https://orzyt.cn/tags/hands-on-ML/"/>
    
  </entry>
  
</feed>
