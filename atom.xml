<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>扬涛的博客</title>
  
  <subtitle>上善若水·大道至简</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://orzyt.cn/"/>
  <updated>2019-04-01T05:33:35.117Z</updated>
  <id>https://orzyt.cn/</id>
  
  <author>
    <name>orzyt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2019年03月度总结</title>
    <link href="https://orzyt.cn/posts/201903-summary/"/>
    <id>https://orzyt.cn/posts/201903-summary/</id>
    <published>2019-04-01T04:18:11.000Z</published>
    <updated>2019-04-01T05:33:35.117Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文暂未公开，请输入密码访问" />    <label for="pass">本文暂未公开，请输入密码访问</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">∑(っ°Д°;)っ 密码错误！</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;本文暂未公开，请输入密码访问
    
    </summary>
    
      <category term="随笔" scheme="https://orzyt.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="研究生学习" scheme="https://orzyt.cn/tags/%E7%A0%94%E7%A9%B6%E7%94%9F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>联合分布自适应</title>
    <link href="https://orzyt.cn/posts/joint-distribution-adaptation/"/>
    <id>https://orzyt.cn/posts/joint-distribution-adaptation/</id>
    <published>2019-03-30T12:51:46.000Z</published>
    <updated>2019-04-29T08:18:53.411Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文题目</strong>：Transfer Feature Learning with Joint Distribution Adaptation</p><p><strong>论文作者</strong>：Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu</p><p><strong>会议期刊</strong>：ICCV 2013</p><a id="more"></a><hr><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在迁移成分分析方法中, 只对不同域的边缘分布进行匹配. 然而在真实情况下, 边缘分布和条件分布可能都是不匹配的, 如图1所示. 本文首次<strong>在无监督领域自适应</strong>条件下(即目标域中没有带标签数据), 对不同域之间的<strong>边缘分布</strong>和<strong>条件分布</strong>进行联合匹配.</p><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cely1g1iu5i9bdmj20pr0alt9j.jpg" alt="图1. 不同域的边缘分布和条件分布都不匹配" width="60%" height="60%"></p><hr><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><blockquote><p><strong>定义1 (领域, Domain)</strong><br>一个领域 $\mathcal{D}$ 由 $m$ 维的特征空间 $\mathcal{X}$ 和边缘概率分布 $P(x)$ 组成. 即 $\mathcal{D} = \{\mathcal{X}, P(x)\}$.</p><p><strong>定义2 (任务, Task)</strong><br>给定领域 $\mathcal{D}$, 任务由 $C$ 类标签集 $\mathcal{Y}$ 和分类器 $f(x)$构成. 即 $\mathcal{T} = \{ \mathcal{Y}, f(x)\}$, 其中 $y \in \mathcal{Y}$ 且 $f(x) = Q(y|x)$ 可以解释为条件概率分布.</p><p><strong>问题1 (联合分布自适应, Joint Distribution Adaptation)</strong><br>给定带标签源域 $\mathcal{D}_s = \{ (\mathrm{x}_1, y_1), \cdots, (\mathrm{x}_{n_s}, y_{n_s})\}$ 和无标签目标域 $\mathcal{D}_t = \{ \mathrm{x}_{n_{s}+1}, \cdots, \mathrm{x}_{n_s + n_t}\}$, 并设定源域和目标域的特征空间及类别空间相同 $\mathcal{X}_s = \mathcal{X}_t$, $\mathcal{Y}_s = \mathcal{Y}_t$, 但边缘分布和条件分布都不相同 $P_s(\mathrm{x}_s) \ne P_t(\mathrm{x}_t)$, $Q_s(y_s|\mathrm{x}_s) \ne Q_t(y_t|\mathrm{x}_t)$, JDA希望学习到减少边缘分布 $P_s(\mathrm{x}_s)$, $P_t(\mathrm{x}_t)$ 及条件分布 $Q_s(y_s|\mathrm{x}_s)$, $Q_t(y_t|\mathrm{x}_t)$之间差异的特征表示.</p></blockquote><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本文通过采用一个特征变换 $T$ 使得不同域之间的特征 $\mathcal{x}$ 和标签 $y$ 之间的联合概率期望相匹配.</p><script type="math/tex; mode=display">\begin{aligned}\min \limits_{T} & \Vert \mathbb{E}_{P(\mathrm{x}_s, y_s)}[T(\mathrm{x_s}), y_s] - \mathbb{E}_{P(\mathrm{x}_t, y_t)}[T(\mathrm{x_t}), y_t] \Vert^2 \\\approx & \Vert \mathbb{E}_{P(\mathrm{x}_s)}[T(\mathrm{x_s})] - \mathbb{E}_{P(\mathrm{x}_t)}[T(\mathrm{x_t})] \Vert^2 {\scriptsize // 边缘分布}\\+ & \Vert \mathbb{E}_{Q_s(y_s|\mathrm{x}_s)}[y_s|T(\mathrm{x_s})] - \mathbb{E}_{Q_t(y_t|\mathrm{x}_t)}[y_t|T(\mathrm{x_t})] \Vert^2 \Vert^2 {\scriptsize // 后验分布}\\\end{aligned}\tag{1}</script><h3 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h3><p>本文采用<strong>主成分分析</strong>作为特征变换的方法.</p><p>定义 $\mathbf{X}=\left[\mathbf{x}_{1}, \dots, \mathbf{x}_{n}\right] \in \mathbb{R}^{m \times n}$ 为输入数据, $\mathbf{H}$ 为<a href="/posts/centering-matrix/">中心矩阵</a>. PCA的优化目标为寻找一个正交变换矩阵 $\mathbf{A} \in \mathbb{R}^{m \times k}$ 使得变换后数据的方差最大化:</p><script type="math/tex; mode=display">\max _{\mathbf{A}^{\mathrm{T}} \mathbf{A}=\mathbf{I}} \operatorname{tr}\left(\mathbf{A}^{\mathrm{T}} \mathbf{X} \mathbf{H} \mathbf{X}^{\mathrm{T}} \mathbf{A}\right)\tag{2}</script><p>该优化目标可以通过特征分解的方式高效地求解, 即 $\mathbf{A}$ 由协方差矩阵 $\mathbf{X} \mathbf{H} \mathbf{X}^{\mathrm{T}}$ 前 $k$ 大的特征值对应的特征向量构成.</p><h3 id="边缘分布自适应"><a href="#边缘分布自适应" class="headerlink" title="边缘分布自适应"></a>边缘分布自适应</h3><p>类似于<a href="/posts/transfer-component-analysis/">迁移成分分析</a>, JDA中边缘分布自适应的优化目标为:</p><script type="math/tex; mode=display">\left\|\frac{1}{n_{s}} \sum_{i=1}^{n_{s}} \mathbf{A}^{\mathrm{T}} \mathbf{x}_{i}-\frac{1}{n_{t}} \sum_{j=n_{s}+1}^{n_{s}+n_{t}} \mathbf{A}^{\mathrm{T}} \mathbf{x}_{j}\right\|^{2}=\operatorname{tr}\left(\mathbf{A}^{\mathrm{T}} \mathbf{X} \mathbf{M}_{0} \mathbf{X}^{\mathrm{T}} \mathbf{A}\right)\tag{3}</script><p>其中,</p><script type="math/tex; mode=display">\left(M_{0}\right)_{i j}=\left\{\begin{array}{ll}{\frac{1}{n_{s} n_{s}},} & {\mathbf{x}_{i}, \mathbf{x}_{j} \in \mathcal{D}_{s}} \\ {\frac{1}{n_{t} n_{t}},} & {\mathbf{x}_{i}, \mathbf{x}_{j} \in \mathcal{D}_{t}} \\ {\frac{-1}{n_{s} n_{t}},} & {\text { otherwise }}\end{array}\right.\tag{4}</script><h3 id="条件分布自适应"><a href="#条件分布自适应" class="headerlink" title="条件分布自适应"></a>条件分布自适应</h3><p>由于目标域中的数据是无标签的, 从而无法估计目标域中的条件分布. 本文提出采用<strong>伪标签(pseudo label)</strong>的方式, 通过在带标签的源域数据中训练一个基分类器, 直接对目标域中的无标签数据进行预测. 由于存在分布差异, 基分类器的效果肯定是不好的, 因此只能得到一个大概的伪标签, 需要通过不断的迭代来调整.</p><p>此外, 文中提到直接估计后验分布 $Q_{s}\left(y_{s} | \mathrm{x}_{s}\right)$ 和 $Q_{t}\left(y_{t} | \mathrm{x}_{t}\right)$ 是十分困难的. 因此, 采用类条件分布 $Q_{s}\left(\mathrm{x}_{s} | y_{s}\right)$ 和 $Q_{t}\left(\mathrm{x}_{t} | y_{t}\right)$ 的<strong>充分统计量(sufficient statistics)</strong>替代.</p><blockquote><p><strong>定义 (充分统计量, sufficient statistics)</strong><br>对于统计量 $t = T(X)$，若数据 $X$ 在已知 $t = T(X)$ 时的条件分布不依赖于参数 $\theta$，则称其是参数 $\theta$ 的充分统计量.</p></blockquote><p>首先, 为什么可以用类条件分布来近似后验分布?</p><p>考虑使用贝叶斯公式 $Q(y|\mathrm{x}) = \frac{Q(\mathrm{x}|y)Q(y)}{Q(\mathrm{x})}$, 可以认为源域和目标域的边缘分布 $Q(\mathrm{x})$ 和类分布 $Q(y)$ 是相同的, 因此在衡量不同域间的后验分布差异时, 定值 $\frac{Q(y)}{Q(x)}$ 可以忽略, 从而可以直接用类条件分布来代替.</p><p>其次, 如何选择分布的充分统计量?</p><p>本文采用最大均值差异MMD准则来度量分布差异, 本质上是使用再生希尔伯特空间中高维数据的均值来作为充分统计量.</p><p>借助于类别伪标签和MMD准则, 从而可以衡量类条件分布 $Q_{s}\left(\mathbf{x}_{s} | y_{s}=c\right)$ 和 $Q_{t}\left(\mathbf{x}_{t} | y_{t}=c\right)$ 之间的差异.</p><script type="math/tex; mode=display">\left\|\frac{1}{n_{S}^{(c)}} \sum_{\mathbf{x}_{i} \in \mathcal{D}_{s}^{(c)}} \mathbf{A}^{\mathrm{T}} \mathbf{x}_{i}-\frac{1}{n_{t}^{(c)}} \sum_{\mathbf{x}_{j} \in \mathcal{D}_{t}^{(c)}} \mathbf{A}^{\mathrm{T}} \mathbf{x}_{j}\right\|^{2}=\operatorname{tr}\left(\mathbf{A}^{\mathrm{T}} \mathbf{X} \mathbf{M}_{c} \mathbf{X}^{\mathrm{T}} \mathbf{A}\right)\tag{5}</script><p>其中, $\mathcal{D}_{s}^{(c)}=\left\{\mathbf{x}_{i} : \mathbf{x}_{i} \in \mathcal{D}_{s} \wedge y\left(\mathbf{x}_{i}\right)=c\right\}$ 是源域中属于类别 $c$ 的样本, $y\left(\mathbf{x}_{i}\right)$ 是样本 $\mathbf{x}_{i}$ 的标签, $n_{s}^{(c)}=\left|\mathcal{D}_{s}^{(c)}\right|$ 为源域的样本个数. 类似的, $\mathcal{D}_{t}^{(c)}=\left\{\mathbf{x}_{j} : \mathbf{x}_{j} \in \mathcal{D}_{t} \wedge \widehat{y}\left(\mathbf{x}_{j}\right)=c\right\}$ 为目标域中属于类别 $c$ 的数据, 其中 $\widehat{y}\left(\mathbf{x}_{j}\right)$ 是样本 $\mathbf{x}_{j}$ 的伪标签, $n_{t}^{(c)}=\left|\mathcal{D}_{t}^{(c)}\right|$ 为目标域的样本个数. </p><p>包含类标签的MMD矩阵 $\mathbf{M}_{c}$ 定义如下:</p><script type="math/tex; mode=display">\left(M_{c}\right)_{i j} = \left\{  \begin{array}{ll}    {\frac{1}{n^{(c)}_{s} n^{(c)}_{s}},} & {\mathbf{x}_{i}, \mathbf{x}_{j} \in \mathcal{D}_{s}} \\    {\frac{1}{n^{(c)}_{t} n^{(c)}_{t}},} & {\mathbf{x}_{i}, \mathbf{x}_{j} \in \mathcal{D}_{t}} \\    {\frac{-1}{n^{(c)}_{s} n^{(c)}_{t}},} & {\left\{\begin{array}{l}{\mathbf{x}_{i} \in \mathcal{D}_{s}^{(c)}, \mathbf{x}_{j} \in \mathcal{D}_{t}^{(c)}} \\ {\mathbf{x}_{j} \in \mathcal{D}_{s}^{(c)}, \mathbf{x}_{i} \in \mathcal{D}_{t}^{(c)}}\end{array}\right.} \\    {0,} & {\text { otherwise }}  \end{array}\right.\tag{6}</script><h3 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h3><p>JDA的优化目标有三个:</p><ul><li>最大化投影后数据的方差, 即 $\max$ 公式$(2)$</li><li>最小化边缘分布, 即 $\min$ 公式$(3)$</li><li>最小化条件分布, 即 $\min$ 公式$(5)$</li></ul><p>根据<strong>广义瑞利商(generalized Rayleigh quotient)</strong>, JDA的优化目标可写为:</p><script type="math/tex; mode=display">\min _{\mathbf{A}^{\mathrm{T}} \mathbf{X H X}^{T} \mathbf{A}=\mathbf{I}} \sum_{c=0}^{C} \operatorname{tr}\left(\mathbf{A}^{\mathrm{T}} \mathbf{X} \mathbf{M}_{c} \mathbf{X}^{\mathrm{T}} \mathbf{A}\right)+\lambda\|\mathbf{A}\|_{F}^{2}\tag{7}</script><p><strong>核化(Kernelization)</strong>: 对于非线性问题, 可以使用核函数映射 $\psi : \mathbf{x} \mapsto \psi(\mathbf{x})$, 或者 $\psi(\mathbf{X})=\left[\psi\left(\mathbf{x}_{1}\right), \ldots, \psi\left(\mathbf{x}_{n}\right)\right]$ 及核矩阵 $\mathbf{K}=\psi(\mathbf{X})^{\top}\psi(\mathbf{X})$, 从而得到kernel-JDA:</p><script type="math/tex; mode=display">\min _{\mathbf{A}^{\mathrm{T}} \mathbf{K} \mathbf{H} \mathbf{K}^{\mathrm{T}} \mathbf{A}=\mathbf{I}} \sum_{c=0}^{C} \operatorname{tr}\left(\mathbf{A}^{\mathrm{T}} \mathbf{K} \mathbf{M}_{c} \mathbf{K}^{\mathrm{T}} \mathbf{A}\right)+\lambda\|\mathbf{A}\|_{F}^{2}\tag{8}</script><h2 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h2><p>根据拉格朗日乘子法, 公式$(7)$的拉格朗日函数为:</p><script type="math/tex; mode=display">L = \operatorname{tr}\left(\mathbf{A}^{\mathrm{T}}\left(\mathbf{X} \sum_{c=0}^{C} \mathbf{M}_{c} \mathbf{X}^{\mathrm{T}}+\lambda \mathbf{I}\right) \mathbf{A}\right) + \operatorname{tr}\left(\left(\mathbf{I}-\mathbf{A}^{\mathrm{T}} \mathbf{X} \mathbf{H} \mathbf{X}^{\mathrm{T}} \mathbf{A}\right) \mathbf{\Phi}\right)\tag{9}</script><p>其中, $\mathbf{\Phi}=\operatorname{diag}\left(\phi_{1}, \dots, \phi_{k}\right) \in \mathbb{R}^{k \times k}$ 为拉格朗日乘子.</p><p>令 $\frac{\partial L}{\partial \mathbf{A}}=0$, 从而得到<strong>广义特征分解(generalized eigendecomposition)</strong>问题:</p><script type="math/tex; mode=display">\left(\mathbf{X} \sum_{c=0}^{C} \mathbf{M}_{c} \mathbf{X}^{\mathrm{T}}+\lambda \mathbf{I}\right) \mathbf{A}=\mathbf{X} \mathbf{H} \mathbf{X}^{\mathrm{T}} \mathbf{A} \boldsymbol{\Phi}\tag{10}</script><p>通过求解该广义特征分解问题, 可以得到变换矩阵 $A$ 的最优解.</p><h2 id="联合分布自适应算法"><a href="#联合分布自适应算法" class="headerlink" title="联合分布自适应算法"></a>联合分布自适应算法</h2><p>联合分布自适应算法流程如下:</p><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g1oo1qscnnj20q70gu0w2.jpg" alt="联合分布自适应算法" width="60%" height="60%"></p><p>主要分为求解变换矩阵$A$, 训练分类器更新伪标签, 构造类MMD矩阵三部分. 注意算法需要不断迭代, 直到收敛.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文题目&lt;/strong&gt;：Transfer Feature Learning with Joint Distribution Adaptation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文作者&lt;/strong&gt;：Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;会议期刊&lt;/strong&gt;：ICCV 2013&lt;/p&gt;
    
    </summary>
    
      <category term="迁移学习" scheme="https://orzyt.cn/categories/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文笔记" scheme="https://orzyt.cn/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="迁移学习" scheme="https://orzyt.cn/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="领域自适应" scheme="https://orzyt.cn/tags/%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94/"/>
    
      <category term="JDA" scheme="https://orzyt.cn/tags/JDA/"/>
    
  </entry>
  
  <entry>
    <title>中心矩阵</title>
    <link href="https://orzyt.cn/posts/centering-matrix/"/>
    <id>https://orzyt.cn/posts/centering-matrix/</id>
    <published>2019-03-30T07:58:24.000Z</published>
    <updated>2019-04-01T05:33:15.341Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看迁移学习的早期文章, 在求解优化问题时, 经常会涉及到<strong>中心矩阵</strong>(centering matrix). </p><p>由于之前没怎么遇到过, 故总结于此.</p><a id="more"></a><hr><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>先从多维随机变量的协方差矩阵讲起.</p><p>假设我们观察到 $n$ 个样本 $X=[\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_n] \in \mathbb{R}^{p \times n}$，其中每个样本有 $p$ 维特征 (即 $p$ 个随机变量), $\mathbf{x}_i = [x_{i1}, \cdots, x_{ip}]^{\top} \in \mathbb{R}^{p \times 1}$. 我们往往需要计算各维度两两之间的协方差，这样各协方差组成了一个 $p*p$ 维的矩阵，称为协方差矩阵 $\Sigma$. 协方差矩阵是个对称矩阵, 对角线上的元素是各维度上随机变量的方差.</p><script type="math/tex; mode=display">\Sigma = \frac{1}{n} \sum^{n}_{i}(\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^{\top}\tag{1}</script><p>其中, $\bar{\mathbf{x}} = \frac{1}{n} \sum^{n}_{i}\mathbf{x}_i \in \mathbb{R}^{p \times 1}$ 为 $p$ 个随机变量的均值.</p><p>更进一步, 我们将公式$(1)$写成矩阵形式</p><script type="math/tex; mode=display">\begin{aligned}\Sigma & = \frac{1}{n} \sum^{n}_{i}(\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^{\top} \\&= \frac{1}{n} \sum^{n}_{i} (\mathbf{x}_i \mathbf{x}^{\top}_i - \mathbf{x}_i \bar{\mathbf{x}}^{\top} - \bar{\mathbf{x}} \mathbf{x}^{\top}_i + \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top}) \\&= \frac{1}{n} XX^{\top} - \frac{1}{n} \sum^{n}_{i} \mathbf{x}_i \bar{\mathbf{x}}^{\top} - \frac{1}{n} \sum^{n}_{i} \bar{\mathbf{x}} \mathbf{x}^{\top}_i + \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top} \\&= \frac{1}{n} XX^{\top} - \left( \frac{1}{n} \sum^{n}_{i} \mathbf{x}_i \right) \bar{\mathbf{x}}^{\top} - \bar{\mathbf{x}} \left( \frac{1}{n} \sum^{n}_{i}  \mathbf{x}^{\top}_i \right) + \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top} \\ &= \frac{1}{n} XX^{\top} - \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top} - \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top} + \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top} \\ &= \frac{1}{n} XX^{\top} - \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top}\end{aligned}\tag{2}</script><p>注意到, 随机变量的均值向量 $\bar{\mathbf{x}}$ 也可以用矩阵 $X$ 表示</p><script type="math/tex; mode=display">\bar{\mathbf{x}} = \frac{1}{n}X\mathbf{1}_n\tag{3}</script><p>其中, $\mathbf{1}_n = [1, \cdots, 1]^{\top} \in \mathbb{R}^{n \times 1}$ 为全 $1$ 的列向量.</p><p>因此, 将$(3)$代入公式$(2)$可得协方差矩阵的另一种表示形式</p><script type="math/tex; mode=display">\begin{aligned}\Sigma & = \frac{1}{n} XX^{\top} - \bar{\mathbf{x}} \bar{\mathbf{x}}^{\top} \\&= \frac{1}{n} XX^{\top} - \frac{1}{n^2}X\mathbf{1}_{n}\mathbf{1}^{\top}_{n}X^{\top} \\&= \frac{1}{n} X(I_n - \frac{1}{n}\mathbf{1}_{n}\mathbf{1}^{\top}_{n})X^{\top} \\&=  \frac{1}{n} XHX^{\top}\end{aligned}\tag{4}</script><p>其中, $I_n$ 为 $n$ 阶单位阵, $H = I_n - \frac{1}{n}\mathbf{1}_{n}\mathbf{1}^{\top}_{n}$ 即定义为<strong>中心矩阵</strong>(centering matrix).</p><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><h3 id="幂等性"><a href="#幂等性" class="headerlink" title="幂等性"></a>幂等性</h3><script type="math/tex; mode=display">\begin{aligned}H^{2} &=\left(I_{n}-\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}\right)\left(I_{n}-\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}\right) \\ &=I_{n}-\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}-\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}+\left(\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}\right)\left(\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}\right) \\&=I_{n}-\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}-\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top}+\frac{1}{n^2}n \mathbf{1}_{n} \mathbf{1}_{n}^{\top} \\&=I_{n}-\frac1n \mathbf{1}_{n} \mathbf{1}_{n}^{\top} \\&=H\end{aligned}\tag{5}</script><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>其他性质详见维基百科: <a href="https://en.wikipedia.org/wiki/Centering_matrix" target="_blank" rel="noopener">Centering Matrix</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看迁移学习的早期文章, 在求解优化问题时, 经常会涉及到&lt;strong&gt;中心矩阵&lt;/strong&gt;(centering matrix). &lt;/p&gt;
&lt;p&gt;由于之前没怎么遇到过, 故总结于此.&lt;/p&gt;
    
    </summary>
    
      <category term="数学基础" scheme="https://orzyt.cn/categories/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="中心矩阵" scheme="https://orzyt.cn/tags/%E4%B8%AD%E5%BF%83%E7%9F%A9%E9%98%B5/"/>
    
      <category term="协方差矩阵" scheme="https://orzyt.cn/tags/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5/"/>
    
  </entry>
  
  <entry>
    <title>深度域混淆</title>
    <link href="https://orzyt.cn/posts/deep-domain-confusion/"/>
    <id>https://orzyt.cn/posts/deep-domain-confusion/</id>
    <published>2019-03-28T06:46:01.000Z</published>
    <updated>2019-04-29T12:21:19.891Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文题目</strong>：Deep Domain Confusion: Maximizing for Domain Invariance</p><p><strong>论文作者</strong>：Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell</p><p><strong>会议期刊</strong>：arxiv 2014</p><a id="more"></a><hr><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文提出了一种新的卷积神经网络结构, 引入了<strong>自适应层</strong>(adaptation layer)和<strong>域混淆</strong>(domain confusion)损失函数, 从而可以学习到域不变(domain invariant)的特征表示.</p><p>同时, 本文指出域混淆度量可以用来进行模型的选择, 即确定自适应层的维度及其在CNN中的位置.</p><hr><h2 id="基于CNN的域不变表示"><a href="#基于CNN的域不变表示" class="headerlink" title="基于CNN的域不变表示"></a>基于CNN的域不变表示</h2><div align="center"><img src="https://wx3.sinaimg.cn/large/8662e3cely1g1ikol7jozj20nz0ffgns.jpg" width="50%" height="50%" target="_blank" rel="noopener noreferrer"></div><p>一个直观的想法是: <strong>学习可以最小化源域和目标域之间分布差异的特征表示, 使得可以在带标签的源域数据上训练分类器, 并直接应用在目标域的数据中.</strong></p><p>为此, 作者考虑使用<strong>最大均值差异</strong>(Maximum Mean Discrepancy, MMD)作为分布距离的度量. </p><script type="math/tex; mode=display">MMD(X_S, X_T) = \left\Vert \frac{1}{|X_S|} \sum \limits_{x_s \in X_S} \phi(x_s) - \frac{1}{|X_T|} \sum \limits_{x_t \in X_T} \phi(x_t) \right\Vert\tag{1}</script><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>最终的目标函数为:</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{C}(X_L, y) + \lambda MMD^2(X_S, X_T)\tag{2}</script><p>其中, $\mathcal{L}_{C}(X_L, y)$ 为带标签数据的分类损失, $MMD$ 为源域与目标域之间的分布差异, $\lambda$ 为权衡参数.</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><div align="center"><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cely1g1im7vuuzaj20g50hbgmj.jpg" alt="网络结构" width="40%" height="“40%”"></div><p>本文所使用的网络基于<code>AlexNet</code>修改而来, 使用两个<strong>共享权重</strong>的CNN, 并在全连接层<code>fc7</code>之后添加了自适应层. 经过<code>fine-tuning</code>后, 作者认为自适应层可以同时学习到<strong>具有判别性</strong>且<strong>具有域不变性</strong>的特征表示.</p><hr><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>本文采用<code>Office</code>数据集, 包含三个域: <code>Amazon</code>,<code>DSLR</code>和<code>Webcam</code>.</p><p>迁移任务: <code>Amazon-&gt;Webcam</code>, <code>DSLR-&gt;Webcam</code>, <code>Webcam-&gt;DSLR</code></p><h3 id="自适应层的位置"><a href="#自适应层的位置" class="headerlink" title="自适应层的位置"></a>自适应层的位置</h3><div align="center"><img rel="noreferrer" src="https://wx3.sinaimg.cn/large/8662e3cely1g1imw2hsyvj20if0emdgw.jpg" width="40%" height="40%"></div><p>作者测试了将自适应层放置在不同的全连接层后面, 根据MMD准则, 发现放置在<code>fc7</code>后效果最好.</p><h3 id="自适应层的维度"><a href="#自适应层的维度" class="headerlink" title="自适应层的维度"></a>自适应层的维度</h3><div align="center"><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cely1g1imx3kkifj20io0ekab8.jpg" width="40%" height="40%"></div><p>同时, 作者也测试了不同的自适应层维度, 根据MMD准则, 选择了<code>256</code>维.</p><h3 id="迁移效果"><a href="#迁移效果" class="headerlink" title="迁移效果"></a>迁移效果</h3><div align="center"><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cely1g1in4274mpj20dd05imxh.jpg" alt="有监督条件下的多分类精度" width="50%" height="50%"></div><div align="center"><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cely1g1in55orc2j20dk05ot91.jpg" alt="无监督条件下的多分类精度" width="50%" height="50%"></div><h3 id="正则化效果"><a href="#正则化效果" class="headerlink" title="正则化效果"></a>正则化效果</h3><div align="center"><img rel="noreferrer" src="https://wx2.sinaimg.cn/large/8662e3cely1g1in768my6j20ii0b53yx.jpg" alt="有无正则化的测试精度" width="50%" height="50%"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文题目&lt;/strong&gt;：Deep Domain Confusion: Maximizing for Domain Invariance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文作者&lt;/strong&gt;：Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;会议期刊&lt;/strong&gt;：arxiv 2014&lt;/p&gt;
    
    </summary>
    
      <category term="迁移学习" scheme="https://orzyt.cn/categories/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文笔记" scheme="https://orzyt.cn/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="迁移学习" scheme="https://orzyt.cn/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="领域自适应" scheme="https://orzyt.cn/tags/%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94/"/>
    
      <category term="深度域混淆" scheme="https://orzyt.cn/tags/%E6%B7%B1%E5%BA%A6%E5%9F%9F%E6%B7%B7%E6%B7%86/"/>
    
  </entry>
  
  <entry>
    <title>迁移成分分析</title>
    <link href="https://orzyt.cn/posts/transfer-component-analysis/"/>
    <id>https://orzyt.cn/posts/transfer-component-analysis/</id>
    <published>2019-03-26T12:34:13.000Z</published>
    <updated>2019-04-29T08:21:35.060Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文题目</strong>：Domain Adaptation via Transfer Component Analysis</p><p><strong>论文作者</strong>：Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok and Qiang Yang</p><p><strong>会议期刊</strong>：IJCAI 2009</p><a id="more"></a><hr><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>领域自适应(Domain Adaptation)</strong>的一个主要问题是如何减少源域和目标域之间的差异. </p><p>一个好的特征表示应该尽可能地减少域间分布差异, 同时保持原始数据重要的特性(如几何/统计特性等). </p><p>本文提出一个新的特征提取方式, 叫做<code>迁移成分分析</code>(transfer component analysis, TCA). </p><p>TCA学习所有域的公共迁移成分(即<strong>不会引起域间分布变化</strong>及<strong>保持原始数据固有结构</strong>的成分), 使得不同域在投影后的子空间中分布差异减少.</p><hr><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="问题设定"><a href="#问题设定" class="headerlink" title="问题设定"></a>问题设定</h3><p>源域(source domain)中有带标签数据 $\mathcal{D}_{S}$, 目标域(target domain)中只有大量无标签数据 $\mathcal{D}_{T}$. </p><ul><li><p>$\mathcal{D}_{S}=\left\{\left(x_{S_{1}}, y_{S_{1}}\right), \ldots,\left(x_{S_{n}}, y_{S_{n}}\right)\right\}$, $x_{S_{i}} \in \mathcal{X}$ 为输入数据, $y_{S_{i}} \in \mathcal{Y}$为对应的标签. </p></li><li><p>$\mathcal{D}_{T}=\left\{x_{T_{1}}, \ldots, x_{T_{n_{2}}}\right\}$, $x_{T_{i}} \in \mathcal{X}$.</p></li><li><p>$\mathcal{P(X_{S})}$ 和 $\mathcal{Q(X_{T})}$ 分别为 $X_S$ 和 $X_T$ 的边缘分布. </p></li></ul><p><strong>设定:</strong> 边缘分布不同 $\mathcal{P} \ne \mathcal{Q}$, 但类条件概率分布相同 $P(Y_{S} | X_{S}) = P(Y_{T} | X_{T})$.</p><p><strong>任务:</strong> 在目标域中预测输入数据 $x_{T_{i}}$ 对应的标签 $y_{T_{i}}$.</p><h3 id="最大均值差异"><a href="#最大均值差异" class="headerlink" title="最大均值差异"></a>最大均值差异</h3><p>我们知道, 存在很多准则可以度量不同分布之间的差异, 如, <code>KL散度</code>等.</p><p>但这些方法通常都需要对分布的概率密度进行估计, 因而是参数化的方法. 为了避免引入额外的参数, 我们希望寻找一种非参数化的方法来度量分布的差异.</p><p>在2006年, Borgwardt等人<sup><a href="#fn_1" id="reffn_1">1</a></sup>提出了一种基于<em>再生核希尔伯特空间(Reproducing Kernel Hilbert Space, RKHS)</em>的分布度量准则 <strong>最大均值差异(Maximum Mean Discrepancy, MMD)</strong>.</p><p>令 $X = \{ x_1, \cdots, x_{n1}\}$ 和 $Y = \{ y_1, \cdots, y_{n2}\}$ 为两个分布 $\mathcal{P}$ 和 $\mathcal{Q}$ 的随机变量集合, 根据MMD的定义, 两个分布的经验估计距离为:</p><script type="math/tex; mode=display">\operatorname{Dist}(\mathrm{X}, \mathrm{Y})=\left\|\frac{1}{n_{1}} \sum_{i=1}^{n_{1}} \phi\left(x_{i}\right)-\frac{1}{n_{2}} \sum_{i=1}^{n_{2}} \phi\left(y_{i}\right)\right\|_{\mathcal{H}}\tag{1}</script><p>其中, $\mathcal{H}$ 是再生核希尔伯特空间, $\phi : \mathcal{X} \to \mathcal{H}$ 为核函数映射.</p><hr><h2 id="迁移成分分析"><a href="#迁移成分分析" class="headerlink" title="迁移成分分析"></a>迁移成分分析</h2><p>令 $\phi : \mathcal{X} \to \mathcal{H}$ 为非线性映射, $X^{\prime}_{S} = \{ x^{\prime}_{S_{i}} \} = \{ \phi(x_{S_{i}}) \}$, $X^{\prime}_{T} = \{ x^{\prime}_{T_{i}} \} = \{ \phi(x_{T_{i}}) \}$, $X^{\prime} = X^{\prime}_{S} \cup X^{\prime}_{T}$ 分别为源域/目标域/结合域映射后的数据. </p><p>我们希望找到这样一个映射, 使得映射后的数据分布一致, 即 $\mathcal{P^{\prime}}(X^{\prime}_{S}) = \mathcal{Q^{\prime}}(X^{\prime}_{T})$.</p><p>根据MMD的定义, 我们可以通过<strong>度量两个域之间的经验均值的距离平方</strong>作为分布的距离.</p><script type="math/tex; mode=display">\operatorname{Dist}\left(X_{S}^{\prime}, X_{T}^{\prime}\right)=\left\|\frac{1}{n_{1}} \sum_{i=1}^{n_{1}} \phi\left(x_{S_{i}}\right)-\frac{1}{n_{2}} \sum_{i=1}^{n_{2}} \phi\left(x_{T_{i}}\right)\right\|_{\mathcal{H}}^{2}\tag{2}</script><p>通过最小化公式$(2)$, 我们可以找到想要的非线性映射 $\phi$.</p><p>然而, 对公式$(2)$直接优化是十分困难的, 通常会陷入局部极值点. 因此, 必须另辟蹊径.</p><h3 id="核学习"><a href="#核学习" class="headerlink" title="核学习"></a>核学习</h3><p>为了避免显式地直接寻找非线性变换 $\phi$, Pan等人<sup><a href="#fn_2" id="reffn_2">2</a></sup>将该问题转化为<strong>核学习</strong>(kernel learning)问题.</p><p>通过利用核技巧 $k(x_i, x_j) = \phi(x_i)^{\prime}\phi(x_j)$, 公式$(2)$中两个域之间的经验均值距离可以被写为:</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Dist}\left(X_{S}^{\prime}, X_{T}^{\prime}\right) &= \frac{1}{n^{2}_{1}} \sum_{i=1}^{n_1} \sum_{j=1}^{n_1} k\left(x_{S_i}, x_{S_j}\right)+\frac{1}{n^{2}_{2}} \sum_{i=1}^{n_2} \sum_{j=1}^{n_2} k\left(x_{T_i}, x_{T_j}\right)-\frac{2}{n_1 n_2} \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} k\left(x_{S_i}, x_{T_j}\right) \\&= \operatorname{tr}(K L) \\\end{aligned}\tag{3}</script><p>其中,</p><script type="math/tex; mode=display">K=\left[ \begin{array}{ll}{K_{S, S}} & {K_{S, T}} \\ {K_{T, S}} & {K_{T, T}}\end{array}\right]\tag{4}</script><p>为 $(n_1 + n_2) \times (n_1 + n_2)$ 大小的核矩阵, $K_{S,S}$, $K_{T,T}$, $K_{S,T}$ 分别为由 $k$ 定义在源域/目标域/跨域的核矩阵.</p><p>$L = [ L_{ij} ] \succeq 0$ 为半正定矩阵, 其中</p><script type="math/tex; mode=display">l_{i j}=\left\{\begin{array}{ll}{\frac{1}{n_{1}^{2}}} & {x_{i}, x_{j} \in \mathcal{D}_{s}} \\ {\frac{1}{n_{2}^{2}}} & {x_{i}, x_{j} \in \mathcal{D}_{t}} \\ {-\frac{1}{n_{1} n_{2}}} & {\text { otherwise }}\end{array}\right.\tag{5}</script><p>在直推学习的设置下(直推学习即<strong>假设未标记的数据就是最终要用来测试的数据, 学习的目的即为在这些数据上取得最佳泛化能力</strong>), 核函数 $k(\cdot, \cdot)$ 可以通过求解核矩阵 $K$ 替代. </p><p>Pan等人<sup><a href="#fn_2" id="reffn_2">2</a></sup>将核矩阵学习问题形式化为<strong>半定规划</strong>(semi-definite program, SDP)问题. 然后, 对学习到的核矩阵使用PCA方法得到跨域的低维隐空间.</p><h3 id="参数化核映射"><a href="#参数化核映射" class="headerlink" title="参数化核映射"></a>参数化核映射</h3><p>MMDE<sup><a href="#fn_2" id="reffn_2">2</a></sup>的方法存在如下局限性:</p><ul><li>它是直推式的, 不能泛化到未见的样本中</li><li>公式$(3)$中的$K$需要是半正定的, 且求解SDP问题十分耗时</li><li>为了构造低维表示, 需要对$K$进行PCA处理, 这会损失$K$中的信息</li></ul><p>本文提出一种基于核特征提取来寻找非线性映射 $\phi$ 的高效方法.</p><ul><li>避免求解SDP问题, 减轻计算负担</li><li>学习到的核函数$k$可以泛化到未见的样本</li><li>利用显式的低秩表示得到统一的核学习方法</li></ul><p>首先, 公式$(4)$中的核矩阵 $K$ 可以被分解为 $K = (KK^{-1/2})(K^{-1/2}K)$, 这通常称为<strong>经验核映射</strong>(empirical kernel map)<sup><a href="#fn_3" id="reffn_3">3</a></sup>. </p><blockquote><p><font color="blue">★注:</font> 这个分解可由矩阵的特征分解得到, 即令 $K = Q^{-1}\Lambda Q$ 代入.<br>至于为什么要对核矩阵 $K$ 进行分解, 可以这样理解, 核矩阵给出的是映射后数据的内积, 即 $K_{ij} = k(x_i, x_j)$, 但我们只想知道映射后的数据 $\phi(x)$ 该怎么办? 便可以将矩阵分解成 $K=A^TA$ 的形式, 使得 $A = [ \phi(x_1), \cdots, \phi(x_{n_1 + n_2}) ]$, 即 $A$ 中的每个元素都是映射后的数据.<br>在上述的分解中, $A$ 即为 $K^{-1/2}K$. 注意到 $K$ 为对称半正定矩阵, 因此 $A^T = (K^{-1/2}K)^T = KK^{-1/2}$.</p></blockquote><p>考虑使用 $(n_1 + n_2) \times m$ 维的矩阵 $\widetilde{W}$ 将特征变化到 $m$ 维空间 (通常 $m \ll n_1 + n_2$), 则得到的核矩阵为:</p><script type="math/tex; mode=display">\widetilde{K} = (KK^{-1/2}\widetilde{W})(\widetilde{W}^TK^{-1/2}K) = KWW^TK\tag{6}</script><p>其中, $W = K^{-1/2}\widetilde{W} \in \mathbb{R}^{(n_1 + n_2) \times m}$. 特别地, 任意两个数据 $x_i$ 和 $x_j$ 的核函数为:</p><script type="math/tex; mode=display">\tilde{k}(x_i, x_j) = k^{T}_{x_i}WW^Tk_{x_j}\tag{7}</script><p>其中, $k_x = [ k(x_1, x), \cdots, k(x_{n_1 + n_2}, x)]^T \in \mathbb{R}^{n_1 + n_2}$. 因此, 公式$(7)$中的核函数给出了未见样本的参数化核估计表示.</p><p>此外, 根据公式$(6)$中$\widetilde{K}$的定义, 两个域之间的经验均值距离可重新写为:</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{Dist}\left(X_{S}^{\prime}, X_{T}^{\prime}\right) &=\operatorname{tr}\left(\left(K W W^{\top} K\right) L\right) \\ &=\operatorname{tr}\left(W^{\top} K L K W\right) \\& {\scriptsize //利用迹循环性质:tr(ABC)=tr(BCA)=tr(CAB)}\end{aligned}\tag{8}</script><h3 id="迁移成分提取"><a href="#迁移成分提取" class="headerlink" title="迁移成分提取"></a>迁移成分提取</h3><p>在最小化公式$(8)$的时候, 通常需要加一个正则项 $tr(W^TW)$ (即矩阵二范数 $\Vert W \Vert_{2}$)来控制参数 $W$ 的复杂度.</p><p>从而, 领域自适应的核学习问题可变为:</p><script type="math/tex; mode=display">\begin{array}{cl}{\min \limits_{W}} & {\operatorname{tr}\left(W^{\top} W\right)+\mu \operatorname{tr}\left(W^{\top} K L K W\right)} \\{\text { s.t. }} & {W^{\top} K H K W=I}\end{array}\tag{9}</script><p>其中, $\mu$ 为权衡参数, $I \in \mathbb{R}^{m \times m}$ 为单位阵, $H = I_{n_1 + n_2} - \frac{1}{n_1 + n_2} \mathrm{1}\mathrm{1}^T$ 为<strong>中心矩阵</strong>(centering matrix), $\mathrm{1} \in \mathbb{R}^{n_1 + n_2}$ 为全1列向量, $I_{n_1 + n_2} \in \mathbb{R}^{(n_1 + n_2) \times (n_1 + n_)}$ 为单位阵.</p><blockquote><p><font color="blue">★注:</font> 添加 $W^{\top} K H K W=I$ 限制条件一方面是为了<strong>避免平凡解</strong>(即$W = 0$), 另一方面是为了<strong>保持数据的散度</strong>($W^{\top} K H K W$为投影后数据$W^{\top} K$的散度矩阵), 即前面简介中所说的保持原始数据固有结构.</p></blockquote><p>尽管公式$(9)$为非凸优化问题, 但其可以转化为迹优化问题:</p><script type="math/tex; mode=display">\min _{W} \operatorname{tr}\left(\left(W^{\top} K H K W\right)^{\dagger} W^{\top}(I+\mu K L K) W\right)\tag{10}</script><p>或者</p><script type="math/tex; mode=display">\max _{W} \operatorname{tr}\left(\left(W^{\top}(I+\mu K L K) W\right)^{-1} W^{\top} K H K W\right)\tag{11}</script><p>上述转化可由拉格朗日乘子法得到, 具体证明略…</p><p>类似于核Fisher判别, <strong>公式$(11)$中 $W$ 的解为 $(I + \mu KLK)^{-1}KHK$ 的前 $m$ 个特征值对应的特征向量</strong>. </p><p>因此, 本文提出的方法命名为<strong>迁移成分分析</strong>(Transfer Component Analysis, TCA).</p><hr><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="toy-dataset"><a href="#toy-dataset" class="headerlink" title="toy dataset"></a>toy dataset</h3><p><img rel="noreferrer" src="https://wx3.sinaimg.cn/large/8662e3cegy1g1hftr8fffj216z0e8dk4.jpg" alt="(左) toy dataset   (中) PCA   (右) TCA" width="100%" height="100%"></p><h3 id="跨域WiFi定位"><a href="#跨域WiFi定位" class="headerlink" title="跨域WiFi定位"></a>跨域WiFi定位</h3><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g1hf7xqzy5j217g0digpo.jpg" width="100%" height="100%"></p><h3 id="跨域文本分类"><a href="#跨域文本分类" class="headerlink" title="跨域文本分类"></a>跨域文本分类</h3><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g1hf9nurmyj216f0kewj1.jpg" width="100%" height="100%"></p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. Karsten M. Borgwardt, Arthur Gretton, Malte J. Rasch, Hans-Peter Kriegel, Bernhard Scholkopf, and Alexander J. Smola. <strong>Integrating structured biological data by kernel maximum mean discrepancy</strong>. In ISMB, pages 49–57, Fortaleza, Brazil, 2006<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Sinno Jialin Pan, James T. Kwok, and Qiang Yang. <strong>Transfer learning via dimensionality reduction</strong>. In Proceedings of AAAI, pages 677–682, Chicago, Illinois, USA, 2008.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. <strong>Nonlinear component analysis as a kernel eigenvalue problem</strong>. Neural Computation, 10(5):1299–1319,1998.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文题目&lt;/strong&gt;：Domain Adaptation via Transfer Component Analysis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文作者&lt;/strong&gt;：Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok and Qiang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;会议期刊&lt;/strong&gt;：IJCAI 2009&lt;/p&gt;
    
    </summary>
    
      <category term="迁移学习" scheme="https://orzyt.cn/categories/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文笔记" scheme="https://orzyt.cn/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="迁移学习" scheme="https://orzyt.cn/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="领域自适应" scheme="https://orzyt.cn/tags/%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94/"/>
    
      <category term="迁移成分分析" scheme="https://orzyt.cn/tags/%E8%BF%81%E7%A7%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
      <category term="TCA" scheme="https://orzyt.cn/tags/TCA/"/>
    
  </entry>
  
  <entry>
    <title>迁移自适应学习综述</title>
    <link href="https://orzyt.cn/posts/transfer-adaptation-learning/"/>
    <id>https://orzyt.cn/posts/transfer-adaptation-learning/</id>
    <published>2019-03-20T14:44:46.000Z</published>
    <updated>2019-04-02T13:00:17.720Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文题目</strong>：Transfer Adaptation Learning: A Decade Survey</p><p><strong>论文作者</strong>：Lei Zhang</p><p><strong>会议期刊</strong>：arxiv 2019</p><a id="more"></a><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在很多实际的情况中, <strong>源域(source domain)</strong>和<strong>目标域(target domain)</strong>之间存在：</p><ul><li><strong>分布不匹配(distribution mismatch)</strong></li><li><strong>领域偏移(domain shift)</strong></li></ul><p><strong>独立同分布(independent identical distribution, i.i.d)</strong>的假设不再满足!</p><ul><li><p><strong>迁移学习(transfer learning)</strong>假设源域与目标域拥有不同的<strong>联合概率分布</strong> </p><script type="math/tex; mode=display">P(X_{source}, Y_{source}) \neq P(X_{target}, Y_{target})</script></li><li><p><strong>领域自适应(domain adaptation)</strong>假设源域与目标域拥有不同的<strong>边缘概率分布</strong>, 但拥有相同的<strong>条件概率分布</strong></p><script type="math/tex; mode=display">P(X_{source}) \neq P(X_{target}), P(Y_{source} | X_{source}) = P(Y_{target} | X_{target})</script></li></ul><hr><h2 id="实例权重调整自适应"><a href="#实例权重调整自适应" class="headerlink" title="实例权重调整自适应"></a>实例权重调整自适应</h2><p>当训练集和测试集来自不同分布时, 这通常被称为<strong>采样选择偏差(sample selection bias)</strong>或者<strong>协方差偏移(covariant shift)</strong>. </p><p>实例权重调整方法旨在<strong>通过非参数方式对跨域特征分布匹配直接推断出重采样的权重</strong>. </p><h3 id="基于直觉的权重调整"><a href="#基于直觉的权重调整" class="headerlink" title="基于直觉的权重调整"></a>基于直觉的权重调整</h3><p>直接对原始数据进行权重调整. </p><p>首次提出于NLP领域<sup><a href="#fn_1" id="reffn_1">1</a></sup>, 主要的方法有著名的<code>TrAdaBoost</code><sup><a href="#fn_2" id="reffn_2">2</a></sup>.</p><h3 id="基于核映射的权重调整"><a href="#基于核映射的权重调整" class="headerlink" title="基于核映射的权重调整"></a>基于核映射的权重调整</h3><p>将原始数据映射到高维空间(如,再生核希尔伯特空间RKHS)中进行权重调整.</p><h4 id="分布匹配"><a href="#分布匹配" class="headerlink" title="分布匹配"></a>分布匹配</h4><p>主要思想是<strong>通过重新采样源数据的权重来匹配再生核希尔伯特空间中源数据和目标数据之间的均值</strong>. </p><p>主要有两种非参数统计量来衡量分布差异:</p><ul><li><strong>核均值匹配(kernel mean matching, KMM)</strong></li></ul><script type="math/tex; mode=display">\begin{array}{l}{\min \limits_{\beta}\left\|E_{x^{\prime} \sim P_{r}^{\prime}}\left[\Phi\left(x^{\prime}\right)\right]-E_{x \sim P_{r}}[\beta(x) \Phi(x)]\right\|} \\{\text {s.t.} \quad \beta(x) \geq 0, E_{x \sim P_{r}}[\beta(x)]=1}\end{array}</script><p>Huang等人<sup><a href="#fn_3" id="reffn_3">3</a></sup>首次提出通过调整源样本的$\beta$权重系数, 使得带权源样本和目标样本的KMM最小.</p><ul><li><strong>最大均值差异(maximum mean discrepancy, MMD)</strong><sup><a href="#fn_4" id="reffn_4">4</a></sup><sup><a href="#fn_5" id="reffn_5">5</a></sup></li></ul><script type="math/tex; mode=display">d_{\mathcal{H}}^{2}\left(\mathcal{D}_{s}, \mathcal{D}_{t}\right)=\left\|\frac{1}{M} \sum_{i=1}^{M} \phi\left(x_{i}^{s}\right)-\frac{1}{N} \sum_{j=1}^{N} \phi\left(x_{j}^{t}\right)\right\|_{\mathcal{H}}^{2}</script><p><code>weighted MMD</code><sup><a href="#fn_6" id="reffn_6">6</a></sup>方法考虑了类别权重偏差.</p><h4 id="样本选择"><a href="#样本选择" class="headerlink" title="样本选择"></a>样本选择</h4><p>主要方法有基于k-means聚类的<code>KMapWeighted</code><sup><a href="#fn_7" id="reffn_7">7</a></sup>, 基于MMD和$\ell_{2,1}$-norm的<code>TJM</code><sup><a href="#fn_8" id="reffn_8">8</a></sup>等.</p><h3 id="协同训练"><a href="#协同训练" class="headerlink" title="协同训练"></a>协同训练</h3><p>主要思想是假设数据集被表征为两个不同的视角, 使两个分类器独立地从每个视角中进行学习.</p><p>主要方法有<code>CODA</code><sup><a href="#fn_9" id="reffn_9">9</a></sup>, 以及基于GAN的<code>RANN</code><sup><a href="#fn_10" id="reffn_10">10</a></sup>等.</p><hr><h2 id="特征自适应"><a href="#特征自适应" class="headerlink" title="特征自适应"></a>特征自适应</h2><p>特征自适应方法旨在<strong>寻找多源数据(multiple sources)的共同特征表示</strong>.</p><h3 id="基于特征子空间"><a href="#基于特征子空间" class="headerlink" title="基于特征子空间"></a>基于特征子空间</h3><p>该方法假设<strong>数据可以被低维线性子空间进行表示, 即低维的格拉斯曼流形(Grassmann manifold)被嵌入到高维数据中</strong>. </p><p>通常用PCA方法来构造该流形, 使得源域和目标域可以看成流形上的两个点, 并得到两者的测地线距离(geodesic flow).</p><ul><li><p>基于流形的方法有<code>SGF</code><sup><a href="#fn_11" id="reffn_11">11</a></sup>和<code>GFK</code><sup><a href="#fn_12" id="reffn_12">12</a></sup></p></li><li><p>基于子空间对齐的方法有<code>SA</code><sup><a href="#fn_13" id="reffn_13">13</a></sup>,<code>SDA</code><sup><a href="#fn_14" id="reffn_14">14</a></sup>和<code>GTH</code><sup><a href="#fn_15" id="reffn_15">15</a></sup></p></li></ul><h3 id="基于特征变换"><a href="#基于特征变换" class="headerlink" title="基于特征变换"></a>基于特征变换</h3><p>特征变换方法旨在<strong>学习变换或投影矩阵,使得源域和目标域中的数据在某种分布度量准则下更接近</strong>.</p><h4 id="基于投影"><a href="#基于投影" class="headerlink" title="基于投影"></a>基于投影</h4><p>该方法通过减少不同域之间的边缘分布和条件分布差异, 求解出最优的投影矩阵.</p><p>主要方法有:</p><ul><li><p>基于边缘分布MMD的<code>TCA</code><sup><a href="#fn_16" id="reffn_16">16</a></sup>, 条件分布MMD的<code>JDA</code><sup><a href="#fn_17" id="reffn_17">17</a></sup></p></li><li><p>基于布拉格曼散度(Bregman divergence)的<code>TSL</code><sup><a href="#fn_18" id="reffn_18">18</a></sup></p></li><li><p>基于希尔伯特-施密特独立性(Hilbert-Schmidt Independence Criterion)<sup><a href="#fn_19" id="reffn_19">19</a></sup></p></li></ul><h4 id="基于度量"><a href="#基于度量" class="headerlink" title="基于度量"></a>基于度量</h4><p>该方法通过在带标签的源域中学习一个好的距离度量, 使得其能够应用于相关但不同的目标域中.</p><p>主要方法有:</p><ul><li><p>基于一阶统计量的<code>RTML</code><sup><a href="#fn_20" id="reffn_20">20</a></sup></p></li><li><p>基于二阶统计量的<code>CORAL</code><sup><a href="#fn_21" id="reffn_21">21</a></sup></p></li></ul><h4 id="基于增强"><a href="#基于增强" class="headerlink" title="基于增强"></a>基于增强</h4><p>该方法假设数据的特征被分为三种类型:公共特征/源域特征/目标域特征.</p><p>主要方法有:</p><ul><li>基于零填充(Zero Padding)的<code>EasyAdapt(EA)</code><sup><a href="#fn_22" id="reffn_22">22</a></sup></li><li>基于生成式模型(Generative Model)<sup><a href="#fn_23" id="reffn_23">23</a></sup></li></ul><h3 id="基于特征重构"><a href="#基于特征重构" class="headerlink" title="基于特征重构"></a>基于特征重构</h3><p>主要方法有:</p><ul><li>低秩重构(Low-rank Reconstruction)<sup><a href="#fn_24" id="reffn_24">24</a></sup></li><li>稀疏重构(Sparse Reconstruction)<sup><a href="#fn_25" id="reffn_25">25</a></sup></li></ul><h3 id="基于特征编码"><a href="#基于特征编码" class="headerlink" title="基于特征编码"></a>基于特征编码</h3><p>主要方法有:</p><ul><li>共享域字典(Domain-shared dictionary)<sup><a href="#fn_26" id="reffn_26">26</a></sup></li><li>指定域字典(Domain-specific dictionary)<sup><a href="#fn_27" id="reffn_27">27</a></sup></li></ul><hr><h2 id="分类器自适应"><a href="#分类器自适应" class="headerlink" title="分类器自适应"></a>分类器自适应</h2><p>分类器自适应旨在<strong>利用源域中带标签数据和目标域中少量带标签数据学习一个通用的分类器</strong>.</p><h3 id="基于核分类器"><a href="#基于核分类器" class="headerlink" title="基于核分类器"></a>基于核分类器</h3><p>主要方法有:</p><ul><li><p>自适应支持向量机(adaptive support vector machine, ASVM)<sup><a href="#fn_28" id="reffn_28">28</a></sup></p></li><li><p>基于多核学习(multiple kernel learning, MKL)的域迁移分类器<sup><a href="#fn_29" id="reffn_29">29</a></sup></p></li></ul><h3 id="基于流形正则项"><a href="#基于流形正则项" class="headerlink" title="基于流形正则项"></a>基于流形正则项</h3><p>主要方法有<code>ARTL</code><sup><a href="#fn_30" id="reffn_30">30</a></sup>,<code>DMM</code><sup><a href="#fn_31" id="reffn_31">31</a></sup>,<code>MEDA</code><sup><a href="#fn_32" id="reffn_32">32</a></sup>等.</p><h3 id="基于贝叶斯分类器"><a href="#基于贝叶斯分类器" class="headerlink" title="基于贝叶斯分类器"></a>基于贝叶斯分类器</h3><p>主要方法有核贝叶斯迁移学习<code>KBTL</code><sup><a href="#fn_33" id="reffn_33">33</a></sup>等.</p><hr><h2 id="深度网络自适应"><a href="#深度网络自适应" class="headerlink" title="深度网络自适应"></a>深度网络自适应</h2><p>2014年, Yosinski等人<sup><a href="#fn_34" id="reffn_34">34</a></sup>讨论了深度神经网络中不同层特征的可迁移特性. </p><h3 id="基于边缘分布对齐"><a href="#基于边缘分布对齐" class="headerlink" title="基于边缘分布对齐"></a>基于边缘分布对齐</h3><p>主要方法有:</p><ul><li>深度域混淆<code>DDC</code><sup><a href="#fn_35" id="reffn_35">35</a></sup></li><li>深度自适应网络<code>DAN</code><sup><a href="#fn_36" id="reffn_36">36</a></sup></li><li>联合自适应网络<code>JAN</code><sup><a href="#fn_37" id="reffn_37">37</a></sup>, 同时提出了<code>Joint MMD</code>准则</li></ul><h3 id="基于条件分布对齐"><a href="#基于条件分布对齐" class="headerlink" title="基于条件分布对齐"></a>基于条件分布对齐</h3><p>主要方法有深度迁移网络<code>DTN</code><sup><a href="#fn_38" id="reffn_38">38</a></sup></p><h3 id="基于自动编码器"><a href="#基于自动编码器" class="headerlink" title="基于自动编码器"></a>基于自动编码器</h3><p>主要方法有边缘堆叠式降噪自动编码器<code>mSDA</code><sup><a href="#fn_39" id="reffn_39">39</a></sup></p><hr><h2 id="对抗式自适应"><a href="#对抗式自适应" class="headerlink" title="对抗式自适应"></a>对抗式自适应</h2><p>通过对抗目标(如,域判别器)来减少域间差异.</p><h3 id="基于梯度转换"><a href="#基于梯度转换" class="headerlink" title="基于梯度转换"></a>基于梯度转换</h3><p>Ganin等人<sup><a href="#fn_40" id="reffn_40">40</a></sup>首次提出可以通过添加一个简单的<strong>梯度转换层(gradient reversal layer, GRL)</strong>来实现领域自适应.</p><h3 id="基于Minimax优化"><a href="#基于Minimax优化" class="headerlink" title="基于Minimax优化"></a>基于Minimax优化</h3><p>Ajakan等人<sup><a href="#fn_41" id="reffn_41">41</a></sup>首次结合分类损失和对抗目标, 提出了<code>DANN</code>方法.</p><p>其它方法还有:</p><ul><li><p>对抗判别领域自适应<code>ADDA</code><sup><a href="#fn_42" id="reffn_42">42</a></sup></p></li><li><p>条件领域对抗网络<code>CDAN</code><sup><a href="#fn_43" id="reffn_43">43</a></sup></p></li><li><p>最大分类器差异<code>MCD</code><sup><a href="#fn_44" id="reffn_44">44</a></sup></p></li></ul><h3 id="基于生成对抗网络"><a href="#基于生成对抗网络" class="headerlink" title="基于生成对抗网络"></a>基于生成对抗网络</h3><p>主要方法有:</p><ul><li><p><code>CyCADA</code><sup><a href="#fn_45" id="reffn_45">45</a></sup></p></li><li><p><code>Duplex GAN</code><sup><a href="#fn_46" id="reffn_46">46</a></sup></p></li><li><p>…</p></li></ul><h2 id="基准数据集"><a href="#基准数据集" class="headerlink" title="基准数据集"></a>基准数据集</h2><ul><li>Office-31 (3DA) </li><li>Office+Caltech-10 (4DA)</li><li>MNIST+USPS</li><li>Multi-PIE</li><li>COIL-20</li><li>MSRC+VOC2007</li><li>IVLSC</li><li>Cross-dataset Testbed</li><li>Office Home<sup><font color="red">NEW</font></sup></li><li>ImageCLEF</li><li>P-A-C-S<sup><font color="red">NEW</font></sup></li></ul><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. J. Jiang and C. Zhai, <strong>Instance weighting for domain adaptation in nlp</strong>, in ACL, 2007, pp. 264–271.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. W. Dai, Q. Yang, G. R. Xue, and Y. Yu, <strong>Boosting for transfer learning</strong>, in ICML, 2007, pp. 193–200.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. J. Huang, A. Smola, A. Gretton, K. Borgwardt, and B. Scholkopf, <strong>Correcting sample selection bias by unlabeled data</strong>, in NIPS, 2007, pp. 1–8.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. A. Gretton, K. Borgwardt, M. Rasch, B. Schoelkopf, and A. Smola, <strong>A kernel method for the two-sample-problem</strong>, in NIPS, 2006.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola, <strong>A kernel two-sample test</strong>, Journal of Machine Learning Research, pp. 723–773, 2012<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo, <strong>Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</strong>, in CVPR, 2017, pp. 2272–2281<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. E. H. Zhong, W. Fan, J. Peng, K. Zhang, J. Ren, D. S. Turaga, and O. Verscheure, <strong>Cross domain distribution adaptation via kernel mapping</strong>, in ACM SIGKDD, 2009, pp. 1027–1036.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu, <strong>Transfer joint matching for unsupervised domain adaptation</strong>, in CVPR, 2014, pp. 1410–1417.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. M. Chen, K. Q. Weinberger, and J. C. Blitzer, <strong>Co-training for domain adaptation</strong>, in NIPS, 2011.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. Q. Chen, Y. Liu, Z. Wang, I. Wassell, and K. Chetty, <strong>Re-weighted adversarial adaptation network for unsupervised domain adaptation</strong>, in CVPR, 2018, pp. 7976–7985.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. R. Gopalan, R. Li, and R. Chellappa, <strong>Domain adaptation for object recognition: An unsupervised approach</strong>, in ICCV, 2011, pp. 999–1006<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. B. Gong, Y. Shi, F. Sha, and K. Grauman, <strong>Geodesic flow kernel for unsupervised domain adaptation</strong>, in CVPR, 2012, pp. 2066–2073<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars, <strong>Unsupervised visual domain adaptation using subspace alignment</strong>, in ICCV, 2013, pp. 2960–2967.<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. B. Sun and K. Saenko, <strong>Subspace distribution alignment for unsupervised domain adaptation</strong>, in BMVC, 2015, pp. 24.1–24.10.<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. J. Liu and L. Zhang, <strong>Optimal projection guided transfer hashing for image retrieval</strong>, in AAAI, 2018.<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, <strong>Domain adaptation via transfer component analysis</strong>, IEEE Trans. Neural Networks, vol. 22, no. 2, p. 199, 2011<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>.  M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu, <strong>Transfer feature learning with joint distribution adaptation</strong>, in ICCV, 2014, pp. 2200–2207.<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. S. Si, D. Tao, and B. Geng, <strong>Bregman divergence-based regularization for transfer subspace learning</strong>, IEEE Trans. Knowledge and Data Engineering, vol. 22, no. 7, pp. 929–942, 2010.<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. A. Gretton, O. Bousquet, A. Smola, and B. Scholkopf, <strong>Measuring statistical dependence with hilbert-schmidt norms</strong>, in ALT, 2005.<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. Z. Ding and Y. Fu, <strong>Robust transfer metric learning for image classification</strong>, IEEE Trans. Image Processing, vol. 26, no. 2, p. 660670, 2017.<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. B. Sun, J. Feng, and K. Saenko, <strong>Return of frustratingly easy domain adaptation</strong>, in AAAI, 2016, pp. 153–171.<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. H. Daume III, <strong>Frustratingly easy domain adaptation</strong>, in arXiv,2009.<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. R. Volpi, P. Morerio, S. Savarese, and V. Murino, <strong>Adversarial feature augmentation for unsupervised domain adaptation</strong>, in CVPR, 2018, pp. 5495–5504.<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. I. H. Jhuo, D. Liu, D. T. Lee, and S. F. Chang, <strong>Robust visual domain adaptation with low-rank reconstruction</strong>, in CVPR, 2012, pp. 2168–2175.<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. L. Zhang, W. Zuo, and D. Zhang, <strong>Lsdt: Latent sparse domain transfer learning for visual adaptation</strong>, IEEE Trans. Image Processing, vol. 25, no. 3, pp. 1177–1191, 2016.<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. S. Shekhar, V. Patel, H. Nguyen, and R. Chellappa, <strong>Generalized domain-adaptive dictionaries</strong>, in CVPR, 2013, pp. 361–368.<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. F. Zhu and L. Shao, <strong>Weakly-supervised cross-domain dictionary learning for visual recognition</strong>, International Journal of Computer Vision, vol. 109, no. 1-2, pp. 42–59, 2014.<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. J. Yang, R. Yan, and A. G. Hauptmann, <strong>Cross-domain video concept detection using adaptive svms</strong>, in ACM MM, 2007, pp. 188–197.<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>.  L. Duan, I. Tsang, D. Xu, and S. Maybank, <strong>Domain transfer svm for video concept detection</strong>, in CVPR, 2009<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. M. Long, J. Wang, G. Ding, S. Pan, and P. Yu, <strong>Adaptation regularization: a general framework for transfer learning</strong>, IEEE Trans. Knowledge and Data Engineering, vol. 26, no. 5, p. 10761089, 2014.<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. Y. Cao, M. Long, and J. Wang, <strong>Unsupervised domain adaptation with distribution matching machines</strong>, in AAAI, 2018<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. J. Wang, W. Feng, Y. Chen, H. Yu, M. Huang, and P. S. Yu, <strong>Visual domain adaptation with manifold embedded distribution alignment</strong>, 2018.<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote><blockquote id="fn_33"><sup>33</sup>. M. Gonen and A. Margolin, <strong>Kernelized bayesian transfer learning</strong>, in AAAI, 2014, pp. 1831–1839.<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a></blockquote><blockquote id="fn_34"><sup>34</sup>.  J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, <strong>How transferable are features in deep neural networks</strong>, in NIPS, 2014.<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a></blockquote><blockquote id="fn_35"><sup>35</sup>. E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, <strong>Deep domain confusion: Maximizing for domain invariance</strong>, arXiv, 2014<a href="#reffn_35" title="Jump back to footnote [35] in the text."> &#8617;</a></blockquote><blockquote id="fn_36"><sup>36</sup>.  M. Long, Y. Cao, J. Wang, and M. I. Jordan, <strong>Learning transferable features with deep adaptation networks</strong>, in ICML, 2015, pp. 97–105.<a href="#reffn_36" title="Jump back to footnote [36] in the text."> &#8617;</a></blockquote><blockquote id="fn_37"><sup>37</sup>. M. Long, H. Zhu, J. Wang, and M. Jordan, <strong>Deep transfer learning with joint adaptation networks</strong>, in ICML, 2017.<a href="#reffn_37" title="Jump back to footnote [37] in the text."> &#8617;</a></blockquote><blockquote id="fn_38"><sup>38</sup>. X. Zhang, F. Yu, S. Wang, and S. Chang, <strong>Deep transfer network: Unsupervised domain adaptation</strong>, in arXiv, 2015.<a href="#reffn_38" title="Jump back to footnote [38] in the text."> &#8617;</a></blockquote><blockquote id="fn_39"><sup>39</sup>. M. Chen, Z. Xu, K. Weinberger, and F. Sha, <strong>Marginalized denoising autoencoders for domain adaptation</strong>, in ICML, 2012<a href="#reffn_39" title="Jump back to footnote [39] in the text."> &#8617;</a></blockquote><blockquote id="fn_40"><sup>40</sup>. Y. Ganin and V. Lempitsky, <strong>Unsupervised domain adaptation by backpropagation</strong>, in arXiv, 2015.<a href="#reffn_40" title="Jump back to footnote [40] in the text."> &#8617;</a></blockquote><blockquote id="fn_41"><sup>41</sup>. H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, and M. Marchand, <strong>Domain-adversarial neural network</strong>, in arXiv, 2015<a href="#reffn_41" title="Jump back to footnote [41] in the text."> &#8617;</a></blockquote><blockquote id="fn_42"><sup>42</sup>. E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, <strong>Adversarial discriminative domain adaptation</strong>, in CVPR, 2017, pp. 7167–7176<a href="#reffn_42" title="Jump back to footnote [42] in the text."> &#8617;</a></blockquote><blockquote id="fn_43"><sup>43</sup>. M. Long, Z. Cao, J. Wang, and M. I. Jordan, <strong>Conditional adversarial domain adaptation</strong>, in NIPS, 2018.<a href="#reffn_43" title="Jump back to footnote [43] in the text."> &#8617;</a></blockquote><blockquote id="fn_44"><sup>44</sup>. K. Saito, K. Watanabe, Y. Ushiku, and T. Harada, <strong>Maximum classifier discrepancy for unsupervised domain adaptation</strong>, in CVPR, 2018, pp. 3723–3732.<a href="#reffn_44" title="Jump back to footnote [44] in the text."> &#8617;</a></blockquote><blockquote id="fn_45"><sup>45</sup>. J. Hoffman, E. Tzeng, T. Park, and J. Zhu, <strong>Cycada: Cycleconsistent adversarial domain adaptation</strong>, in ICML, 2018.<a href="#reffn_45" title="Jump back to footnote [45] in the text."> &#8617;</a></blockquote><blockquote id="fn_46"><sup>46</sup>. L. Hu, M. Kan, S. Shan, and X. Chen, <strong>Duplex generative adversarial network for unsupervised domain adaptation</strong>, in CVPR, 2018, pp. 1498–1507.<a href="#reffn_46" title="Jump back to footnote [46] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文题目&lt;/strong&gt;：Transfer Adaptation Learning: A Decade Survey&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文作者&lt;/strong&gt;：Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;会议期刊&lt;/strong&gt;：arxiv 2019&lt;/p&gt;
    
    </summary>
    
      <category term="迁移学习" scheme="https://orzyt.cn/categories/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文笔记" scheme="https://orzyt.cn/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="迁移学习" scheme="https://orzyt.cn/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="领域自适应" scheme="https://orzyt.cn/tags/%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94/"/>
    
  </entry>
  
  <entry>
    <title>强化学习实践（二）：Gridworld</title>
    <link href="https://orzyt.cn/posts/gridworld/"/>
    <id>https://orzyt.cn/posts/gridworld/</id>
    <published>2019-03-13T11:31:33.000Z</published>
    <updated>2019-04-29T08:20:05.991Z</updated>
    
    <content type="html"><![CDATA[<p>《Reinforcement Learning: An Introduction》在第三章中给出了一个简单的例子:<code>Gridworld</code>, 以帮助我们理解<code>finite MDPs</code>, 同时也求解了该问题的<strong>贝尔曼期望方程</strong>和<strong>贝尔曼最优方程</strong>. 本文简要说明如何进行编程求解.</p><a id="more"></a><hr><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><blockquote><p>下图用一个矩形网格展示了一个简单finite MDP - <code>Gridworld</code>.<br>网格中的每一格对应于environment的一个state.<br>在每一格, 有四种可能的actions：<code>上/下/左/右</code>, 对应于agent往相应的方向移动一个单元格.<br>使agent离开网格的actions会使得agent留在原来的位置, 但是会有一个值为<code>-1</code>的reward.<br>除了那些使得agent离开<code>state A</code>和<code>state B</code>的action, 其他的actions对应的reward都是<code>0</code>.<br>处在<code>state A</code>时, 所有的actions会有值为<code>+10</code>的reward, 并且agent会移动到<code>state A&#39;</code>.<br>处在<code>state B</code>时, 所有的actions会有值为<code>+5</code>的reward, 并且agent会移动到<code>state B&#39;</code>.</p></blockquote><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g11f0p6x8vj20al05yq2y.jpg" alt="Girdworld示意图" width="40%" height="40%"></p><hr><h2 id="元素"><a href="#元素" class="headerlink" title="元素"></a>元素</h2><ul><li><strong>状态(State)</strong>: 网格的坐标, 共 $5 \times 5 = 25$ 个状态;</li><li><strong>动作(Action)</strong>: <code>上/下/左/右</code>四种动作;</li><li><strong>策略(Policy)</strong>: $\pi(a | s) = \frac14 \; \text{for} \; \forall s \in S, \text{and} \; \forall \; a \in \{↑,↓,←,→\}$;</li><li><strong>奖励(Reward)</strong>: 如题所述;</li><li><strong>折扣因子(Discount rate)</strong>: $\gamma \in [0, 1]$, 本文采用 $\gamma=0.9$.</li></ul><hr><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul><li>使用贝尔曼期望方程, 求解给定随机策略 $\pi(a | s) = \frac14$ 下的状态值函数.</li><li>使用贝尔曼最优方程, 求解最优状态值函数.</li></ul><hr><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><iframe src="../../src/gridworld.html" scrolling="no" width="100%" height="3950px"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Reinforcement Learning: An Introduction》在第三章中给出了一个简单的例子:&lt;code&gt;Gridworld&lt;/code&gt;, 以帮助我们理解&lt;code&gt;finite MDPs&lt;/code&gt;, 同时也求解了该问题的&lt;strong&gt;贝尔曼期望方程&lt;/strong&gt;和&lt;strong&gt;贝尔曼最优方程&lt;/strong&gt;. 本文简要说明如何进行编程求解.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="贝尔曼方程" scheme="https://orzyt.cn/tags/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>强化学习实践（一）：Tic-Tac-Toe</title>
    <link href="https://orzyt.cn/posts/tic-tac-toe/"/>
    <id>https://orzyt.cn/posts/tic-tac-toe/</id>
    <published>2019-03-08T10:55:54.000Z</published>
    <updated>2019-04-29T08:21:13.104Z</updated>
    
    <content type="html"><![CDATA[<p>为了对强化学习的基本概念有一个直观的认识,《Reinforcement Learning: An Introduction》第一章给出了一个简单的例子：<code>Tic-Tac-Toe</code>游戏.</p><h2 id="游戏规则"><a href="#游戏规则" class="headerlink" title="游戏规则"></a>游戏规则</h2><p>游戏的规则很简单, 两位玩家在 <code>3x3</code> 的棋盘上轮流下棋, 一位打 <code>X</code>, 另一位打 <code>O</code>, 若棋盘的任意一行、任意一列、正反对角线上有三个相同的棋, 则执该棋的玩家获胜. 若棋盘下满仍没有决出胜负, 则平局.</p><a id="more"></a><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0vltwtubvj20bb0b574c.jpg" alt="Tic-Tac-Toe示例" width="25%" height="25%"></p><hr><p>我们尝试使用强化学习的方法来训练一个Agent, 使其能够在该游戏上表现出色(即Agent在任何情况下都不会输, 最多平局).</p><p>由于没有外部经验, 因此我们需要同时训练两个Agent进行上万轮的对弈来寻找最优策略.</p><p><strong>注:下面的代码只给出部分关键实现过程, 完整代码见:<a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter01/tic_tac_toe.py" target="_blank" rel="noopener">tic_tac_toe.py</a>.</strong> </p><p><strong>版权归 <a href="https://github.com/ShangtongZhang" target="_blank" rel="noopener">@Shangtong Zhang</a> 等人所有, 仅添加中文注释便于理解.</strong></p><hr><h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>强化学习一个重要的概念就是——<strong>状态(State)</strong>. </p><p>状态是指Agent在一个特定时刻从环境中所感知的信号. </p><p>在<code>Tic-Tac-Toe</code>游戏中, 状态即为 <code>3*3</code> 棋盘的布局. </p><p>定义一个<code>State类</code>用来表示状态.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">State</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''状态初始化</span></span><br><span class="line"><span class="string">        棋盘使用 n * n 的数组进行表示</span></span><br><span class="line"><span class="string">        棋盘中的数字: 1代表先手, -1代表后手下, 0代表该位置无棋子</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 该状态的数组表示</span></span><br><span class="line">        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))</span><br><span class="line">        <span class="comment"># 该状态下的胜利者</span></span><br><span class="line">        self.winner = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># 该状态的哈希值表示</span></span><br><span class="line">        self.state_hash = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># 该状态是否为终结状态</span></span><br><span class="line">        self.end = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''计算状态的哈希值表示</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        int</span></span><br><span class="line"><span class="string">            状态的哈希值表示</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_end</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''判断当前状态是否为终结状态.</span></span><br><span class="line"><span class="string">        如果为终结状态, 同时判断胜利者是谁</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        bool</span></span><br><span class="line"><span class="string">            当前状态是否为终结状态</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_state</span><span class="params">(self, i, j, symbol)</span>:</span></span><br><span class="line">        <span class="string">'''计算当前状态的后继状态</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        i : int</span></span><br><span class="line"><span class="string">            下一步动作的行坐标</span></span><br><span class="line"><span class="string">        j : int</span></span><br><span class="line"><span class="string">            下一步动作的列坐标</span></span><br><span class="line"><span class="string">        symbol : int</span></span><br><span class="line"><span class="string">            动作的执行者(1代表先手, -1代表后手)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        State</span></span><br><span class="line"><span class="string">            下一步棋盘的状态</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_state</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''打印状态信息</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><hr><p>根据游戏规则我们知道, 每个格子仅有三种状态, 即先手(1), 后手(-1), 空(0), 那么该游戏的状态数上限仅有 $3^9=19683$ 个.</p><p>因此, 我们可以预处理出所有合法的棋盘状态, 供后面强化学习算法使用.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_states = get_all_states()</span><br></pre></td></tr></table></figure><hr><h2 id="AgentPlayer相关"><a href="#AgentPlayer相关" class="headerlink" title="AgentPlayer相关"></a>AgentPlayer相关</h2><p>定义一个<code>AgentPlayer类</code>用来表示强化学习中和环境进行交互的智能体.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AgentPlayer</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, step_size=<span class="number">0.1</span>, epsilon=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">'''Agent初始化</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        step_size : float, optional</span></span><br><span class="line"><span class="string">            更新步长</span></span><br><span class="line"><span class="string">        epsilon : float, optional</span></span><br><span class="line"><span class="string">            探索概率</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 值函数</span></span><br><span class="line">        self.value = dict()</span><br><span class="line">        <span class="comment"># 值函数更新步长</span></span><br><span class="line">        self.step_size = step_size</span><br><span class="line">        <span class="comment"># Agent探索概率</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        <span class="comment"># Agent在一轮游戏中经历的所有状态</span></span><br><span class="line">        self.states = []</span><br><span class="line">        <span class="comment"># 记录每个状态是否采取贪心策略</span></span><br><span class="line">        self.greedy = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''重置Agent的状态, 开启新一轮游戏</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        <span class="string">'''将当前棋盘状态加到Agent的状态列表</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        state : State</span></span><br><span class="line"><span class="string">            当前棋盘的状态</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">        <span class="string">'''根据先后手, 初始化Agent的值函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        symbol : int</span></span><br><span class="line"><span class="string">            先手还是后手</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''值函数迭代</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''根据状态采取动作</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        list</span></span><br><span class="line"><span class="string">            采取的动作</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_policy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''保存策略</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_policy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''加载策略</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><hr><p>在AgentPlayer类中, 我们重点关注 <code>set_symbol</code>, <code>backup</code>, <code>act</code> 三个函数.</p><h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>Agent每次与环境进行交互的时候, 都会得到一个反馈信号称之为<strong>奖励(Reward)</strong>. </p><p>Agent的目标是<strong>最大化游戏过程中的奖励总和</strong>.</p><p>在 <code>Tic-Tac-Toe</code> 游戏中, 由于只有在游戏结束的时候才知道胜负, 故没有给出每一步显式的奖励, 而是直接评估状态的<strong>值函数(Value Function)</strong>.</p><p>根据我们的先验知识, 可以对不同的状态设置不同的初始值函数.</p><p>对于导致游戏结束的终结状态, 可分为胜利/平局/失败三种情况, 相应的值函数为1.0/0.5/0.0.</p><p>而对于非终结状态, 可以简单地将状态的值函数设为0.5, 代表无法判断胜负.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">    <span class="string">'''根据先后手, 初始化Agent的值函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    symbol : int</span></span><br><span class="line"><span class="string">        先手还是后手</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    self.symbol = symbol</span><br><span class="line">    <span class="keyword">for</span> state_hash <span class="keyword">in</span> all_states.keys():</span><br><span class="line">        (state, is_end) = all_states[state_hash]</span><br><span class="line">        <span class="keyword">if</span> is_end: <span class="comment"># 终结状态</span></span><br><span class="line">            <span class="keyword">if</span> state.winner == self.symbol: <span class="comment"># 获胜</span></span><br><span class="line">                self.value[state_hash] = <span class="number">1.0</span></span><br><span class="line">            <span class="keyword">elif</span> state.winner == <span class="number">0</span>: <span class="comment"># 平局</span></span><br><span class="line">                self.value[state_hash] = <span class="number">0.5</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 失败</span></span><br><span class="line">                self.value[state_hash] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 非终结状态</span></span><br><span class="line">            self.value[state_hash] = <span class="number">0.5</span></span><br></pre></td></tr></table></figure><hr><h3 id="值函数迭代"><a href="#值函数迭代" class="headerlink" title="值函数迭代"></a>值函数迭代</h3><p>使用<strong>时序差分学习(Temporal-Difference Learning)</strong>方法进行值函数的更新:</p><script type="math/tex; mode=display">V \left( S _ { t } \right) \leftarrow V \left( S _ { t } \right) + \alpha \left[ V \left( S _ { t + 1 } \right) - V \left( S _ { t } \right) \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''值函数迭代</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#　获取状态的哈希值表示</span></span><br><span class="line">    self.states = [state.hash() <span class="keyword">for</span> state <span class="keyword">in</span> self.states]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 逆序遍历所有的状态, 并进行值函数的更新</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(len(self.states) - <span class="number">1</span>)):</span><br><span class="line">        state = self.states[i]  </span><br><span class="line">        <span class="comment"># TD误差 = V(s_&#123;t + 1&#125;) - V(s_&#123;t&#125;)</span></span><br><span class="line">        td_error = self.greedy[i] * (self.value[self.states[i + <span class="number">1</span>]] - self.value[state])</span><br><span class="line">        <span class="comment"># TD-Learning(时序差分学习)更新公式</span></span><br><span class="line">        self.value[state] += self.step_size * td_error</span><br></pre></td></tr></table></figure><hr><h3 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h3><p>采用 $\epsilon$-greedy 的贪心策略选择动作, 即有 $1 - \epsilon$ 的概率选择后继状态值函数最大的动作, 有 $\epsilon$ 概率进行随机选择动作.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''根据状态采取动作</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    list</span></span><br><span class="line"><span class="string">        采取的动作</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取当前状态</span></span><br><span class="line">    state = self.states[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># 下一步所有可能的状态</span></span><br><span class="line">    next_states = []</span><br><span class="line">    <span class="comment"># 下一步所有可能的位置</span></span><br><span class="line">    next_positions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(BOARD_ROWS):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(BOARD_COLS):</span><br><span class="line">            <span class="comment"># 当前棋盘位置上无棋子</span></span><br><span class="line">            <span class="keyword">if</span> state.data[i, j] == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 可行的位置</span></span><br><span class="line">                next_positions.append([i, j])</span><br><span class="line">                <span class="comment"># 可行的状态</span></span><br><span class="line">                next_states.append(state.next_state(i, j, self.symbol).hash())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 有epsilon概率采取随机动作</span></span><br><span class="line">    <span class="keyword">if</span> np.random.rand() &lt; self.epsilon:</span><br><span class="line">        action = next_positions[np.random.randint(len(next_positions))]</span><br><span class="line">        action.append(self.symbol)</span><br><span class="line">        self.greedy[<span class="number">-1</span>] = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    values = []</span><br><span class="line">    <span class="comment"># 遍历下一步所有可能的状态和位置</span></span><br><span class="line">    <span class="keyword">for</span> state_hash, pos <span class="keyword">in</span> zip(next_states, next_positions):</span><br><span class="line">        <span class="comment"># 获取对应状态的值函数</span></span><br><span class="line">        values.append((self.value[state_hash], pos))</span><br><span class="line">    <span class="comment"># 如果有多个状态的值函数相同,且都是最高的,shuffle则起到在这些状态中随机选择的作用</span></span><br><span class="line">    np.random.shuffle(values)</span><br><span class="line">    <span class="comment"># 按值函数从大到小排序</span></span><br><span class="line">    values.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># 选取最优动作</span></span><br><span class="line">    action = values[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    action.append(self.symbol)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><hr><h2 id="HumanPlayer相关"><a href="#HumanPlayer相关" class="headerlink" title="HumanPlayer相关"></a>HumanPlayer相关</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HumanPlayer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        self.symbol = <span class="keyword">None</span></span><br><span class="line">        self.keys = [<span class="string">'q'</span>, <span class="string">'w'</span>, <span class="string">'e'</span>, <span class="string">'a'</span>, <span class="string">'s'</span>, <span class="string">'d'</span>, <span class="string">'z'</span>, <span class="string">'x'</span>, <span class="string">'c'</span>]</span><br><span class="line">        self.state = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state</span><span class="params">(self, state)</span>:</span></span><br><span class="line">        self.state = state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_symbol</span><span class="params">(self, symbol)</span>:</span></span><br><span class="line">        self.symbol = symbol</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup</span><span class="params">(self, _)</span>:</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.state.print_state()</span><br><span class="line">        key = input(<span class="string">"Input your position:"</span>)</span><br><span class="line">        data = self.keys.index(key)</span><br><span class="line">        i = data // int(BOARD_COLS)</span><br><span class="line">        j = data % BOARD_COLS</span><br><span class="line">        <span class="keyword">return</span> [i, j, self.symbol]</span><br></pre></td></tr></table></figure><hr><h2 id="Agent训练"><a href="#Agent训练" class="headerlink" title="Agent训练"></a>Agent训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epochs, print_every_n=<span class="number">500</span>)</span>:</span></span><br><span class="line">    <span class="string">'''对Agent进行训练</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    epochs : int</span></span><br><span class="line"><span class="string">        训练轮数</span></span><br><span class="line"><span class="string">    print_every_n : int, optional</span></span><br><span class="line"><span class="string">        每多少轮输出训练信息</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义两个Agent</span></span><br><span class="line">    player1 = AgentPlayer(epsilon=<span class="number">0.01</span>)</span><br><span class="line">    player2 = AgentPlayer(epsilon=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 定义判决器</span></span><br><span class="line">    game = Game(player1, player2)</span><br><span class="line">    <span class="comment"># 先手赢的次数</span></span><br><span class="line">    player1_win = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 后手赢的次数</span></span><br><span class="line">    player2_win = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 新的一轮游戏</span></span><br><span class="line">        game.reset()</span><br><span class="line">        winner = game.play(print_state=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">1</span>:</span><br><span class="line">            player1_win += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">-1</span>:</span><br><span class="line">            player2_win += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 打印各自的胜率</span></span><br><span class="line">        <span class="keyword">if</span> i % print_every_n == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f'</span> % (i, player1_win / i, player2_win / i))</span><br><span class="line">        <span class="comment"># 在每轮游戏结束后,对Agent进行学习</span></span><br><span class="line">        player1.backup()</span><br><span class="line">        player2.backup()</span><br><span class="line">    <span class="comment"># 保存训练好的策略</span></span><br><span class="line">    player1.save_policy()</span><br><span class="line">    player2.save_policy()</span><br></pre></td></tr></table></figure><hr><h2 id="Agent对弈"><a href="#Agent对弈" class="headerlink" title="Agent对弈"></a>Agent对弈</h2><p>经过充分的训练后, 两个Agent对弈的胜率应该都为0%. 即任何局面都只能打成平手, 没有一方可以胜过另一方.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compete</span><span class="params">(turns)</span>:</span></span><br><span class="line">    <span class="string">'''将训练好的两个Agent进行对弈</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    turns : int</span></span><br><span class="line"><span class="string">        对弈轮数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对弈的时候不进行动作的探索, 故epsilon设为0.</span></span><br><span class="line">    player1 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">    player2 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">    game = Game(player1, player2)</span><br><span class="line">    player1.load_policy()</span><br><span class="line">    player2.load_policy()</span><br><span class="line">    player1_win = <span class="number">0.0</span></span><br><span class="line">    player2_win = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, turns):</span><br><span class="line">        game.reset()</span><br><span class="line">        winner = game.play()</span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">1</span>:</span><br><span class="line">            player1_win += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> winner == <span class="number">-1</span>:</span><br><span class="line">            player2_win += <span class="number">1</span></span><br><span class="line">    print(<span class="string">'%d turns, player 1 win %.02f, player 2 win %.02f'</span> % (turns, player1_win / turns, player2_win / turns))</span><br></pre></td></tr></table></figure><hr><h2 id="Human-v-s-Agent"><a href="#Human-v-s-Agent" class="headerlink" title="Human v.s. Agent"></a>Human v.s. Agent</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''人类玩家和Agent进行对弈</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        player1 = HumanPlayer()</span><br><span class="line">        player2 = AgentPlayer(epsilon=<span class="number">0</span>)</span><br><span class="line">        game = Game(player1, player2)</span><br><span class="line">        player2.load_policy()</span><br><span class="line">        winner = game.play()</span><br><span class="line">        <span class="keyword">if</span> winner == player2.symbol:</span><br><span class="line">            print(<span class="string">"失败!"</span>)</span><br><span class="line">        <span class="keyword">elif</span> winner == player1.symbol:</span><br><span class="line">            print(<span class="string">"胜利!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"平局!"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为了对强化学习的基本概念有一个直观的认识,《Reinforcement Learning: An Introduction》第一章给出了一个简单的例子：&lt;code&gt;Tic-Tac-Toe&lt;/code&gt;游戏.&lt;/p&gt;
&lt;h2 id=&quot;游戏规则&quot;&gt;&lt;a href=&quot;#游戏规则&quot; class=&quot;headerlink&quot; title=&quot;游戏规则&quot;&gt;&lt;/a&gt;游戏规则&lt;/h2&gt;&lt;p&gt;游戏的规则很简单, 两位玩家在 &lt;code&gt;3x3&lt;/code&gt; 的棋盘上轮流下棋, 一位打 &lt;code&gt;X&lt;/code&gt;, 另一位打 &lt;code&gt;O&lt;/code&gt;, 若棋盘的任意一行、任意一列、正反对角线上有三个相同的棋, 则执该棋的玩家获胜. 若棋盘下满仍没有决出胜负, 则平局.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（三）：动态规划</title>
    <link href="https://orzyt.cn/posts/planning-by-dp/"/>
    <id>https://orzyt.cn/posts/planning-by-dp/</id>
    <published>2019-03-01T06:10:25.000Z</published>
    <updated>2019-04-29T08:20:57.872Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章 <a href="https://orzyt.cn/posts/markov-decision-processes/">强化学习（二）：马尔可夫决策过程</a> 中, 我们介绍用来对强化学习问题进行建模的马尔可夫决策过程(Markov Decision Processes, MDPs). </p><p>由于MDPs的贝尔曼最优方程没有封闭解, 因此一般采用迭代的方法对其进行求解. </p><p>本文将介绍使用<strong>动态规划(Dynamic Programming)</strong>算法来求解MDPs.</p><a id="more"></a><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><ul><li><p><strong>动态(Dynamic)</strong>: 问题中的时序部分</p></li><li><p><strong>规划(Planning)</strong>: 对问题进行优化</p></li></ul><p>动态规划将问题分解为子问题, 从子问题的解中得到原始问题的解.</p><hr><h3 id="动态规划的性质"><a href="#动态规划的性质" class="headerlink" title="动态规划的性质"></a>动态规划的性质</h3><ul><li><p><strong>最优子结构(Optimal substructure)</strong></p><ul><li>应用最优性原则(Principle of optimality)</li><li>最优解可以从子问题的最优解中得到</li></ul></li><li><p><strong>重叠子问题(Overlapping subproblems)</strong></p><ul><li>相同的子问题出现多次</li><li>问题的解可以被缓存和复用</li></ul></li></ul><p>马尔可夫决策过程满足上面两种性质:</p><blockquote><p><em>贝尔曼方程</em> 给出了问题的递归分解表示, <em>值函数</em> 存储和复用了问题的解.</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi(a|s) (\mathcal{R}_s^a + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}v_{\pi}(s'))</script></blockquote><hr><h3 id="用动态规划进行Planning"><a href="#用动态规划进行Planning" class="headerlink" title="用动态规划进行Planning"></a>用动态规划进行Planning</h3><p>动态规划假设我们知道MDP的所有知识, 包括状态、行为、转移矩阵、奖励甚至策略等.</p><p>对于<strong>预测(Prediction)</strong>问题: </p><ul><li><p>输入: </p><ul><li>MDP $&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;$ 和 策略 $\pi$</li><li>MRP $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$</li></ul></li><li><p>输出: 值函数 $v_{\pi}$</p></li></ul><p>对于<strong>控制(Control)</strong>问题:</p><ul><li><p>输入:</p><ul><li>MDP $&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma&gt;$</li></ul></li><li><p>输出:</p><ul><li>最优值函数 $v_{*}$</li><li>最优策略 $\pi_{*}$</li></ul></li></ul><hr><h2 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h2><blockquote><p>问题: 评估一个给定的策略 $\pi$<br>求解: 对贝尔曼期望方程进行迭代, $v_1 \to v_2 \to \dots \to v_{\pi}$</p></blockquote><p>通常使用<strong>同步备份(synchronous backups)</strong>方法:</p><p>对于第 $k+1$ 次迭代, 所有状态 $s$ 在第 $k+1$ 时刻的价值 $v_{k+1}(s)$ 用 $v_k(s’)$ 进行更新, 其中 $s’$ 是 $s$ 的后继状态.</p><p><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0nb9rn3v4j20ar06eq31.jpg" alt="迭代策略评估" width="30%" height="30%"></p><script type="math/tex; mode=display">\begin{aligned} v _ { k + 1 } ( s ) & = \sum _ { a \in \mathcal { A } } \pi ( a | s ) \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { k } \left( s ^ { \prime } \right) \right) \\\mathbf { v } ^ { k + 1 } & = \mathcal { R } ^ { \pi } + \gamma \mathcal { P } ^ { \pi } \mathbf { v } ^ { k } \end{aligned}</script><hr><p><strong>迭代策略评估算法</strong>:</p><p>迭代策略评估算法用来估计 $V \approx v_{\pi}$.</p><p>这里使用<code>in-place</code>版本, 即只保留一份 $v$ 数组, 没有新旧之分. </p><p>通常来说, 该方法也能收敛到 $v_{\pi}$, 而且收敛速度可能更快.</p><p>终止条件: $\max \limits_ { s \in \mathcal{S} } \left| v _ { k + 1 } ( s ) - v _ { k } ( s ) \right|$ 小于给定的误差 $\Delta$</p><p><img rel="noreferrer" src="https://ws1.sinaimg.cn/large/8662e3cegy1g0nj071hc6j20km08hq3t.jpg" alt="迭代策略评估伪代码" width="60%" height="60%"></p><hr><p>例子: <strong>Small Gridworld</strong> <a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter04/grid_world.py" target="_blank" rel="noopener">[代码]</a></p><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0nbvkvvotj20k00dkdgx.jpg" alt="Small Gridworld" width="50%" height="50%"></p><p><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0nc5gkmd8j20e30lkwgd.jpg" alt="Small Gridworld Solution" width="50%" height="50%"></p><hr><h2 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h2><p>让我们考虑一个<strong>确定性策略</strong>(即对于一个状态来说, 其采取的动作是确定的, 而不是考虑每个动作的概率) $a = \pi(s)$.</p><blockquote><p>我们可以通过贪心选择来改进策略 $\pi$:</p><script type="math/tex; mode=display">\pi ^ { \prime } ( s ) = \underset { a \in \mathcal { A } } { \operatorname { argmax } } q _ { \pi } ( s , a )</script></blockquote><p>即状态 $s$ 的新策略为令动作值函数 $q_{\pi}(s, a)$ 取得最大值的动作.</p><p>相应地, 动作值函数 $q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right)$ 得到了改进:</p><script type="math/tex; mode=display">q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a ) \geq q _ { \pi } ( s , \pi ( s ) ) = v _ { \pi } ( s ) \\{\scriptsize 由于是确定性策略, 才会有 v_{\pi}(s) = q_{\pi}(s, \pi(s))}\tag{1}</script><p>注: 确定性策略下的动作值函数 $q_{\pi}(s, a)$ 为:</p><script type="math/tex; mode=display">\begin{aligned} q _ { \pi } ( s , a ) & = \mathbb { E } \left[ R _ { t + 1 } + \gamma v _ { \pi } \left( S _ { t + 1 } \right) | S _ { t } = s , A _ { t } = a \right] \\ & = \sum _ { s ^ { \prime } , r } p \left( s ^ { \prime } , r | s , a \right) \left[ r + \gamma v _ { \pi } \left( s ^ { \prime } \right) \right] \end{aligned}\tag{2}</script><p>从而, 值函数 $v _ { \pi ^ { \prime } } ( s )$ 也得到了改进:</p><script type="math/tex; mode=display">\begin{aligned} v_\pi(s) & \le q_\pi(s,\pi^{'}(s)) {\scriptsize //公式(1)} \\ &={\Bbb E}[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t=s, A_t=\pi^{'}(s)] {\scriptsize //公式(2)} \\&={\Bbb E}_{\pi'}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\  {\scriptsize //注意外层是在新策略 \pi^{'} 下求期望} \\   & \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma q_\pi(S_{t+1},\pi'(S_{t+1}))|S_t=s] {\scriptsize //对状态S_{t+1}使用公式(1)} \\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma {\Bbb E}_{\pi'}\left[ R_{t+2}+\gamma v_{\pi}\left( S_{t+2}\right) | S_{t+1}, A_{t+1}=\pi^{'}(S_{t+1}) \right] | S_t=s]\\ &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_{\pi}\left( S_{t+2} \right)|S_t=s] {\scriptsize //去掉括号内的期望} \\ & \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma ^2 q_\pi(S_{t+2},\pi'(S_{t+2}))|S_t=s] {\scriptsize //对状态S_{t+2}使用公式(1)} \\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 {\Bbb E}_{\pi'}\left( R_{t+3}+\gamma v_{\pi}\left( S_{t+3} \right) \right)|S_t=s]\\  &= {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_{\pi}\left( S_{t+3} \right)|S_t=s]\\  & \vdots \\& \le {\Bbb E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4} + \dots |S_t=s]\\ &=v_{\pi^{'}}(s) \\ \end{aligned}</script><p>当改进停止时, 有如下等式:</p><script type="math/tex; mode=display">q _ { \pi } \left( s , \pi ^ { \prime } ( s ) \right) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a ) = q _ { \pi } ( s , \pi ( s ) ) = v _ { \pi } ( s )\tag{3}</script><p>可以说, 此时公式(3)满足了贝尔曼最优方程:</p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \max _ { a \in \mathcal { A } } q _ { \pi } ( s , a )</script><p>从而, 对所有状态 $s$ 来说, 有$v_{\pi}(s) = v_{*}(s)$, 即策略 $\pi$ 改进到了最优策略.</p><hr><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><h3 id="策略迭代-1"><a href="#策略迭代-1" class="headerlink" title="策略迭代"></a>策略迭代</h3><p>给定一个策略 $\pi$, 我们可以首先对策略进行评估, 然后根据值函数 $v_{\pi}$ 进行贪心地改进策略.</p><script type="math/tex; mode=display">\pi _ { 0 } \stackrel { \mathrm { E } } { \longrightarrow } v _ { \pi _ { 0 } } \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { 1 } \stackrel { \mathrm { E } } { \longrightarrow } v _ { \pi _ { 1 } } \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { 2 } \stackrel { \mathrm { E } } { \longrightarrow } \cdots \stackrel { \mathrm { I } } { \longrightarrow } \pi _ { * } \stackrel { \mathrm { E } } { \longrightarrow } v _ { * }</script><p>其中, $\stackrel { \mathrm { E } } { \longrightarrow }$ 表示策略评估, $\stackrel { \mathrm { I } } { \longrightarrow }$ 表示策略改进. </p><ul><li><p><strong>评估(Evaluate):</strong></p><script type="math/tex; mode=display">v _ { \pi } ( s ) = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots | S _ { t } = s \right]</script></li><li><p><strong>改进(Improve):</strong></p><script type="math/tex; mode=display">\pi^{'} = \text{greedy}(v_{\pi})</script></li></ul><p>由于每个策略都比前一个策略更优, 同时一个有限状态的马尔可夫决策过程(finite MDP)仅有有限个策略, 因此该过程一定能够在有限次的迭代中收敛到最优策略 $\pi_{*}$ 和最优值函数 $v_{*}$.</p><hr><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0ncddk93hj20ni0cc0uq.jpg" alt="策略迭代" width="50%" height="50%"></p><hr><p><strong>策略迭代算法:</strong></p><p>策略迭代算法分为: <strong>初始化</strong>, <strong>策略评估</strong> 以及 <strong>策略改进</strong> 三部分.</p><p>其中, 策略改进部分的终止条件为: <strong>是否所有状态的策略不再发生变化</strong>.</p><p><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0njmn8jy5j20kq0e3760.jpg" alt="策略迭代算法" width="60%" height="60%"></p><hr><p>例子: <strong>Jack’s Car Rental</strong> <a href="https://github.com/orzyt/reinforcement-learning-an-introduction/blob/master/chapter04/car_rental.py" target="_blank" rel="noopener">[代码]</a>  (<em>先占个坑 , 等有时间把这个例子详细写下</em>)</p><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0ncjvapk8j20ke0ebn32.jpg" alt="Jack’s Car Rental" width="50%" height="50%"></p><p>策略迭代求解结果:</p><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0nclbnxl1j20jw0dgtam.jpg" alt="Jack’s Car Rental Solution" width="50%" height="50%"></p><p>图中纵坐标是位置 $1$ 的汽车数量, 横坐标是位置 $2$ 的汽车数量, 该问题共有 $21 \times 21$ 个状态. </p><p>图中的等高线将状态划分为不同的区域, 区域内的数值代表相应的策略(正数代表从位置 $1$ 移往位置 $2$ 的汽车数量, 负数则往反方向移动).</p><hr><h3 id="策略迭代的扩展"><a href="#策略迭代的扩展" class="headerlink" title="策略迭代的扩展"></a>策略迭代的扩展</h3><h4 id="改良策略迭代"><a href="#改良策略迭代" class="headerlink" title="改良策略迭代"></a>改良策略迭代</h4><p>策略评估并不需要真正的收敛到 $v_{\pi}$. (比如在 <code>Small Gridworld</code>例子中, 迭代 $k=3$次 即可以得到最优策略.)</p><p>为此我们可以引进终止条件, 如:</p><ul><li>值函数的 $\epsilon$ -收敛</li><li>简单地迭代 $k$ 次便停止策略评估</li></ul><p>或者每次迭代(即 $k=1$ )都对策略进行更新改进, 这种情况等价于<strong>值迭代(value iteration)</strong>.</p><hr><h4 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h4><p><strong>广义策略迭代</strong>(Generalized Policy iteration，GPI)指代让策略评估(policy-evaluation)和策略改进(policyimprovement)过程进行交互的一般概念, 其不依赖于两个过程的粒度(granularity)和其他细节.</p><p>几乎所有强化学习方法都可以很好地被描述为GPI. 也就是说, 它们都具有可辨识的策略与值函数. 其中, 策略 $\pi$ 通过相应的值函数 $v$ 进行改进, 而值函数 $V$ 总是趋向策略 $\pi$ 的值函数 $v^{\pi}$. 如下图所示,</p><p><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0nik26512j206y0a5dfz.jpg" alt="广义策略迭代" width="20%" height="20%"></p><hr><h2 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h2><p>策略迭代的一个缺点是它的每次迭代都涉及策略评估, 这本身就是一个需要对状态集进行多次扫描的耗时迭代计算. </p><p>而在值迭代的过程中, 并没有出现显式的策略, 并且中间过程的值函数可能也不和任何策略对应.</p><hr><h3 id="最优性原则"><a href="#最优性原则" class="headerlink" title="最优性原则"></a>最优性原则</h3><p>一个最优策略可以被分解为两部分:</p><ul><li>当前状态的最优动作 $A_{*}$</li><li>后继状态 $S^{\prime}$ 的最优策略</li></ul><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0nk02i8apj20mi06v75b.jpg" alt="最优性原则" width="60%" height="60%"></p><p>该原则的意思是说, 一个策略 $\pi(a|s)$ 在状态 $s$ 取到最优值函数 $v_{\pi}(s) = v_{*}(s)$ <strong>当且仅当</strong> 对于所有从状态 $s$ 出发可到达的状态 $s^{\prime}$, 策略 $\pi$ 也能够在状态 $s^{\prime}$ 取到最优值函数.</p><hr><h3 id="确定性值迭代"><a href="#确定性值迭代" class="headerlink" title="确定性值迭代"></a>确定性值迭代</h3><p>如果我们已经知道子问题的最优解 $v_{*}(s^{\prime})$, 那么状态 $s$ 的最优解可以通过向前看(lookahead)一步得到, 这称为<strong>值迭代(Value Iteration)</strong>:</p><script type="math/tex; mode=display">v_{*}(s) \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{*}(s') \right)</script><hr><p><strong>值迭代算法:</strong></p><p>值迭代算法和策略迭代算法一样, 是用来估计最优策略 $\pi_{*}$ 的, 它将策略评估和策略改进有效地结合在了一起.</p><p><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0nkt7xfz0j20kn09k0ts.jpg" alt="值迭代算法" width="60%" height="60%"></p><hr><h2 id="同步动态规划算法总结"><a href="#同步动态规划算法总结" class="headerlink" title="同步动态规划算法总结"></a>同步动态规划算法总结</h2><div class="table-container"><table><thead><tr><th style="text-align:center">问题</th><th style="text-align:center">贝尔曼方程</th><th style="text-align:center">算法</th></tr></thead><tbody><tr><td style="text-align:center">预测(Prediction)</td><td style="text-align:center">贝尔曼期望方程</td><td style="text-align:center">迭代策略评估</td></tr><tr><td style="text-align:center">控制(Control)</td><td style="text-align:center">贝尔曼期望方程 + 贪心策略改进</td><td style="text-align:center">策略迭代</td></tr><tr><td style="text-align:center">控制(Control)</td><td style="text-align:center">贝尔曼最优方程</td><td style="text-align:center">值迭代</td></tr></tbody></table></div><p>对于有 $m$ 个动作和 $n$ 个状态 的MDP来说, 每次迭代的时间复杂度如下:</p><div class="table-container"><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">复杂度</th></tr></thead><tbody><tr><td style="text-align:center">$v_{\pi}(s)$ or $v_{*}(s)$</td><td style="text-align:center">$\mathcal{O}(mn^2)$</td></tr><tr><td style="text-align:center">$q_{\pi}(s, a)$ or $q_{*}(s, a)$</td><td style="text-align:center">$\mathcal{O}(m^2n^2)$</td></tr></tbody></table></div><hr><h2 id="动态规划的扩展"><a href="#动态规划的扩展" class="headerlink" title="动态规划的扩展"></a>动态规划的扩展</h2><h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>同步DP算法的主要缺点是每次迭代都需要对整个状态集进行扫描, 这对于状态数非常多的MDP来说耗费巨大. 而异步DP算法则将所有的状态独立地,以任意顺序进行备份, 并且每个状态的更新次数不一, 这可以显著地减少计算量.</p><p>为了保证算法的正确收敛, 异步动态规划算法必须保证<strong>所有状态都能够持续地被更新</strong>(continue to update the values of all the states), 也就是说在任何时刻任何状态都有可能被更新, 而不能忽略某个状态.</p><p>异步DP算法主要有三种简单的思想:</p><ul><li>就地动态规划(<em>In-place</em> dynamic programming)</li><li>优先扫描(<em>Prioritised sweeping</em>)</li><li>实时动态规划(<em>Real-time</em> dynamic programming)</li></ul><hr><h4 id="就地动态规划"><a href="#就地动态规划" class="headerlink" title="就地动态规划"></a>就地动态规划</h4><p>同步DP保留值函数的两个备份, $v_{new}$ 和 $v_{old}$</p><script type="math/tex; mode=display">{\color{red} {v_{new}(s)}} \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} {\color{red} {v_{old}(s')}} \right)</script><p>就地值迭代只保留值函数的一个备份.</p><script type="math/tex; mode=display">{\color{red} {v(s)}} \gets \max \limits_{a \in \mathcal{A}} \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} {\color{red} {v(s')}} \right)</script><hr><h4 id="优先扫描"><a href="#优先扫描" class="headerlink" title="优先扫描"></a>优先扫描</h4><p>使用贝尔曼误差的大小来进行状态的选择:</p><script type="math/tex; mode=display">\left| \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v \left( s ^ { \prime } \right) \right) - v ( s ) \right|</script><ul><li><p>仅备份有最大贝尔曼误差的状态</p></li><li><p>在每次备份后, 需要更新受到影响的状态(即备份状态的前驱状态)的贝尔曼误差</p></li><li><p>可以使用优先队列进行实现</p></li></ul><hr><h4 id="实时动态规划"><a href="#实时动态规划" class="headerlink" title="实时动态规划"></a>实时动态规划</h4><ul><li>思想: <strong>只使用和Agent相关的状态</strong></li><li>使用Agent的经验来进行状态的选择</li><li>在每个时间步 $S_t, A_t, R_{t+1}$ 对状态 $S_t$ 进行备份</li></ul><script type="math/tex; mode=display">{\color{red} {v \left( S _ { t } \right)}} \gets \max _ { a \in \mathcal { A } } \left( \mathcal { R } _ { {\color{red}{S _ { t }}} } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { {\color{red} {S _ { t }}} s ^ { \prime }}  ^ { a } {\color{red} {v \left( s ^ { \prime } \right)}} \right)</script><hr><h3 id="全宽和采样备份"><a href="#全宽和采样备份" class="headerlink" title="全宽和采样备份"></a>全宽和采样备份</h3><h4 id="全宽备份"><a href="#全宽备份" class="headerlink" title="全宽备份"></a>全宽备份</h4><ul><li><p>DP使用<strong>全宽备份</strong>(<em>full-width</em> backups)</p></li><li><p>对于每次备份(不管同步还是异步)</p><ul><li>每个后继状态和动作都会被考虑进去</li><li>需要知道MDP转移矩阵和奖励函数</li></ul></li><li><p>对于大规模DP问题会遇到维数灾难</p></li><li><p>进行一次备份都太奢侈了</p></li></ul><hr><h4 id="采样备份"><a href="#采样备份" class="headerlink" title="采样备份"></a>采样备份</h4><p><strong>采样备份(Sample Backups)</strong>使用采样的奖励和采样的转移 $&lt; S , A , R , S ^ { \prime } &gt;$ 来替代奖励函数 $\mathcal{R}$ 和 转移矩阵 $\mathcal{P}$. </p><p>采样备份的优点:</p><ul><li><strong>Model-free</strong>: 不需要知道MDP的先验知识</li><li>通过采样<strong>缓解维数灾难</strong></li><li><strong>备份代价成为常量</strong>, 独立于状态数 $n = |\mathcal{S}|$</li></ul><hr><h2 id="压缩映射"><a href="#压缩映射" class="headerlink" title="压缩映射"></a>压缩映射</h2><p>关于上面的种种算法, 我们可能会有如下疑问:</p><ul><li>值迭代是否会收敛到 $v_{*}$ ?</li><li>迭代策略评估是否会收敛到 $v_{\pi}$ ?</li><li>策略迭代是否会收敛到 $v_{*}$ ?</li><li>解唯一吗 ?</li><li>算法收敛速度有多快 ?</li></ul><p>为了解决这些问题, 需要引入压缩映射(contraction mapping)理论.<br>可以参考: <a href="https://zhuanlan.zhihu.com/p/39279611" target="_blank" rel="noopener">如何证明迭代式策略评价、值迭代和策略迭代的收敛性？</a></p><hr><p>(关于压缩映射理论有时间再补充, 先到这里吧…)</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第四章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf" target="_blank" rel="noopener">Lecture3: Planning by Dynamic Programming</a></li><li><a href="https://zhuanlan.zhihu.com/p/51393982" target="_blank" rel="noopener">David Silver 增强学习——Lecture 3 动态规划</a></li><li><a href="https://www.cnblogs.com/pinard/p/9463815.html" target="_blank" rel="noopener">强化学习（三）用动态规划（DP）求解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇文章 &lt;a href=&quot;https://orzyt.cn/posts/markov-decision-processes/&quot;&gt;强化学习（二）：马尔可夫决策过程&lt;/a&gt; 中, 我们介绍用来对强化学习问题进行建模的马尔可夫决策过程(Markov Decision Processes, MDPs). &lt;/p&gt;
&lt;p&gt;由于MDPs的贝尔曼最优方程没有封闭解, 因此一般采用迭代的方法对其进行求解. &lt;/p&gt;
&lt;p&gt;本文将介绍使用&lt;strong&gt;动态规划(Dynamic Programming)&lt;/strong&gt;算法来求解MDPs.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="动态规划" scheme="https://orzyt.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（二）：马尔可夫决策过程</title>
    <link href="https://orzyt.cn/posts/markov-decision-processes/"/>
    <id>https://orzyt.cn/posts/markov-decision-processes/</id>
    <published>2019-02-27T07:38:18.000Z</published>
    <updated>2019-04-29T08:20:45.468Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章 <a href="https://orzyt.cn/posts/introduction-to-rl">强化学习（一）：强化学习简介</a> 中, 我们介绍了强化学习的一些基本概念.</p><p>本文将介绍用来对强化学习问题进行建模的<strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>.</p><a id="more"></a><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><h3 id="马尔可夫决策过程简介"><a href="#马尔可夫决策过程简介" class="headerlink" title="马尔可夫决策过程简介"></a>马尔可夫决策过程简介</h3><p><strong>马尔可夫决策过程(Markov Decision Processes, MDPs)</strong>形式上用来描述强化学习中的环境.</p><p>其中,环境是<strong>完全可观测的(fully observable)</strong>,即当前状态可以完全表征过程.</p><p>几乎所有的强化学习问题都能用MDPs来描述：</p><ul><li>最优控制问题可以描述成连续MDPs;</li><li>部分观测环境可以转化成MDPs;</li><li>赌博机问题是只有一个状态的MDPs.</li></ul><hr><h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3><p><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3nzaa8yj20mn0593ym.jpg" alt="马尔科夫性质" width="60%" height="60%"></p><p>马尔科夫性质(Markov Property)表明: <strong>未来只与现在有关,而与过去无关.</strong></p><hr><h3 id="状态转移矩阵"><a href="#状态转移矩阵" class="headerlink" title="状态转移矩阵"></a>状态转移矩阵</h3><p>对于一个马尔可夫状态$S$及其后继状态$S’$,其状态转移概率由下式定义:</p><script type="math/tex; mode=display">\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]</script><p><strong>状态转移矩阵(State Transition Matrix)$\mathcal{P}$</strong>定义了从所有状态$S$转移到所有后继状态$S’$的概率.</p><script type="math/tex; mode=display">\mathcal { P } = \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \dots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n 1 } } & { \cdots } & { \mathcal { P } _ { n n } } \end{array} \right]</script><p>其中,$n$为状态个数,且矩阵的每行和为1.</p><hr><h3 id="马尔可夫过程-1"><a href="#马尔可夫过程-1" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h3><p><strong>马尔可夫过程(Markov Process)</strong>是一个无记忆的随机过程(memoryless random process).</p><p>即,随机状态$S_1, S_2, \dots$序列具有马尔可夫性质.</p><blockquote><p>马尔可夫过程(或马尔可夫链)是一个二元组$&lt;\mathcal{S}, \mathcal{P}&gt;$</p><ul><li>$\mathcal{S}$: (有限)状态集</li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li></ul></blockquote><p><img rel="noreferrer" src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l1vm9xkzj20c80act96.jpg" alt="Example: Student Markov Chain" width="50%" height="50%"></p><p>圆圈代表状态, 箭头代表状态之间的转移, 数值代表转移概率.</p><p>状态转移矩阵$\mathcal{P}$如下:</p><script type="math/tex; mode=display">{\mathcal P} =\begin{bmatrix}  & C1 & C2 & C3 &  Pass & Pub & FB & Sleep\\  C1 & &0.5 &  &   & & 0.5 & \\ C2  & & &  0.8 & & & &0.2\\ C3  & & &  & 0.6& 0.4& &\\ Pass  & & &  & & & &1.0\\ Pub  &0.2 & 0.4& 0.4 & & & &\\ FB  &0.1 & &  & & & 0.9 &\\ Sleep  & & &  & & & &1.0 \end{bmatrix}</script><hr><h2 id="马尔可夫奖励过程"><a href="#马尔可夫奖励过程" class="headerlink" title="马尔可夫奖励过程"></a>马尔可夫奖励过程</h2><p><strong>马尔可夫奖励过程(Markov Reward Process, MRP)</strong>是<em>带有奖励的马尔可夫链</em>.</p><blockquote><p>马尔可夫奖励过程是一个四元组&lt;$\mathcal{S}$, $\mathcal{P}$, <font color="red">$\mathcal{R}$</font>, <font color="red">$\mathcal{\gamma}$</font>&gt;</p><ul><li>$\mathcal{S}$: (有限)状态集</li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } = \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s \right]$</li><li><font color="red"> $\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right]$ </font></li><li><font color="red"> $\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </font></li></ul></blockquote><p><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0l2klvnixj20cf0aowf0.jpg" alt="Example: Student MRP" width="50%" height="50%"></p><h3 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h3><blockquote><p><strong>回报(Return)</strong> $G_t$ 是从时间 $t$ 开始的总折扣奖励.</p><script type="math/tex; mode=display">G _ { t } = R _ { t + 1 } + \gamma R _ { t + 2 } + \ldots = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } R _ { t + k + 1 }</script></blockquote><ul><li>折扣因子 $\gamma \in [ 0,1 ]$ 表示未来的奖励在当前的价值. 由于未来的奖励充满不确定性, 因此需要乘上折扣因子;</li><li>$\gamma$ 接近 $0$ 表明更注重当前的奖励(myopic);</li><li>$\gamma$ 接近 $1$ 表明更具有远见(far-sighted).</li></ul><hr><h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数(Value Function) $v(s)$ 表示一个状态 $s$ 的长期价值(long-term value).</p><blockquote><p>一个马尔可夫奖励过程(MRP)的<strong>状态值函数 $v(s)$</strong>是从状态 $s$ 开始的期望回报.</p><script type="math/tex; mode=display">v ( s ) = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right]</script></blockquote><hr><h3 id="MRPs的贝尔曼方程"><a href="#MRPs的贝尔曼方程" class="headerlink" title="MRPs的贝尔曼方程"></a>MRPs的贝尔曼方程</h3><p>值函数可以被分解为两部分:</p><ul><li>立即奖励 $R_{t+1}$</li><li>后继状态的折扣价值 $\gamma v(S_{t+1})$</li></ul><script type="math/tex; mode=display">\begin{aligned} v ( s ) & = \mathbb { E } \left[ G _ { t } | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma R _ { t + 2 } + \gamma ^ { 2 } R _ { t + 3 } + \ldots | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma \left( R _ { t + 2 } + \gamma R _ { t + 3 } + \ldots \right) | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma G _ { t + 1 } | S _ { t } = s \right] \\ & = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \mathbb { E } \left[ \gamma G _ { t + 1 } | S _ { t } = s \right]\\ & = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s \right] + \gamma v \left( S _ { t + 1 } \right)\\ & = \mathbb { E } \left[ R _ { t + 1 } + \gamma v \left( S _ { t + 1 } \right) | S _ { t } = s \right] \end{aligned}\tag{1}\label{eq:mrp-bellman-equation}</script><p>上式表明, $t$ 时刻的状态 $S_t$ 和 $t+1$ 时刻的状态 $S_{t+1}$ 的值函数之间满足递推关系. </p><p>该递推式也称为<strong>贝尔曼方程(Bellman Equation)</strong>.</p><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0l3fh3jb3j207802zglh.jpg" alt="Bellman Equation for MRPs" width="30%" height="30%"></p><p>如果已知概率转移矩阵 $\mathcal{P}$, 则可将公式\eqref{eq:mrp-bellman-equation}变形为:</p><script type="math/tex; mode=display">v ( s ) = \mathcal { R } _ { s } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } v \left( s ^ { \prime } \right)\tag{2}\label{eq:mrp-bellman-equation-2}</script><p>例子:</p><p><img rel="noreferrer" src="https://wx2.sinaimg.cn/large/8662e3cegy1g0l3pbm9ixj20c30b5mxp.jpg" alt="Example: Bellman Equation for Student MRP" width="40%" height="40%"></p><p><strong>贝尔曼方程的矩阵形式:</strong></p><p>可将公式\eqref{eq:mrp-bellman-equation-2}改写为矩阵形式:</p><script type="math/tex; mode=display">v = \mathcal { R } + \gamma \mathcal { P } v</script><p>其中, $v$ 为一个列向量, 向量的元素为每个状态的值函数.</p><script type="math/tex; mode=display">\left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right] = \left[ \begin{array} { c } { \mathcal { R } _ { 1 } } \\ { \vdots } \\ { \mathcal { R } _ { n } } \end{array} \right] + \gamma \left[ \begin{array} { c c c } { \mathcal { P } _ { 11 } } & { \ldots } & { \mathcal { P } _ { 1 n } } \\ { \vdots } & { } & { } \\ { \mathcal { P } _ { n1 } } & { \ldots } & { \mathcal { P } _ { n n } } \end{array} \right] \left[ \begin{array} { c } { v ( 1 ) } \\ { \vdots } \\ { v ( n ) } \end{array} \right]</script><p>观测贝尔曼方程的矩阵形式, 可知其为线性方程, 可直接求解如下.</p><script type="math/tex; mode=display">\begin{aligned} v & = \mathcal { R } + \gamma \mathcal { P } v \\( I - \gamma \mathcal { P } ) v & = \mathcal { R } \\v & = ( I - \gamma \mathcal { P } ) ^ { - 1 } \mathcal { R }\end{aligned}</script><p>计算复杂度为: $\mathcal{O}(n^3)$. 因此, 只适合直接求解小规模的MRP问题.</p><p>对于大规模的MRP问题, 通常采取以下的迭代方法:</p><ul><li>动态规划(Dynamic programming)</li><li>蒙特卡洛评估(Monte-Carlo evaluation)</li><li>时序差分学习(Temporal-Difference learning)</li></ul><hr><h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p><strong>马尔可夫决策过程(Markov Decision Process, MDP)</strong>是<em>带有决策的马尔可夫奖励过程</em>.</p><blockquote><p>马尔可夫决策过程是一个五元组&lt;$\mathcal{S}$, <font color="red">$\mathcal{A}$</font>, $\mathcal{P}$, $\mathcal{R}$, $\mathcal{\gamma}$&gt;</p><ul><li>$\mathcal{S}$: 有限的状态集</li><li><font color="red"> $\mathcal{A}$: 有限的动作集</font></li><li>$\mathcal{P}$: 状态转移概率矩阵, $\mathcal { P } _ { s s ^ { \prime } } ^ {a}= \mathbb { P } \left[ S _ { t + 1 } = s ^ { \prime } | S _ { t } = s, A _ { t } = a \right]$</li><li>$\mathcal{R}$: 奖励函数, $\mathcal { R } _ { s } ^ {a} = \mathbb { E } \left[ R _ { t + 1 } | S _ { t } = s, A _ { t } = a \right]$</li><li>$\gamma$: 折扣因子, $\gamma \in [ 0,1 ]$ </li></ul></blockquote><p>例子:</p><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0l47drh0vj20g30d93zc.jpg" alt="Example: Student MDP" width="45%" height="45%"></p><hr><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><blockquote><p><strong>策略(Policy) $\pi$</strong> 是给定状态的动作分布.</p><script type="math/tex; mode=display">\pi ( a | s ) = \mathbb { P } \left[ A _ { t } = a | S _ { t } = s \right]</script></blockquote><ul><li>策略完全决定智能体的行为;</li><li>MDP策略值依赖于当前状态(无关历史);</li><li>策略是固定的(与时间无关). $A _ { t } \sim \pi ( \cdot | S _ { t } ) , \forall t &gt; 0$</li></ul><p>给定一个马尔可夫决策过程 $M = &lt;\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{\gamma}&gt;$ 和 一个策略 $\pi$, 其可以转化为<em>马尔可夫过程</em>和<em>马尔可夫奖励过程</em>.</p><ul><li><p>状态序列 $S_1, S_2, \dots$ 是马尔科夫决策过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}&gt;$.</p></li><li><p>状态和奖励序列 $S_1, R_2, S_2, \dots$ 是马尔科夫奖励过程 $&lt;\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma&gt;$.</p></li></ul><p>其中,</p><script type="math/tex; mode=display">\mathcal{P}_{s,s'}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{P}_{ss'}^{a}</script><script type="math/tex; mode=display">\mathcal{R}_{s}^{\pi} = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \mathcal{R}_{s}^{a}</script><hr><h3 id="值函数-1"><a href="#值函数-1" class="headerlink" title="值函数"></a>值函数</h3><p><strong>值函数(Value Function)</strong>可分为<strong>状态值函数(state-value function)</strong>和<strong>动作值函数(action-value function)</strong>.</p><blockquote><p>MDP的<strong>状态值函数 $v_{\pi}(s)$ </strong>是从状态 $s$ 开始, 然后按照策略 $\pi$ 决策所获得的期望回报.</p><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_t | S_t = s \right]</script><p>MDP的<strong>动作值函数 $q_{\pi}(s, a)$ </strong>是从状态 $s$ 开始, 采取动作 $a$, 然后按照策略 $\pi$ 决策所获得的期望回报.</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t | S_t = s, A_t = a \right]</script></blockquote><hr><h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>状态值函数可以被分解为两部分, <strong>立即奖励 + 后继状态的折扣价值</strong>.</p><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s \right]</script><p>动作值函数也可以类似地分解.</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \right]</script><hr><p><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0ldl141fkj20bb04xq2x.jpg" width="40%" height="40%"></p><p>上图中, 空心圆圈代表状态, 实心圆圈代表动作.</p><p>在已知策略 $\pi$ 的情况下, 状态值函数 $v_{\pi}(s)$ 可以用动作值函数 $q_{\pi}(s, a)$ 进行表示:</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a) \tag{3}\label{eq:mdp-state-value-function}</script><hr><p><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0lds6jc80j20b004rmx6.jpg" width="40%" height="40%"></p><p>同理, 动作值函数 $q_{\pi}(s, a)$ 也可以用状态值函数 $v_{\pi}(s)$ 进行表示:</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}v_{\pi}(s') \tag{4}\label{eq:mdp-action-value-function}</script><hr><p><strong>状态值函数的贝尔曼期望方程:</strong></p><p><img rel="noreferrer" src="https://wx2.sinaimg.cn/large/8662e3cegy1g0le5yxgeij20b706hdfx.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:mdp-action-value-function}代入公式\eqref{eq:mdp-state-value-function}中, 可得状态值函数的贝尔曼期望方程:</p><script type="math/tex; mode=display">v_{\pi}(s) = \sum \limits_{a \in \mathcal{A}} \pi (a | s) \left( \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} v_{\pi}(s')  \right)</script><hr><p><strong>动作值函数的贝尔曼期望方程:</strong></p><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0le9cf2u7j20bd05wwek.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:mdp-state-value-function}代入公式\eqref{eq:mdp-action-value-function}中, 可得动作值函数的贝尔曼期望方程:</p><script type="math/tex; mode=display">q_{\pi}(s, a) = \mathcal{R}_{s}^{a} + \gamma \sum \limits_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a} \sum \limits_{a' \in \mathcal{A}} \pi (a' | s') q_{\pi}(s', a')</script><hr><p>例子:</p><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lecy0oxgj20h90dcwfj.jpg" alt="状态值函数的贝尔曼期望方程示例" width="55%" height="55%"></p><hr><p><strong>贝尔曼期望方程的矩阵形式:</strong></p><script type="math/tex; mode=display">v_{\pi} = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi} v_{\pi}</script><p>可直接求解:</p><script type="math/tex; mode=display">v_{\pi} = (I - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}</script><hr><h3 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h3><blockquote><p><strong>最优状态值函数(optimal state-value function)</strong> $v_{*}(s)$ 是所有策略中最大的值函数.</p><script type="math/tex; mode=display">v_{*}(s) = \max \limits_{\pi}v_{\pi}(s)</script><p><strong>最优动作值函数(optimal action-value function)</strong> $q_{*}(s, a)$ 是所有策略中最大的动作值函数.</p><script type="math/tex; mode=display">q_{*}(s, a) = \max \limits_{\pi}q_{\pi}(s, a)</script></blockquote><ul><li>最优值函数代表了MDP的最好性能.</li><li>当得知最优值函数时, MDP可被认为”已解决”.</li></ul><hr><p>例子: </p><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leoxfaylj20h70ee75c.jpg" alt="Student MDP中的最优状态值函数" width="50%" height="50%"></p><hr><p>例子:</p><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0leqk38l4j20hh0eg75i.jpg" alt="Student MDP中的最优动作值函数" width="50%" height="50%"></p><p>注: 根据公式\eqref{eq:mdp-state-value-function}, Pub动作的最优值应为 $q_{*} = +1 + (0.2 \times 6 + 0.4 \times 8 + 0.4 \times 10) = 9.4$.</p><hr><h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p>首先定义策略之间的偏序关系, 使得策略之间可以进行比较:</p><script type="math/tex; mode=display">\pi \geq \pi ' \quad \text{if} \quad  v_{\pi}(s) \geq v_{\pi '}(s) , \forall s</script><p>对于任意的MDP来说:</p><ul><li>存在一个最优策略 $\pi_{*}$, 使得 $\pi_{*} \geq \pi, \forall \pi$</li><li>所有的最优策略都能取得最优值函数 $v_{\pi_{*}}(s) = v_{*}(s)$</li><li>所有的最优策略都能取得最优动作值函数 $q_{\pi_{*}}(s, a) = q_{*}(s, a)$</li></ul><hr><p><strong>寻找最优策略</strong></p><p>一个最优策略可以通过最大化所有的 $q_{*}(s, a)$ 得到:</p><script type="math/tex; mode=display">\pi_{*} \left( a | s \right) = \left \{ \begin{array}{ll}1 \ {\mathbb {if}} \ a = \operatorname*{argmax} \limits_{a \in \mathcal{A}} \ q_{*} \left( s,a \right) \\              0 \ {\mathbb {otherwise}}              \end{array} \right.</script><ul><li>对于任意的MDP, 总存在确定的最优策略</li><li>如果我们知道 $q_{*}(s, a)$, 则可以立即得到最优策略</li></ul><hr><p>例子:</p><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0lfhg0710j20hn0ehjsl.jpg" alt="Student MDP的最优策略" width="50%" height="50%"></p><p>图中红色弧线表示每个状态的最优决策.</p><hr><h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>$v_{*}$可以通过贝尔曼最优方程递归得到:</p><p><img rel="noreferrer" src="https://ws1.sinaimg.cn/large/8662e3cegy1g0lfkujh38j20b804uaa2.jpg" width="40%" height="40%"></p><script type="math/tex; mode=display">v_{*}(s) = \max \limits_{a} q_{*}(s, a)\tag{5}\label{eq:state-bellman-optimal-equation}</script><p>与公式\eqref{eq:mdp-state-value-function}的贝尔曼期望方程进行比较, 此时不再取均值, 而是取最大值.</p><hr><p>$q_{*}$与公式\eqref{eq:mdp-action-value-function}类似:</p><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m10t6s7vj208003a747.jpg" width="40%" height="40%"></p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)\tag{6}\label{eq:action-bellman-optimal-equation}</script><hr><p><strong>状态值函数的贝尔曼最优方程</strong></p><p><img rel="noreferrer" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0m14a2fenj208m04xq2x.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:action-bellman-optimal-equation}代入公式\eqref{eq:state-bellman-optimal-equation}可得 $v_{*}$ 的贝尔曼最优方程:</p><script type="math/tex; mode=display">v _ { * } ( s ) = \max _ { a } \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } v _ { * } \left( s ^ { \prime } \right)</script><hr><p><strong>动作值函数的贝尔曼最优方程</strong></p><p><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0m18irqg7j208804bgll.jpg" width="40%" height="40%"></p><p>将公式\eqref{eq:state-bellman-optimal-equation}代入公式\eqref{eq:action-bellman-optimal-equation}可得 $q_{*}$ 的贝尔曼最优方程:</p><script type="math/tex; mode=display">q _ { * } ( s , a ) = \mathcal { R } _ { s } ^ { a } + \gamma \sum _ { s ^ { \prime } \in \mathcal { S } } \mathcal { P } _ { s s ^ { \prime } } ^ { a } \max _ { a ^ { \prime } } q _ { * } \left( s ^ { \prime } , a ^ { \prime } \right)</script><hr><p>例子:</p><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0m1ato6q2j20d70atjs3.jpg" alt="Student MDP贝尔曼最优方程" width="50%" height="50%"></p><hr><h3 id="贝尔曼最优方程的求解"><a href="#贝尔曼最优方程的求解" class="headerlink" title="贝尔曼最优方程的求解"></a>贝尔曼最优方程的求解</h3><p>贝尔曼最优方程<strong>不是线性的</strong>(因为有取$max$操作), 因此没有封闭解(Closed-form solution).</p><p>通常采用迭代求解方法:</p><ul><li>值迭代(Value Iteration)</li><li>策略迭代(Policy Iteration)</li><li>Q-Learning</li><li>Sarsa</li></ul><h2 id="MDP的扩展"><a href="#MDP的扩展" class="headerlink" title="MDP的扩展"></a>MDP的扩展</h2><ul><li>无穷和连续的MDPs</li><li>部分可观测的MDPs</li><li>不折扣, 平均奖励MDPs</li></ul><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第三章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf" target="_blank" rel="noopener">Lecture2: Markov Decision Processes</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇文章 &lt;a href=&quot;https://orzyt.cn/posts/introduction-to-rl&quot;&gt;强化学习（一）：强化学习简介&lt;/a&gt; 中, 我们介绍了强化学习的一些基本概念.&lt;/p&gt;
&lt;p&gt;本文将介绍用来对强化学习问题进行建模的&lt;strong&gt;马尔可夫决策过程(Markov Decision Processes, MDPs)&lt;/strong&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="马尔可夫决策过程" scheme="https://orzyt.cn/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（一）：强化学习简介</title>
    <link href="https://orzyt.cn/posts/introduction-to-rl/"/>
    <id>https://orzyt.cn/posts/introduction-to-rl/</id>
    <published>2019-02-25T14:56:13.000Z</published>
    <updated>2019-04-29T08:20:19.693Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍强化学习中的一些基本概念.</p><a id="more"></a><h2 id="强化学习的特征"><a href="#强化学习的特征" class="headerlink" title="强化学习的特征"></a>强化学习的特征</h2><p>作为机器学习的一个分支，强化学习主要的特征为:</p><ul><li><p>无监督,仅有奖励信号；</p></li><li><p>反馈有延迟,不是瞬时的;</p></li><li><p>时间是重要的(由于是时序数据,不是独立同分布的);</p></li><li><p>Agent的动作会影响后续得到的数据;</p></li></ul><hr><h2 id="强化学习的概念"><a href="#强化学习的概念" class="headerlink" title="强化学习的概念"></a>强化学习的概念</h2><h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>奖励(Rewards) $R_t$ 是一个标量的反馈信号,表示Agent在 $t$ 时刻的表现如何.</p><p><strong>Agent的目标</strong>: 最大化累积奖励(maximise cumulative reward).</p><p>强化学习基于<strong>奖励假设(reward hypothesis)</strong>.</p><blockquote><p><strong>奖励假设(Reward Hypothesis)</strong>:<br>所有强化学习任务的目标都可以被描述为最大化期望累积奖励.</p></blockquote><hr><h3 id="序贯决策"><a href="#序贯决策" class="headerlink" title="序贯决策"></a>序贯决策</h3><p><strong>序贯决策(Sequential Decision Making)的目标</strong>: 选择合适的动作最大化将来的累积奖励.</p><ul><li>动作可能会产生长期后果；</li><li>奖励会有延迟性;</li><li>牺牲立即回报可能会获得更多的长期回报.</li></ul><hr><h3 id="智能体和环境"><a href="#智能体和环境" class="headerlink" title="智能体和环境"></a>智能体和环境</h3><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k2ozf0lzj20aq0bxtb1.jpg" alt="Agent和环境" width="35%" height="35%"></p><p>智能体(Agent)在每个时刻$t$会:</p><ul><li>执行动作(Action)$A_t$;</li><li>接收观测(Observation)$O_t$;</li><li>接收标量奖励(Reward)$R_t$.</li></ul><p>而环境(Environment)则会:</p><ul><li>接收动作(Action)$A_t$;</li><li>产生观测(Observation)$O_{t+1}$;</li><li>产生标量奖励(Reward)$R_{t+1}$.</li></ul><hr><h3 id="历史与状态"><a href="#历史与状态" class="headerlink" title="历史与状态"></a>历史与状态</h3><blockquote><p><strong>历史(History):</strong>由一系列观测,动作和奖励构成.</p></blockquote><script type="math/tex; mode=display">H_t = O_1, R_1, A_1, \dots, A_{t-1}, O_t, R_t</script><p>下一步将发生什么取决于历史:</p><ul><li>智能体选择的action;</li><li>环境选择的observations/rewards.</li></ul><blockquote><p><strong>状态(State)</strong>:用来决定接下来会发生什么的信息.</p></blockquote><p><strong>状态是历史的函数:</strong></p><script type="math/tex; mode=display">S_t = f(H_t)</script><hr><h4 id="环境状态"><a href="#环境状态" class="headerlink" title="环境状态"></a>环境状态</h4><p><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3akygqpj20b20cptb5.jpg" alt="环境状态" width="35%" height="35%"></p><p>环境状态 $S_{t}^{e}$ 是环境的私有表示,通常对于智能体来说该状态不可见.</p><p>即使$S_{t}^{e}$可见,也可能包含不相关信息.</p><hr><h4 id="智能体状态"><a href="#智能体状态" class="headerlink" title="智能体状态"></a>智能体状态</h4><p><img rel="noreferrer" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k3e8bw4aj20b00cx419.jpg" alt="智能体状态" width="35%" height="35%"></p><p>智能体状态 $S_{t}^{a}$ 是智能体的内部表示,包含其用来决定下一步动作的信息,也是强化学习算法使用的信息.</p><p>可以写成历史的函数: $S_{t}^{a} = f(H_t)$</p><hr><h4 id="信息状态"><a href="#信息状态" class="headerlink" title="信息状态"></a>信息状态</h4><p><strong>信息状态(也称为马尔科夫状态)</strong>: 包含历史中所有有用的信息.</p><p><img rel="noreferrer" src="https://ws3.sinaimg.cn/large/8662e3cegy1g0k3nzaa8yj20mn0593ym.jpg" alt="马尔科夫状态定义" width="60%" height="60%"></p><p>马尔科夫状态表明: <strong>未来只与现在有关,而与过去无关.</strong></p><p>其中,<strong>环境状态$S_t^e$</strong>和<strong>历史$H_t$</strong>具有马尔科夫性质.</p><hr><h4 id="Rat-Example"><a href="#Rat-Example" class="headerlink" title="Rat Example"></a>Rat Example</h4><p><img rel="noreferrer" src="https://wx2.sinaimg.cn/large/8662e3cegy1g0k3trc5qxj20ny0doq88.jpg" alt="Rat Example" width="60%" height="60%"></p><ul><li><p>假如个体状态=序列中的后三个事件(不包括电击、获得奶酪，下同),事件序列3的结果会是什么? (答案是：电击)</p></li><li><p>假如个体状态=亮灯、响铃和拉电闸各自事件发生的次数,那么事件序列3的结果又是什么? (答案是：奶酪)</p></li><li><p>假如个体状态=完整的事件序列,那结果又是什么? (答案是：未知)</p></li></ul><hr><h4 id="完全可观测环境"><a href="#完全可观测环境" class="headerlink" title="完全可观测环境"></a>完全可观测环境</h4><p><strong>完全可观测性(Full observability):</strong> 智能体可以直接观测到环境状态,即</p><script type="math/tex; mode=display">O_t = S_t^a = S_t^e</script><ul><li>智能体状态 = 环境状态 = 信息状态</li><li>实际上是马尔科夫决策过程(Markov Decision Process, MDP)</li></ul><hr><h4 id="部分可观测环境"><a href="#部分可观测环境" class="headerlink" title="部分可观测环境"></a>部分可观测环境</h4><p><strong>部分可观测性(Partial observability):</strong> 智能体不能够直接观测到环境.</p><p>如,机器人不能通过摄像头得知自身的绝对位置.</p><ul><li>智能体状态 $\neq$ 环境状态</li><li>部分可观测马尔科夫决策过程(POMDP)</li></ul><p>此时,智能体必须构建其自身的状态表示 $S_t^a$,比如:</p><ul><li>完全的历史: $S_t^a = H_t$;</li><li>环境状态的置信度: $S _ { t } ^ { a } = \left( \mathbb { P } \left[ S _ { t } ^ { e } = s ^ { 1 } \right] , \ldots , \mathbb { P } \left[ S _ { t } ^ { e } = s ^ { n } \right] \right)$;</li><li>循环神经网络: $S_t^a = \sigma \left(S_{t-1}^{a}W_{s} + O_{t}W_{o}\right)$</li></ul><hr><h2 id="智能体的构成"><a href="#智能体的构成" class="headerlink" title="智能体的构成"></a>智能体的构成</h2><p>智能体主要包含以下几种成分:</p><ul><li><strong>策略(Policy)</strong>: 智能体的行为函数;</li><li><strong>值函数(Value Function)</strong>: 每个state或action的好坏;</li><li><strong>模型(Model)</strong>: 智能体对环境的表示.</li></ul><hr><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><ul><li>策略(Policy)是智能体的行为;</li><li>是<strong>状态</strong>到<strong>动作</strong>的映射;</li><li>确定性策略: $a = \pi(s)$;</li><li>随机性策略: $\pi(a|s) = \mathbb{P} \left[ A_{t} = a | S_{t} = s\right]$</li></ul><hr><h3 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h3><p>值函数(Value Function)是对于未来奖励的预测.</p><ul><li>用于评价状态的好坏;</li><li>因此可以用来选择动作.</li></ul><script type="math/tex; mode=display">v_{\pi}(s) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_{t} = s \right]</script><hr><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型(Model)用来预测环境接下来会做什么.</p><ul><li>$\mathcal{P}$: 预测下一个状态.<script type="math/tex; mode=display">\mathcal{P}_{ss'}^{a} = \mathbb{P} \left[ S_{t+1} = s' | S_{t} = s, A_{t} = a\right]</script></li><li>$\mathcal{R}$: 预测下一个(立即)奖励.<script type="math/tex; mode=display">\mathcal{R}_{s}^{a} = \mathbb{E} \left[ R_{t+1} | S_{t} = s, A_{t} = a\right]</script></li></ul><hr><h3 id="Maze-Example"><a href="#Maze-Example" class="headerlink" title="Maze Example"></a>Maze Example</h3><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k4norup9j20mj092dg5.jpg" alt="Maze Example" width="60%" height="60%"></p><hr><p><strong>策略表示:</strong></p><p>箭头表示每个状态的策略 $\pi(s)$.</p><p><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k4u9pdcdj20f10c5q38.jpg" alt="Maze Example: Policy" width="40%" height="40%"></p><hr><p><strong>值函数表示:</strong></p><p>数值表示每个状态的值 $v_{\pi}(s)$.</p><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k4w9vn7wj20f60cct8y.jpg" alt="Maze Example: Value Function" width="40%" height="40%"></p><hr><p><strong>模型表示:</strong></p><p>智能体可能对环境建立内部模型</p><ul><li>网格布局表示转移模型 $\mathcal{P}_{ss’}^{a}$;</li><li>数值表示每个状态的立即奖励 $\mathcal{R}_{s}^{a}$.</li></ul><p><img rel="noreferrer" src="https://wx1.sinaimg.cn/large/8662e3cegy1g0k51h023dj20c109kt8o.jpg" alt="Maze Example: Value Function" width="40%" height="40%"></p><hr><h3 id="智能体的分类"><a href="#智能体的分类" class="headerlink" title="智能体的分类"></a>智能体的分类</h3><p>按智能体的成分分类:</p><ul><li>基于值函数(Value Based)</li><li>基于策略(Policy Based)</li><li>演员-评论家(Actor Critic)</li></ul><p>或者按有无模型分类:</p><ul><li>无模型(Model Free)</li><li>基于模型(Model Based)</li></ul><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k55tidg0j20f30eaab9.jpg" alt="智能体分类" width="40%" height="40%"></p><hr><h2 id="强化学习的问题"><a href="#强化学习的问题" class="headerlink" title="强化学习的问题"></a>强化学习的问题</h2><h3 id="学习与规划"><a href="#学习与规划" class="headerlink" title="学习与规划"></a>学习与规划</h3><p><strong>强化学习</strong></p><ul><li>环境的初始状态未知;</li><li>智能体与环境进行交互;</li><li>智能体提升其策略.</li></ul><p><img rel="noreferrer" src="https://ws4.sinaimg.cn/large/8662e3cegy1g0k5dprnn4j20p80ejn0q.jpg" alt="学习" width="60%" height="60%"></p><p><strong>规划</strong></p><ul><li>环境的模型已知;</li><li>智能体通过模型进行计算,无须与外部进行交互;</li><li>智能体提升其策略</li></ul><p><img rel="noreferrer" src="https://ws2.sinaimg.cn/large/8662e3cegy1g0k5eleerwj20ok0df0u3.jpg" alt="规划" width="60%" height="60%"></p><hr><h3 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h3><p>强化学习是一种试错(trial-and-error)学习.</p><p>智能体需要从与环境的交互中找到一种好的策略,同时不损失过多的奖励.</p><ul><li><strong>探索(Exploration):</strong> 从环境中寻找更多信息;</li><li><strong>利用(Exploitation):</strong> 利用已知信息使奖励最大化.</li></ul><p>探索和利用同等重要,即使根据已有信息选择出的最优动作可以得到不错的奖励,不妨尝试全新的动作对环境进行探索,也许可以得到更好的结果.</p><hr><h3 id="预测和控制"><a href="#预测和控制" class="headerlink" title="预测和控制"></a>预测和控制</h3><ul><li><strong>预测(Prediction):</strong> 对未来进行评估.</li></ul><p><img rel="noreferrer" src="https://wx3.sinaimg.cn/large/8662e3cegy1g0k5ryo676j20nz0ds0tn.jpg" alt="Gridworld Example: Prediction" width="60%" height="60%"></p><hr><ul><li><strong>控制(Control):</strong> 最优化未来的结果.</li></ul><p><img rel="noreferrer" src="https://wx4.sinaimg.cn/large/8662e3cegy1g0k5r6vdmgj20nv0eot9w.jpg" alt="Gridworld Example: Control" width="60%" height="60%"></p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement learning: An introduction (second edition)</a> 第一章</li><li>UCL Course on RL <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf" target="_blank" rel="noopener">Lecture1: Introduction to Reinforcement Learning</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍强化学习中的一些基本概念.&lt;/p&gt;
    
    </summary>
    
      <category term="强化学习" scheme="https://orzyt.cn/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://orzyt.cn/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode637 Average of Levels in Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode637-average-of-levels-in-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode637-average-of-levels-in-binary-tree/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/average-of-levels-in-binary-tree/" target="_blank" rel="noopener">LeetCode637 Average of Levels in Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的层次遍历,使用空指针作为每层的分界.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; averageOfLevels(TreeNode* root) &#123;</span><br><span class="line">        <span class="built_in">queue</span>&lt;TreeNode*&gt; que;</span><br><span class="line">        que.push(root); que.push(<span class="literal">nullptr</span>);</span><br><span class="line">        <span class="keyword">double</span> sum = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; ret;</span><br><span class="line">        <span class="keyword">while</span> (!que.empty()) &#123;</span><br><span class="line">            TreeNode* u = que.front(); que.pop();</span><br><span class="line">            <span class="keyword">if</span> (u == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                ret.push_back(sum / cnt);</span><br><span class="line">                sum = cnt = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span> (!que.empty()) que.push(<span class="literal">nullptr</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                sum += u-&gt;val; cnt++;</span><br><span class="line">                <span class="keyword">if</span> (u-&gt;left) que.push(u-&gt;left);</span><br><span class="line">                <span class="keyword">if</span> (u-&gt;right) que.push(u-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/average-of-levels-
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode654 Maximum Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode654-maximum-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode654-maximum-binary-tree/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/maximum-binary-tree/" target="_blank" rel="noopener">LeetCode654 Maximum Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>递归构造二叉搜索树,树的左右儿子都比父结点小.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">helper</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">        TreeNode* node = <span class="keyword">new</span> TreeNode(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">int</span> num = nums[l], id = l;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = l; i &lt;= r; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (num &lt; nums[i]) &#123;</span><br><span class="line">                num = nums[i];</span><br><span class="line">                id = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        node-&gt;val = num;</span><br><span class="line">        <span class="keyword">if</span> (l &lt;= id - <span class="number">1</span>) node-&gt;left = helper(nums, l, id - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (id + <span class="number">1</span> &lt;= r) node-&gt;right = helper(nums, id + <span class="number">1</span>, r);</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">TreeNode* <span class="title">constructMaximumBinaryTree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> helper(nums, <span class="number">0</span>, nums.size() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/maximum-binary-tre
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode657 Robot Return to Origin</title>
    <link href="https://orzyt.cn/posts/leetcode657-robot-return-to-origin/"/>
    <id>https://orzyt.cn/posts/leetcode657-robot-return-to-origin/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/robot-return-to-origin/" target="_blank" rel="noopener">LeetCode657 Robot Return to Origin</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>判断上和下,左和右的次数是否相同即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">judgeCircle</span><span class="params">(<span class="built_in">string</span> moves)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; f;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; c: moves) f[c]++;</span><br><span class="line">        <span class="keyword">return</span> f[<span class="string">'U'</span>] == f[<span class="string">'D'</span>] &amp;&amp; f[<span class="string">'L'</span>] == f[<span class="string">'R'</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/robot-return-to-or
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode661 Image Smoother</title>
    <link href="https://orzyt.cn/posts/leetcode661-image-smoother/"/>
    <id>https://orzyt.cn/posts/leetcode661-image-smoother/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/image-smoother/" target="_blank" rel="noopener">LeetCode661 Image Smoother</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>按题意模拟即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dx[<span class="number">9</span>] = &#123;<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> dy[<span class="number">9</span>] = &#123;<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; imageSmoother(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; M) &#123;</span><br><span class="line">        <span class="keyword">int</span> n = M.size(), m = M[<span class="number">0</span>].size();</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ret(n, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; m; ++j) &#123;</span><br><span class="line">                <span class="keyword">int</span> sum = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">9</span>; ++k) &#123;</span><br><span class="line">                    <span class="keyword">int</span> x = i + dx[k], y = j + dy[k];</span><br><span class="line">                    <span class="keyword">if</span> (x &lt; <span class="number">0</span> || x &gt;= n || y &lt; <span class="number">0</span> || y &gt;= m) <span class="keyword">continue</span>;</span><br><span class="line">                    sum += M[x][y];</span><br><span class="line">                    cnt++;</span><br><span class="line">                &#125;</span><br><span class="line">                ret[i].push_back(sum / cnt);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/image-smoother/&quot; t
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode665 Non-decreasing Array</title>
    <link href="https://orzyt.cn/posts/leetcode665-non-decreasing-array/"/>
    <id>https://orzyt.cn/posts/leetcode665-non-decreasing-array/</id>
    <published>2019-02-08T09:23:30.000Z</published>
    <updated>2019-02-08T10:25:59.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/non-decreasing-array/" target="_blank" rel="noopener">LeetCode665 Non-decreasing Array</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>判断只修改一个数能否使得数组非递减.</p><p>首先计算数组从左往右能延伸的距离$l$,从右往左能延伸的距离$r$.</p><p>可行的情况有:</p><ul><li>$ r \leq l $</li><li>$r == l + 1$ 且 满足下列情况之一<ul><li>r 为最后一位</li><li>l位置的值 $ \leq $ r + 1位置的值</li><li>l为首位 </li><li>l-1位置的值 $ \leq $ r 位置的值</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">checkPossibility</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> siz = nums.size(), l = <span class="number">0</span>, r = siz - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (l + <span class="number">1</span> &lt; siz &amp;&amp; nums[l] &lt;= nums[l + <span class="number">1</span>]) l++;</span><br><span class="line">        <span class="keyword">while</span> (r &gt; <span class="number">0</span> &amp;&amp; nums[r - <span class="number">1</span>] &lt;= nums[r]) r--;</span><br><span class="line">        <span class="keyword">return</span> r &lt;= l || (r == l + <span class="number">1</span> &amp;&amp; ((nums[l] &lt;= nums[r + <span class="number">1</span>] || r == siz - <span class="number">1</span>) || (l == <span class="number">0</span> || nums[l - <span class="number">1</span>] &lt;= nums[r])));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/non-decreasing-arr
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode617 Merge Two Binary Trees</title>
    <link href="https://orzyt.cn/posts/leetcode617-merge-two-binary-trees/"/>
    <id>https://orzyt.cn/posts/leetcode617-merge-two-binary-trees/</id>
    <published>2019-02-08T09:02:05.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/merge-two-binary-trees/" target="_blank" rel="noopener">LeetCode617 Merge Two Binary Trees</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的合并.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">mergeTrees</span><span class="params">(TreeNode* t1, TreeNode* t2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (t1 &amp;&amp; t2) &#123;</span><br><span class="line">            t1-&gt;val += t2-&gt;val;</span><br><span class="line">            t1-&gt;left = mergeTrees(t1-&gt;left, t2-&gt;left);</span><br><span class="line">            t1-&gt;right = mergeTrees(t1-&gt;right, t2-&gt;right);</span><br><span class="line">            <span class="keyword">return</span> t1;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">return</span> t1 ? t1 : t2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/merge-two-binary-t
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode606 Construct String from Binary Tree</title>
    <link href="https://orzyt.cn/posts/leetcode606-construct-string-from-binary-tree/"/>
    <id>https://orzyt.cn/posts/leetcode606-construct-string-from-binary-tree/</id>
    <published>2019-02-08T09:01:50.000Z</published>
    <updated>2019-02-08T10:25:59.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/construct-string-from-binary-tree/" target="_blank" rel="noopener">LeetCode606 Construct String from Binary Tree</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>二叉树的简单遍历.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">dfs</span><span class="params">(TreeNode* t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (t == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">if</span> (t-&gt;left == <span class="literal">NULL</span> &amp;&amp; t-&gt;right == <span class="literal">NULL</span>) <span class="keyword">return</span> to_string(t-&gt;val);</span><br><span class="line">        <span class="built_in">string</span> ret = to_string(t-&gt;val) + <span class="string">"("</span> + dfs(t-&gt;left) + <span class="string">")"</span>;</span><br><span class="line">        <span class="keyword">if</span> (t-&gt;right) ret += <span class="string">"("</span> + dfs(t-&gt;right) + <span class="string">")"</span>;</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">tree2str</span><span class="params">(TreeNode* t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> dfs(t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/construct-string-f
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode599 Minimum Index Sum of Two Lists</title>
    <link href="https://orzyt.cn/posts/leetcode599-minimum-index-sum-of-two-lists/"/>
    <id>https://orzyt.cn/posts/leetcode599-minimum-index-sum-of-two-lists/</id>
    <published>2019-02-08T09:01:34.000Z</published>
    <updated>2019-02-08T10:25:59.950Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/minimum-index-sum-of-two-lists/" target="_blank" rel="noopener">LeetCode599 Minimum Index Sum of Two Lists</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>找出交集中下标和最小值.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; findRestaurant(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list1, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list2) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; ans;</span><br><span class="line">        <span class="keyword">int</span> n = list1.size(), m = list2.size();</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; hs;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) hs[list1[i]] = i;</span><br><span class="line">        <span class="keyword">int</span> minSum = INT_MAX;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; ++i) &#123;</span><br><span class="line">            <span class="keyword">if</span> (hs.count(list2[i])) &#123;</span><br><span class="line">                <span class="keyword">int</span> j = hs[list2[i]];</span><br><span class="line">                <span class="keyword">if</span> (i + j &lt; minSum) &#123;</span><br><span class="line">                    minSum = i + j;</span><br><span class="line">                    ans.clear();</span><br><span class="line">                    ans.push_back(list2[i]);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i + j == minSum) &#123;</span><br><span class="line">                    ans.push_back(list2[i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/minimum-index-sum-
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode38 Count and Say</title>
    <link href="https://orzyt.cn/posts/leetcode38-count-and-say/"/>
    <id>https://orzyt.cn/posts/leetcode38-count-and-say/</id>
    <published>2019-02-08T09:00:37.000Z</published>
    <updated>2019-02-08T10:25:59.942Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><blockquote><p><a href="https://leetcode.com/problems/count-and-say/" target="_blank" rel="noopener">LeetCode38 Count and Say</a></p></blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>按照规则模拟即可.</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">countAndSay</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">string</span> s = <span class="string">"1"</span>, ans = s;</span><br><span class="line">        <span class="keyword">while</span> (--n) &#123;</span><br><span class="line">            ans.clear();</span><br><span class="line">            <span class="keyword">int</span> len = s.size();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; ++i) &#123;</span><br><span class="line">                <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">while</span> (s[i] == s[i + <span class="number">1</span>] &amp;&amp; i + <span class="number">1</span> &lt; len) &#123;</span><br><span class="line">                    count++;</span><br><span class="line">                    i++;</span><br><span class="line">                &#125;</span><br><span class="line">                ans += to_string(count) + s[i];</span><br><span class="line">            &#125;</span><br><span class="line">            s = ans;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/count-and-say/&quot; ta
      
    
    </summary>
    
      <category term="LeetCode" scheme="https://orzyt.cn/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://orzyt.cn/tags/LeetCode/"/>
    
  </entry>
  
</feed>
